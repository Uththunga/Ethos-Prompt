"""
Marketing Agent Implementation
Uses LangGraph create_react_agent pattern with tools for marketing queries

Performance Optimizations (Week 5):
- Lazy module imports (Task 1.3.1): Defer heavy imports until first use
- HTTP session reuse (Task 1.4.1): Connection pooling for OpenRouter API
- System prompt caching (Task 1.1.1): Cache static prompts in memory
- Model metadata caching (Task 1.1.2): Cache model config at instance level
"""
import logging
import os
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone

# ============================================================================
# PERFORMANCE OPTIMIZATION: Lazy Module Imports (Week 5, Task 1.3.1)
# Defer heavy imports (langgraph, langchain) until first use to reduce cold start
# Estimated impact: 500ms-1s reduction in cold start time
# ============================================================================

# Check if LangGraph is available (but don't import yet)
LANGGRAPH_AVAILABLE = False
try:
    import langgraph  # noqa: F401
    LANGGRAPH_AVAILABLE = True
except ImportError:
    logging.warning("LangGraph not available. Install with: pip install langgraph langchain-core langchain-openai")

# Import lightweight dependencies immediately
from ..common.base_agent import BaseAgent, AgentResponse
from .marketing_retriever import marketing_retriever, RetrievalResult
from langchain_core.messages import AIMessage, ToolMessage, HumanMessage
from .marketing_kb_content import get_kb_documents_by_category, get_kb_documents_by_subcategory
from ..common.monitoring import AgentMonitoring
from .config import get_config, MarketingAgentConfig
# Import centralized prompts (single source of truth for system prompt)
from .prompts.marketing_prompts import get_system_prompt, get_dynamic_policies
# Import error handling utilities (circuit breakers, backoff)
from .error_handling import get_context_aware_fallback, watsonx_circuit_breaker, kb_circuit_breaker
# PERF-003 FIX: Import validation module
from .validation import validate_marketing_response

logger = logging.getLogger(__name__)
# Monitoring helper (logs-based metrics; no extra deps)
_mon = AgentMonitoring(service="marketing-api", environment=os.getenv("ENVIRONMENT", "unknown"))


def validate_input_message(message: str, config: MarketingAgentConfig) -> str:
    """
    Validate and sanitize user input message.

    Args:
        message: User input message
        config: Agent configuration with max_input_length

    Returns:
        Sanitized message

    Raises:
        ValueError: If message is invalid
    """
    if not message or not isinstance(message, str):
        raise ValueError("Message must be a non-empty string")

    message = message.strip()

    if not message:
        raise ValueError("Message cannot be empty or only whitespace")

    if len(message) > config.max_input_length:
        raise ValueError(
            f"Message too long: {len(message)} chars (max: {config.max_input_length})"
        )

    return message

# ============================================================================
# PERFORMANCE OPTIMIZATION: HTTP Session Reuse (Week 5, Task 1.4.1)
# Global HTTP client with connection pooling for OpenRouter API calls
# Estimated impact: 50-100ms reduction per API call
# ============================================================================

_http_client = None

def get_http_client():
    """
    Get or create global HTTP client with connection pooling.

    Performance optimization: Reuses HTTP connections to OpenRouter API
    with keep-alive to reduce connection overhead.

    Returns:
        httpx.AsyncClient instance with connection pooling
    """
    global _http_client

    if _http_client is None:
        try:
            import httpx
            _http_client = httpx.AsyncClient(
                timeout=30.0,
                limits=httpx.Limits(
                    max_keepalive_connections=5,
                    max_connections=10,
                    keepalive_expiry=30.0
                ),
                http2=True  # Enable HTTP/2 for better performance
            )
            logger.info("HTTP client initialized with connection pooling (max_keepalive=5, max_connections=10)")
        except ImportError:
            logger.warning("httpx not available, HTTP session reuse disabled")
            _http_client = None

    return _http_client

# ============================================================================
# PERFORMANCE OPTIMIZATION: System Prompt Caching (Week 5, Task 1.1.1)
# System prompt is now centralized in prompts/marketing_prompts.py
# The get_system_prompt() function is imported from there (single source of truth)
# Dynamic policy injection is handled by get_dynamic_policies() from prompts module
# ============================================================================


class MarketingAgent(BaseAgent):
    """
    Marketing Agent using LangGraph create_react_agent pattern.

    Tools:
    - search_kb: Search marketing knowledge base
    - get_pricing: Get pricing information
    - request_consultation: Request a consultation with EthosPrompt

    Features:
    - LangGraph checkpointing for conversation persistence
    - Streaming support
    - Tool-calling for accurate information retrieval

    Performance Optimizations (Week 5):
    - Cached system prompt (Task 1.1.1)
    - Cached model metadata (Task 1.1.2)
    - Singleton pattern for agent reuse (Task 1.3.3)
    """

    def __init__(self, db=None, openrouter_api_key: Optional[str] = None, config: Optional[MarketingAgentConfig] = None):
        super().__init__()
        self.name = "marketing_agent"
        self.db = db

        # Load configuration
        self.config = config or get_config()
        logger.info(f"Marketing Agent config loaded: temp={self.config.temperature}, max_tokens={self.config.max_tokens}")

        # Validate configuration
        if self.config.max_tokens < 1:
            raise ValueError(f"Invalid max_tokens: {self.config.max_tokens}")
        if not 0 <= self.config.temperature <= 1:
            raise ValueError(f"Invalid temperature: {self.config.temperature} (must be 0-1)")

        if not LANGGRAPH_AVAILABLE:
            raise ImportError("LangGraph is required. Install with: pip install langgraph langchain-core langchain-openai")

        # ============================================================================
        # PERFORMANCE OPTIMIZATION: Lazy Module Imports (Week 5, Task 1.3.1)
        # Import heavy modules only when needed (on first agent initialization)
        # ============================================================================
        from langgraph.prebuilt import create_react_agent
        from langgraph.checkpoint.memory import MemorySaver

        # Store for later use
        self._create_react_agent = create_react_agent
        self._MemorySaver = MemorySaver

        # Initialize retriever
        marketing_retriever.db = db

        # Initialize Data Drift Monitor (Phase 3)
        from .data_drift_monitor import DataDriftMonitor
        self.drift_monitor = DataDriftMonitor(db=self.db)

        # Initialize RAG Quality Metrics (Phase 3)
        from .rag_quality_metrics import RAGQualityMetrics
        self.rag_metrics = RAGQualityMetrics(db=self.db)

        # Check which LLM provider to use (CHECK THIS FIRST!)
        self.use_granite = os.getenv("USE_GRANITE_LLM", "false").lower() == "true"

        # Get API key (only required for OpenRouter mode)
        self.api_key = openrouter_api_key or os.getenv("OPENROUTER_API_KEY")
        if not self.use_granite and not self.api_key:
            raise ValueError("OPENROUTER_API_KEY is required when USE_GRANITE_LLM is not enabled")

        # Check if mock mode
        self.use_mock = os.getenv("OPENROUTER_USE_MOCK", "false").lower() == "true"

        # Initialize LLM with config
        self.llm = self._initialize_llm()

        # BIZ-001 FIX: Try FirestoreCheckpointer, fallback to MemorySaver
        self.checkpointer = self._initialize_checkpointer()

        # Define tools
        self.tools = self._define_tools()

        # Create agent with cached system prompt
        self.agent = self._create_react_agent(
            self.llm,
            self.tools,
            prompt=get_system_prompt(),  # Use cached system prompt
            checkpointer=self.checkpointer
        )

        logger.info(f"Marketing Agent initialized (mock_mode={self.use_mock})")

    def _initialize_checkpointer(self):
        """
        Initialize checkpointer with FirestoreCheckpointer or MemorySaver fallback.

        BIZ-001 FIX: Attempts FirestoreCheckpointer first for persistence,
        falls back to MemorySaver if Firestore unavailable or errors occur.
        """
        # Check if Firestore persistence is desired
        use_firestore = os.getenv("USE_FIRESTORE_CHECKPOINTER", "false").lower() == "true"

        if use_firestore and self.db:
            try:
                from .firestore_checkpointer import get_firestore_checkpointer
                checkpointer = get_firestore_checkpointer(self.db)
                if checkpointer:
                    logger.info("âœ“ Using FirestoreCheckpointer for conversation persistence")
                    return checkpointer
            except Exception as e:
                logger.warning(f"FirestoreCheckpointer failed, using MemorySaver: {e}")

        # Fallback to in-memory (conversations lost on restart)
        logger.info("Using MemorySaver (conversations will not persist across restarts)")
        return self._MemorySaver()

    def _initialize_llm(self):
        """
        Initialize LLM (IBM Granite 4.0 H-Small or OpenRouter).

        Performance optimizations:
        - Lazy imports (Task 1.3.1)
        - Uses cached model metadata from instance variables (Task 1.1.2)
        """
        if self.use_mock:
            logger.info("Using MOCK LLM (zero billing)")
            # Mock LLM for testing - for now use real LLM but log mock mode

        if self.use_granite:
            #  Use IBM Granite via watsonx.ai
            model_name = os.getenv("WATSONX_MODEL_ID", "ibm/granite-4-h-small")
            logger.info(f"Initializing IBM Granite via watsonx.ai (model={model_name})")

            # Get credentials from environment (set via Cloud Run secrets)
            watsonx_api_key = os.getenv("WATSONX_API_KEY")
            watsonx_project_id = os.getenv("WATSONX_PROJECT_ID")

            if not watsonx_api_key or not watsonx_project_id:
                raise ValueError("WATSONX_API_KEY and WATSONX_PROJECT_ID are required when USE_GRANITE_LLM=true")

            # Lazy import watsonx client
            from llm.watsonx_client import WatsonxGraniteLangChain

            # Initialize Granite via watsonx.ai
            llm = WatsonxGraniteLangChain(
                model=model_name,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                streaming=True,
                watsonx_api_key=watsonx_api_key,
                watsonx_project_id=watsonx_project_id
            )

            logger.info(f"IBM Granite initialized (model={model_name}, temp={self.config.temperature})")
        else:
            # Use OpenRouter (fallback/legacy)
            from langchain_openai import ChatOpenAI

            llm = ChatOpenAI(
                model=self.config.model_name,
                openai_api_key=self.api_key,
                openai_api_base="https://openrouter.ai/api/v1",
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                streaming=True
            )

            logger.info(f"OpenRouter LLM initialized: {self.config.model_name}")

        return llm

    def _clean_sources(self, sources: List[Dict]) -> List[Dict]:
        """
        Remove scores from sources before returning to user.

        FIX #1: Strip relevance scores from user-facing source citations.
        Internal scores are useful for debugging but shouldn't be exposed.

        Args:
            sources: List of source dicts from retriever

        Returns:
            List of source dicts without "score" field
        """
        clean = []
        for src in sources:
            clean.append({
                "index": src.get("index"),
                "title": src.get("title"),
                "category": src.get("category"),
                "page": src.get("page")
                # Omit "score" and "excerpt" fields for cleaner UX
            })
        return clean

    # ============================================================================
    # GRANITE 4.0 OPTIMIZATION: Lightweight Response Verification
    # Granite has intrinsic reasoning via Mamba layers - avoid full CoT
    # Simple Yes/No grounding check for high-stakes responses
    # ============================================================================

    async def _lightweight_verify(self, response: str, context: str) -> bool:
        """
        Granite-optimized response verification.

        Uses simple grounding check instead of full CoT verification.
        Granite 4.0's Mamba layers provide implicit verification, so we
        only need a lightweight explicit check for high-stakes responses.

        Args:
            response: Generated response to verify
            context: Retrieved KB context

        Returns:
            True if response is grounded in context
        """
        if not response or not context:
            return False

        # Truncate for efficiency - Granite handles this well
        response_sample = response[:300] if len(response) > 300 else response
        context_sample = context[:500] if len(context) > 500 else context

        verification_prompt = f"""Verify grounding. Is this response factually supported by the context?

Context: {context_sample}

Response: {response_sample}

Answer only: Yes or No"""

        try:
            # Use the LLM for quick verification
            result = await self.llm.ainvoke(verification_prompt)
            result_text = result.content.lower() if hasattr(result, 'content') else str(result).lower()
            return "yes" in result_text
        except Exception as e:
            logger.warning(f"Lightweight verification failed: {e}")
            return True  # Fail open - don't block on verification errors

    async def _check_hallucination(self, response: str, context: str) -> float:
        """
        Granite Guardian-style hallucination detection.

        Returns a grounding score between 0.0 (hallucinated) and 1.0 (grounded).
        Uses simple keyword overlap + semantic check pattern.

        Args:
            response: Generated response to check
            context: Retrieved KB context

        Returns:
            Float score 0.0-1.0 indicating grounding level
        """
        if not response or not context:
            return 0.0

        # Quick keyword overlap check (fast, no LLM call)
        response_words = set(response.lower().split())
        context_words = set(context.lower().split())
        overlap = len(response_words & context_words)
        keyword_score = min(overlap / max(len(response_words), 1), 1.0)

        # If keyword overlap is high, likely grounded
        if keyword_score > 0.3:
            return min(keyword_score * 1.5, 1.0)  # Boost for high overlap

        # For low overlap, use LLM verification
        try:
            is_grounded = await self._lightweight_verify(response, context)
            return 0.8 if is_grounded else 0.2
        except Exception as e:
            logger.debug(f"Verification failed: {e}")
            return 0.5  # Uncertain

    def _define_tools(self) -> List:
        """
        Define tools for the marketing agent.

        Performance optimization: Lazy import of langchain_core.tools (Task 1.3.1)
        """
        # Lazy import (Task 1.3.1)
        from langchain_core.tools import tool

        # ================================================================
        # CONTEXT OPTIMIZATION: Concise Tool Descriptions
        # Reduced verbose docstrings from ~150 tokens to ~50 tokens each
        # Tool descriptions are included in every LLM call context
        # ================================================================

        @tool
        async def search_kb(query: Optional[str] = None, category: Optional[str] = None) -> str:
            """Search EthosPrompt KB for company info, services, features, pricing, or FAQs.
            Args: query (search term), category (optional: company/services/product/pricing/support/faq)
            """
            # Be resilient to missing/empty query from the model's tool call
            if not query or not str(query).strip():
                logger.warning("search_kb called without a query; using safe default fallback.")
                query = "EthosPrompt overview"
            try:
                # Retrieve relevant content
                results = await marketing_retriever.retrieve(
                    query=query,
                    top_k=5,
                    category_filter=category,
                    use_hybrid=True
                )

                if not results:
                    # Provide helpful fallback response
                    return """I don't have specific information about that in my knowledge base right now.

However, I'd be happy to help! Here's what I can tell you about EthosPrompt:

EthosPrompt is an AI-powered platform designed to help businesses streamline their operations through intelligent prompt management and RAG (Retrieval-Augmented Generation) technology. We offer solutions for:
- Smart Business Assistance
- System Integration
- Intelligent Applications
- Digital Transformation
- AI Prompt Optimization

For detailed information about specific features or pricing, I'd recommend scheduling a consultation with our team. They can provide personalized guidance based on your needs."""

                # Format context
                # FIX #2: Remove scores from tool output (defense in depth)
                # Don't include sources in tool response - LLM doesn't need them
                # The response dict will handle clean source citations
                context = marketing_retriever.format_context(results, max_tokens=2000)
                return context

            except Exception as e:
                logger.error(f"Error in search_kb tool: {e}")
                # Provide helpful fallback from config
                return self.config.kb_error_message


        @tool
        async def get_pricing(service: Optional[str] = None) -> str:
            """Get pricing info. Args: service (optional: intelligent_applications/smart_assistant/etc)"""
            try:
                # Get pricing documents (from engagement/pricing and support/faq/pricing)
                pricing_docs = get_kb_documents_by_subcategory("pricing")

                # Also check for FAQ pricing topic if needed
                faq_pricing = [
                    doc for doc in get_kb_documents_by_category("support")
                    if doc.get("metadata", {}).get("topic") == "pricing"
                ]

                # Combine unique docs
                all_docs = pricing_docs + [d for d in faq_pricing if d not in pricing_docs]

                if not all_docs:
                    return "Pricing information not available. Please contact us for a custom quote."

                # Format pricing info
                pricing_text = "\n\n".join([
                    f"{doc['title']}\n{doc['content']}"
                    for doc in pricing_docs
                ])

                return pricing_text

            except Exception as e:
                logger.error(f"Error in get_pricing tool: {e}")
                return f"Error retrieving pricing: {str(e)}"

        @tool
        def request_consultation(name: str, email: str, service: Optional[str] = None, message: Optional[str] = None) -> str:
            """Book consultation. Args: name, email, service (optional), message (optional)"""
            try:
                # GAP-006 FIX: Persist consultation to Firestore
                reference_number = None
                if self.db:
                    try:
                        import uuid
                        from datetime import datetime, timezone as tz

                        # Generate reference number
                        reference_number = f"CR-{datetime.now(tz.utc).year}-{str(uuid.uuid4())[:6].upper()}"

                        # Create consultation document
                        consultation_doc = {
                            "referenceNumber": reference_number,
                            "name": name.strip() if name else "",
                            "email": email.strip().lower() if email else "",
                            "service": service or "general",
                            "message": message or "",
                            "source": "marketing_chat_agent",
                            "status": "pending",
                            "createdAt": datetime.now(tz.utc),
                            "metadata": {
                                "conversationId": None,  # Could be passed from context
                                "agentVersion": "2.0",
                            }
                        }

                        # Store in Firestore
                        self.db.collection("consultation_requests").add(consultation_doc)
                        logger.info(f"[GAP-006] Consultation persisted: {reference_number} for {email}")

                        # Also upsert to contacts collection for lead tracking
                        contacts_ref = self.db.collection("contacts")
                        existing = contacts_ref.where("email", "==", email.strip().lower()).limit(1).get()

                        if not existing:
                            # Create new contact
                            contacts_ref.add({
                                "name": name.strip() if name else "",
                                "email": email.strip().lower() if email else "",
                                "source": "chat_consultation",
                                "status": "new",
                                "meta": {
                                    "service": service or "general",
                                    "originalLeadIds": {"consultationRef": reference_number}
                                },
                                "createdAt": datetime.now(tz.utc),
                                "updatedAt": datetime.now(tz.utc),
                            })
                            logger.info(f"[GAP-006] New contact created for {email}")
                        else:
                            # Update existing contact
                            existing[0].reference.update({
                                "updatedAt": datetime.now(tz.utc),
                                "lastContactedAt": datetime.now(tz.utc),
                            })
                            logger.info(f"[GAP-006] Existing contact updated for {email}")

                    except Exception as db_error:
                        logger.warning(f"[GAP-006] Firestore persistence failed (non-blocking): {db_error}")
                        # Continue - don't let DB errors break the tool response

                logger.info(f"Consultation request: {name} ({email}) - Service: {service} - Ref: {reference_number}")

                ref_text = f"\n\nYour reference number: {reference_number}" if reference_number else ""
                return f"""Thank you for your interest, {name}!

I've recorded your request for a consultation{f' about {service}' if service else ''}.{ref_text}

Next steps:
1. You'll receive a confirmation email at {email} within 24 hours
2. Our human agents will contact you to discuss your needs and begin working together
3. We'll assess your requirements and design a custom solution tailored to your business

In the meantime, feel free to ask me any questions about EthosPrompt!

Contact page: {os.getenv('APP_BASE_URL', 'https://ethosprompt-staging.web.app')}/contact
"""

            except Exception as e:
                logger.error(f"Error in request_consultation tool: {e}")
                return f"Error requesting consultation: {str(e)}. Please contact us directly at /contact"

        @tool
        def calculate_roi_estimate(
            business_type: str = "E-commerce",
            monthly_visitors: int = 5000,
            conversion_rate: float = 2.0,
            order_value: float = 125.0,
            maintenance_costs: float = 1500.0,
            current_platform: str = "WordPress"
        ) -> str:
            """Calculate ROI for digital transformation.
            Args: business_type (E-commerce/SaaS/Healthcare/etc), monthly_visitors,
            conversion_rate (%), order_value ($), maintenance_costs ($/month),
            current_platform (WordPress/Shopify/Custom PHP/Wix/Other)
            """
            try:
                # Import centralized ROI logic
                from .roi_calculator import calculate_roi

                # Perform calculation
                results = calculate_roi(
                    business_type=business_type,
                    monthly_visitors=monthly_visitors,
                    conversion_rate=conversion_rate,
                    order_value=order_value,
                    maintenance_costs=maintenance_costs,
                    current_platform=current_platform
                )

                # Format response
                return f"""**ROI Analysis for {business_type} Business:**

ðŸ“Š **Your Inputs:**
- Platform: {current_platform}
- Monthly Visitors: {monthly_visitors:,}
- Current Conversion Rate: {conversion_rate}%
- Average Order Value: ${order_value:,.0f}
- Current Maintenance Costs: ${maintenance_costs:,.0f}/month

ðŸ’° **Projected Benefits:**
- Cost Savings: ${results['monthly_cost_savings']:,.0f}/month (${results['monthly_cost_savings'] * 12:,.0f}/year)
- Revenue Growth: ${results['monthly_revenue_growth']:,.0f}/month (${results['monthly_revenue_growth'] * 12:,.0f}/year)
- **Total Monthly Benefit: ${results['total_monthly_benefit']:,.0f}/month**
- **Annual Benefit: ${results['annual_benefit']:,.0f}/year**

ðŸ“ˆ **ROI Metrics:**
- Estimated Implementation: ${results['implementation_cost']:,}
- Payback Period: {results['payback_formatted']}
- **3-Year ROI: {results['three_year_roi']:,.0f}%**
- 3-Year Total Benefit: ${results['three_year_benefit']:,.0f}

ðŸ’¡ *Based on Australian 2025 market data. Actual results may vary based on your specific situation.*

Would you like to schedule a consultation to discuss a custom solution for your business?"""

            except Exception as e:
                logger.error(f"Error in calculate_roi_estimate tool: {e}")
                return f"I encountered an error calculating your ROI. Please try our online calculator at /contact or schedule a consultation for a personalized analysis."


        @tool
        def transfer_to_prompt_library() -> str:
            """
            Transfer the user to the Prompt Library (Prompt Engine) product.
            Use this tool when the user asks to:
            - Create, edit, or manage prompts
            - Access the Prompt Library/Engine
            - Use the prompt engineering features
            """
            try:
                # Return a structured signal that the frontend can parse
                # Or simply a helpful link if deep integration isn't ready
                return "TRANSITION: PROMPT_LIBRARY\n\nI'm redirecting you to our Prompt Engine, where the specialized prompt engineering agent can assist you further."
            except Exception as e:
                logger.error(f"Error in transfer_to_prompt_library tool: {e}")
                return "Please visit /prompt-library to access the Prompt Engine."

        return [search_kb, get_pricing, request_consultation, calculate_roi_estimate, transfer_to_prompt_library]

    def _extract_follow_up_questions(self, response_text: str) -> List[str]:
        """
        Extract follow-up questions from the response text.

        The LLM is instructed to include follow-up questions in the format:
        "You might also want to know:\n1. Question 1\n2. Question 2\n3. Question 3"

        Returns:
            List of follow-up questions (empty if none found)
        """
        try:
            # Look for the follow-up questions section
            if "You might also want to know:" in response_text:
                parts = response_text.split("You might also want to know:")
                if len(parts) > 1:
                    questions_section = parts[1].strip()

                    # Extract numbered questions
                    questions = []
                    for line in questions_section.split('\n'):
                        line = line.strip()
                        # Match patterns like "1. Question?" or "1) Question?"
                        if line and (line[0].isdigit() or line.startswith('-')):
                            # Remove numbering and clean up
                            question = line.lstrip('0123456789.-) ').strip()
                            if question:
                                questions.append(question)

                    return questions[:self.config.max_follow_up_questions]  # L2 FIX: Use config

            return []
        except Exception as e:
            logger.warning(f"Failed to extract follow-up questions: {e}")
            return []

    def _generate_fallback_questions(self, page_context: str, message: str) -> List[str]:
        """
        Generate fallback follow-up questions based on page context.
        Used when LLM doesn't provide follow-up questions.

        Args:
            page_context: Current page context (homepage, solutions, pricing, etc.)
            message: User's original message

        Returns:
            List of 3 contextually relevant follow-up questions
        """
        # Context-specific question sets
        question_sets = {
            "homepage": [
                "What services does EthosPrompt offer?",
                "How does the RAG technology work?",
                "What are the pricing plans?"
            ],
            "solutions": [
                "How can EthosPrompt help my business?",
                "What integrations are available?",
                "How do I request a consultation?"
            ],
            "pricing": [
                "What's included in each plan?",
                "Is there a free trial available?",
                "How do I upgrade my plan?"
            ],
            "product": [
                "What are the key features?",
                "How does document intelligence work?",
                "What AI models are supported?"
            ],
            "onboarding": [
                "How long does setup take?",
                "What support is available?",
                "Can I import existing prompts?"
            ],
            "default": [
                "Tell me more about EthosPrompt's features",
                "How do I get started?",
                "What makes EthosPrompt different?"
            ]
        }

        # Get questions for the current context
        questions = question_sets.get(page_context, question_sets["default"])

        return questions

    async def chat(
        self,
        message: str,
        context: Optional[Dict[str, Any]] = None
    ) -> AgentResponse:
        """
        Chat with the marketing agent.

        Args:
            message: User message
            context: Optional context dict with:
                - conversation_id: Conversation ID for checkpointing
                - page_context: Current page context (homepage, solutions, etc.)
                - user_id: User ID (if authenticated)

        Returns:
            AgentResponse with response, sources, and metadata
        """
        mon_ctx = None
        try:
            # Validate input message
            message = validate_input_message(message, self.config)

            # Extract context
            conversation_id = context.get("conversation_id", "default") if context else "default"
            page_context = context.get("page_context", "unknown") if context else "unknown"

            # Start monitoring span
            mon_ctx = _mon.start_request(agent_type="marketing", page_context=page_context)

            # PII-safe logging
            from ..security.pii_detector import redact_pii_from_text, log_safe
            redacted_query = redact_pii_from_text(message)
            log_safe(logger, "INFO", f"Marketing agent chat: '{redacted_query[:50]}...' (page: {page_context})")

            # Track query for drift detection (Phase 3) - non-blocking
            try:
                await self.drift_monitor.track_query(
                    query=redacted_query,
                    conversation_id=conversation_id
                )
            except Exception as drift_error:
                logger.warning(f"Drift tracking failed (non-blocking): {drift_error}")

            # Configure checkpointing
            config = {"configurable": {"thread_id": conversation_id}}

            # ================================================================
            # CONTEXT OPTIMIZATION: Conversation History Pruning
            # Limit context growth by keeping only the last N messages
            # This prevents unbounded memory growth in long conversations
            # ================================================================
            max_history = self.config.history_retention_limit  # Use config instead of magic number

            try:
                # Get current conversation state from checkpointer
                current_state = await self.agent.aget_state(config)
                if current_state and current_state.values:
                    current_messages = current_state.values.get("messages", [])
                    if len(current_messages) > max_history:
                        # Prune old messages, keeping only the most recent
                        pruned_messages = current_messages[-max_history:]
                        logger.info(
                            f"Context optimization: Pruned conversation history from "
                            f"{len(current_messages)} to {len(pruned_messages)} messages"
                        )
                        # Update state with pruned messages
                        await self.agent.aupdate_state(
                            config,
                            {"messages": pruned_messages}
                        )
            except Exception as prune_error:
                # Non-blocking - if pruning fails, continue with full history
                logger.warning(f"History pruning failed (non-blocking): {prune_error}")

            # Invoke agent asynchronously via LangGraph's async interface.
            # This avoids calling synchronous .invoke(), which in turn would
            # call asyncio.run() from within an already running event loop
            # (the LangGraph runner), causing RuntimeError.

            # M1 FIX: Circuit breaker protection for LLM calls
            try:
                if watsonx_circuit_breaker.state.value == "open":
                    logger.warning("Circuit breaker OPEN - using fallback")
                    raise Exception("LLM circuit breaker open")

                result = await self.agent.ainvoke(
                    {"messages": [("user", message)]},
                    config
                )
                # Record success for circuit breaker
                watsonx_circuit_breaker._on_success()
            except Exception as llm_error:
                watsonx_circuit_breaker._on_failure()
                logger.error(f"LLM call failed: {llm_error}")
                # Use fallback response
                fallback = get_context_aware_fallback("llm_error", context)
                return {
                    "response": fallback,
                    "sources": [],
                    "suggested_questions": [],
                    "conversation_id": conversation_id,
                    "metadata": {"fallback": True, "error": str(llm_error)}
                }

            # Extract response
            messages = result.get("messages", [])
            if not messages:
                raise ValueError("No response from agent")

            # Get last AI message
            ai_message = messages[-1]
            # Extract content and ensure it's a string for type safety
            raw_content = ai_message.content if hasattr(ai_message, 'content') else ai_message
            response_text: str = str(raw_content) if raw_content is not None else ""

            # Validate response completeness (detect truncation early)
            word_count = len(response_text.split())
            char_count = len(response_text)
            if word_count < self.config.min_word_warning_threshold:  # L2 FIX: Use config
                logger.warning(
                    f"âš ï¸ Response suspiciously short: {word_count} words, {char_count} chars. "
                    f"Possible truncation. Check max_tokens configuration. "
                    f"Query: '{message[:50]}...'"
                )

            # Check if response indicates tool failure or lack of information
            if any(phrase in response_text.lower() for phrase in [
                "technical issue",
                "trouble retrieving",
                "unable to retrieve",
                "don't have access",
                "no relevant information",
                "error retrieving"
            ]):
                logger.warning("Agent response indicates tool failure, attempting direct KB retrieval")
                # Fallback: directly retrieve from KB
                try:
                    retrieval_results = await marketing_retriever.retrieve(
                        query=message,
                        top_k=5,
                        category_filter=None,
                        use_hybrid=True
                    )
                    if retrieval_results:
                        # Use different variable name to avoid shadowing function parameter 'context'
                        kb_context = marketing_retriever.format_context(retrieval_results, max_tokens=2000)
                        response_text = kb_context
                        logger.info("Fallback KB retrieval successful, using retrieved context")
                except Exception as e:
                    logger.error(f"Fallback KB retrieval failed: {e}")

            # Extract follow-up questions from response
            follow_up_questions = self._extract_follow_up_questions(response_text)

            # Clean response text (remove follow-up questions section if present)
            # NOTE: Must be defined before the fallback logic uses it
            clean_response = response_text
            if "You might also want to know:" in response_text:
                clean_response = response_text.split("You might also want to know:")[0].strip()

            # Only use fallback questions if:
            # 1. LLM didn't provide any AND
            # 2. Response appears to be asking for user input (ends with ?)
            # 3. User hasn't signaled conversation end (exit signal)
            # This allows natural conversation endings when appropriate

            # AUSTRALIAN COMMUNICATION: Check for exit signals FIRST
            # This overrides both LLM-generated AND fallback follow-ups
            from .intent_classifier import is_exit_signal
            user_wants_to_exit = is_exit_signal(message)
            logger.info(f"Exit signal check: message='{message[:30]}...', is_exit={user_wants_to_exit}, followups_before={len(follow_up_questions)}")

            if user_wants_to_exit:
                # User signaled end - skip ALL follow-ups entirely (LLM-generated or fallback)
                follow_up_questions = []
                logger.info("Exit signal detected - clearing all follow-up questions")
            elif not follow_up_questions:
                # Check if response is soliciting user input
                response_ends_with_question = clean_response.rstrip().endswith("?") if clean_response else False
                if response_ends_with_question or page_context == "homepage":
                    follow_up_questions = self._generate_fallback_questions(page_context, message)
                    # Limit to 2 follow-ups for Australian brevity
                    follow_up_questions = follow_up_questions[:2]
                    logger.info(f"Using fallback follow-up questions for page_context: {page_context}")
                else:
                    logger.info("No follow-up questions needed - response is complete")
            else:
                # Limit LLM-generated follow-ups to 2 max for brevity
                follow_up_questions = follow_up_questions[:2]
                logger.info(f"Extracted {len(follow_up_questions)} follow-up questions from LLM response")

            # ================================================================
            # KB GROUNDING VALIDATION (FIX: Activate hallucination detection)
            # Extract KB context from tool messages and validate grounding
            # ================================================================
            kb_context = ""
            for msg in messages:
                if isinstance(msg, ToolMessage):
                    kb_context += msg.content + "\n\n"

            # Initialize grounding score for monitoring
            grounding_score = 1.0  # Default: assume grounded if no KB context to check

            # Check KB grounding if we have substantial context
            if kb_context and len(kb_context.strip()) > 50:
                try:
                    grounding_score = await self._check_hallucination(response_text, kb_context)
                    logger.info(f"âœ“ KB grounding score: {grounding_score:.2f}")

                    # If grounding is low, retrieve KB content directly and reformulate
                    # BIZ-004 FIX: Use config threshold instead of hardcoded 0.6
                    if grounding_score < self.config.grounding_threshold:
                        logger.warning(
                            f"âš ï¸ Response failed KB grounding check (score: {grounding_score:.2f}), "
                            f"retrieving KB content directly for query: '{message[:50]}...'"
                        )
                        # Fallback: retrieve KB content and provide grounded response
                        try:
                            retrieval_results = await marketing_retriever.retrieve(
                                query=message,
                                top_k=5,
                                category_filter=None,
                                use_hybrid=True
                            )
                            if retrieval_results:
                                kb_text = marketing_retriever.format_context(retrieval_results, max_tokens=1500)
                                response_text = f"""Based on our knowledge base:

{kb_text}

For more specific information, I'd recommend visiting our contact page at /contact or scheduling a consultation with our team."""
                                logger.info("âœ“ KB fallback retrieval successful, response reformulated from KB")
                                grounding_score = 1.0  # Mark as grounded since we used direct KB content

                                # Re-clean the response after reformulation
                                clean_response = response_text.strip()
                        except Exception as fallback_error:
                            logger.error(f"KB fallback retrieval failed: {fallback_error}")
                            # Keep original response but log the issue
                except Exception as grounding_error:
                    logger.warning(f"KB grounding check failed (non-blocking): {grounding_error}")
                    grounding_score = 0.5  # Uncertain
            # ================================================================


            # ================================================================
            # PERF-001 FIX: Extract sources from tool messages instead of redundant KB call
            # The tool already retrieved KB content - reuse it for source citations
            # This saves ~100ms latency and reduces API costs
            # ================================================================
            sources = []
            try:
                # Extract source info from kb_context that was already retrieved
                if kb_context and len(kb_context.strip()) > 50:
                    # Parse sources from formatted context (format: [N] Title: Content)
                    import re
                    source_pattern = r'\[(\d+)\]\s+([^:]+):'
                    matches = re.findall(source_pattern, kb_context)
                    for idx, title in matches[:3]:  # Limit to 3 sources
                        sources.append({
                            "index": int(idx),
                            "title": title.strip(),
                            "category": "marketing_kb",
                            "page": "kb"
                        })
                    logger.info(f"Extracted {len(sources)} sources from tool context (no redundant KB call)")
                else:
                    logger.debug("No KB context available for source extraction")
            except Exception as e:
                logger.warning(f"Source extraction failed: {e}")
                sources = []

            # Extract token usage from LLM response metadata (if available)
            token_usage = None
            estimated_cost_usd = None
            tool_calls_count = 0

            try:
                # Count tool calls from messages
                for msg in messages:
                    if hasattr(msg, 'tool_calls') and msg.tool_calls:
                        tool_calls_count += len(msg.tool_calls)

                # Try to extract token usage from response metadata
                # LangChain/OpenAI responses may include usage_metadata
                if hasattr(ai_message, 'response_metadata'):
                    usage = ai_message.response_metadata.get('token_usage') or ai_message.response_metadata.get('usage')
                    if usage:
                        token_usage = {
                            "prompt_tokens": usage.get("prompt_tokens", 0),
                            "completion_tokens": usage.get("completion_tokens", 0),
                            "total_tokens": usage.get("total_tokens", 0)
                        }

                        # Estimate cost for free model (z-ai/glm-4.5-air:free is $0)
                        # For paid models, use pricing: ~$0.50/1M input, ~$1.50/1M output tokens
                        if ":free" not in self.config.model_name.lower():
                            input_cost = (token_usage["prompt_tokens"] / 1_000_000) * 0.50
                            output_cost = (token_usage["completion_tokens"] / 1_000_000) * 1.50
                            estimated_cost_usd = input_cost + output_cost
                        else:
                            estimated_cost_usd = 0.0
            except Exception as e:
                logger.warning(f"Failed to extract token usage: {e}")

            # Track RAG quality metrics (Phase 3) - non-blocking
            try:
                # Extract retrieved documents if available
                tools_output = result.get("tools_output", [])
                retrieved_docs: List[Dict[str, Any]] = []
                for output in tools_output:
                    if isinstance(output, dict) and "results" in output:
                        retrieved_docs.extend(output["results"])

                # Use log_retrieval_quality (the actual method name in RAGQualityMetrics)
                await self.rag_metrics.log_retrieval_quality(
                    query=redacted_query,
                    results=retrieved_docs,
                    relevance_labels=None,  # No manual labels available
                    conversation_id=conversation_id
                )
            except Exception as metrics_error:
                logger.warning(f"RAG metrics logging failed (non-blocking): {metrics_error}")

            # FIX #1: Clean sources to remove scores before user sees them
            clean_sources = self._clean_sources(sources) if sources else []

            # PERF-003 FIX: Validate response quality before returning
            try:
                validation_result = validate_marketing_response(clean_response)
                if not validation_result["validation_passed"]:
                    logger.warning(f"Response validation issues: {validation_result['validation_errors']}")
                    # Log validation metrics
                    _mon.record_validation(
                        agent_type="marketing",
                        passed=False,
                        errors=validation_result["validation_errors"],
                        word_count=validation_result["word_count"]
                    )
                else:
                    _mon.record_validation(
                        agent_type="marketing",
                        passed=True,
                        errors=[],
                        word_count=validation_result["word_count"]
                    )
            except Exception as val_error:
                logger.debug(f"Response validation failed (non-blocking): {val_error}")

            # Build response
            agent_response: AgentResponse = {
                "response": clean_response,
                "sources": clean_sources,
                "suggested_questions": follow_up_questions,
                "conversation_id": conversation_id,
                "metadata": {
                    "agent": "marketing",
                    "page_context": page_context,
                    "model": self.llm.model_name,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "response_length": len(clean_response.split()),  # Word count for monitoring
                    "token_usage": token_usage,
                    "estimated_cost_usd": estimated_cost_usd,
                    "tool_calls": tool_calls_count
                }
            }

            logger.info(
                f"Marketing agent response generated ({len(clean_response)} chars, {len(clean_response.split())} words, "
                f"{len(follow_up_questions)} follow-up questions, {tool_calls_count} tool calls, "
                f"{token_usage.get('total_tokens', 0) if token_usage else 0} tokens)"
            )

            # ================================================================
            # KB GROUNDING MONITORING (Track grounding quality)
            # ================================================================
            # Log KB grounding score if KB context was used
            if kb_context and len(kb_context.strip()) > 50:
                try:
                    # Log grounding score for monitoring
                    logger.info(f"ðŸ“Š KB grounding score for this response: {grounding_score:.2f}")

                    # Alert if grounding is low
                    if grounding_score < self.config.grounding_threshold:
                        logger.warning(
                            f"âš ï¸ LOW KB GROUNDING DETECTED: score={grounding_score:.2f}, "
                            f"query='{message[:50]}...', page_context={page_context}"
                        )
                        # Record as error for monitoring dashboards
                        try:
                            _mon.record_error(
                                agent_type="marketing",
                                error_type="LowKBGrounding",
                                details=f"Grounding score: {grounding_score:.2f}",
                                page_context=page_context
                            )
                        except Exception as mon_error:
                            logger.debug(f"Monitoring error recording failed: {mon_error}")
                except Exception as grounding_log_error:
                    logger.debug(f"KB grounding logging failed (non-blocking): {grounding_log_error}")
            # ================================================================


            # End monitoring span
            try:
                _mon.end_request(mon_ctx, success=True, extra_metadata={
                    "response_words": len(clean_response.split()),
                    "sources_count": len(sources),
                    "tool_calls": tool_calls_count,
                    "total_tokens": token_usage.get("total_tokens", 0) if token_usage else 0
                })
            except Exception as e:
                # Monitoring must not break user flow
                logger.debug(f"Monitoring end_request failed: {e}")

            return agent_response

        except Exception as e:
            logger.error(f"Error in marketing agent chat: {e}")
            try:
                _mon.record_error(agent_type="marketing", error_type=type(e).__name__, details=str(e), page_context=context.get("page_context") if context else None)
                if mon_ctx is not None:
                    _mon.end_request(mon_ctx, success=False)
            except Exception as mon_e:
                logger.debug(f"Monitoring error recording failed: {mon_e}")
            raise

    async def chat_stream(
        self,
        message: str,
        context: Optional[Dict[str, Any]] = None
    ):
        """
        Stream chat responses from the marketing agent.

        Args:
            message: User message
            context: Optional context dict

        Yields:
            Chunks of the response
        """
        try:
            # Extract context
            conversation_id = context.get("conversation_id", "default") if context else "default"

            # Configure checkpointing
            config = {"configurable": {"thread_id": conversation_id}}

            # Stream agent responses
            # Use "updates" mode to ensure we get the final response after tool execution
            # Note: Watsonx API returns full response in one chunk (no token-streaming),
            # so responses will arrive as single chunks. This is a Watsonx limitation.
            async for update in self.agent.astream(
                {"messages": [("user", message)]},
                config,
                stream_mode="updates"
            ):
                # Extract content from state updates
                # Update format: {'node_name': {'messages': [...]}}
                for node_update in update.values():
                    if isinstance(node_update, dict) and "messages" in node_update:
                        messages = node_update["messages"]
                        if not isinstance(messages, list):
                            messages = [messages]

                        for msg in messages:
                            # FIX #4 (CRITICAL): Only yield AI messages, NOT ToolMessages
                            # This prevents raw RAG context (with markers/scores) from leaking to users
                            if isinstance(msg, AIMessage) and hasattr(msg, 'content') and msg.content:
                                # Check if it's a tool call (which might have empty content but tool_calls)
                                if not (hasattr(msg, 'tool_calls') and msg.tool_calls and not msg.content):
                                    yield {"content": msg.content}

        except Exception as e:
            logger.error(f"Error in marketing agent stream: {e}")
            raise
_marketing_agent_instance = None

def get_marketing_agent(db=None, openrouter_api_key: Optional[str] = None) -> MarketingAgent:
    """
    Get or create the global marketing agent instance (singleton pattern).

    Performance optimization: Reuses the same agent instance across all requests
    to avoid repeated initialization of LLM, tools, and checkpointer.

    Args:
        db: Firestore database instance
        openrouter_api_key: OpenRouter API key

    Returns:
        MarketingAgent instance (singleton)
    """
    global _marketing_agent_instance

    if _marketing_agent_instance is None:
        logger.info("Creating new marketing agent instance (singleton)")
        _marketing_agent_instance = MarketingAgent(db, openrouter_api_key)
    else:
        logger.debug("Reusing existing marketing agent instance (singleton)")

    return _marketing_agent_instance
