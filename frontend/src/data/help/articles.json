[
  {
    "id": "quick-start-guide",
    "slug": "quick-start-guide",
    "title": "Quick Start Guide",
    "excerpt": "Get up and running with RAG Prompt Library in just 5 minutes",
    "content": "# Quick Start Guide\n\nWelcome to RAG Prompt Library! This guide will get you started in just 5 minutes.\n\n## What You'll Learn\n- How to create your first account\n- Setting up your workspace\n- Creating your first prompt with variables\n- Uploading documents for RAG\n- Running your first AI-powered prompt execution\n\n## Prerequisites\n- A modern web browser (Chrome, Firefox, Safari, or Edge)\n- Internet connection\n- Email address for account creation\n- Optional: Documents you want to use with RAG (PDF, DOCX, TXT, MD)\n\n## Step 1: Create Your Account\n\nSign up using your email or Google account. Navigate to the authentication page and choose your preferred sign-in method.\n\n**Tips:**\n- Use a business email for team features\n- Enable 2FA for enhanced security\n- Verify your email to unlock all features\n\n## Step 2: Set Up Your Workspace\n\nCreate your first workspace to organize your prompts and documents. Workspaces help you separate different projects or teams.\n\n**Tips:**\n- Give your workspace a descriptive name\n- You can create multiple workspaces\n- Invite team members later from workspace settings\n\n## Step 3: Create Your First Prompt\n\nNavigate to the Prompts page and click \"Create Prompt\". Add a title, description, and your prompt content with variables using `{{variable_name}}` syntax.\n\n**Example:**\n```\nHello {{customer_name}}, thank you for your inquiry about {{product_name}}.\n```\n\n**Tips:**\n- Use clear variable names like `{{customer_name}}` or `{{product_description}}`\n- Test your prompt with sample data\n- Save frequently to avoid losing work\n\n## Step 4: Upload Documents (Optional)\n\nIf you want to use RAG, upload your knowledge base documents from the Documents page. Supported formats: PDF, DOCX, TXT, MD.\n\n**Tips:**\n- Organize documents by topic or project\n- Larger documents may take longer to process\n- You can upload multiple files at once\n\n## Step 5: Execute Your Prompt\n\nClick \"Execute\" on your prompt, fill in the variable values, optionally enable RAG to search your documents, and run the AI generation.\n\n**Tips:**\n- Start with a simple prompt to test\n- Review the execution history\n- Adjust model parameters for better results\n\n## Next Steps\n\n- Explore advanced prompting techniques\n- Learn about RAG configuration\n- Set up API integrations\n- Invite team members to collaborate",
    "category": "getting-started",
    "subcategory": "quick-start",
    "tags": ["beginner", "setup", "tutorial", "onboarding", "rag"],
    "difficulty": "beginner",
    "lastUpdated": "2025-01-15",
    "featured": true,
    "steps": [
      {
        "id": "step-1",
        "title": "Create Your Account",
        "content": "Sign up using your email or Google account. Navigate to the authentication page and choose your preferred sign-in method.",
        "tips": [
          "Use a business email for team features",
          "Enable 2FA for enhanced security",
          "Verify your email to unlock all features"
        ]
      },
      {
        "id": "step-2",
        "title": "Set Up Your Workspace",
        "content": "Create your first workspace to organize your prompts and documents. Workspaces help you separate different projects or teams.",
        "tips": [
          "Give your workspace a descriptive name",
          "You can create multiple workspaces",
          "Invite team members later from workspace settings"
        ]
      },
      {
        "id": "step-3",
        "title": "Create Your First Prompt",
        "content": "Navigate to the Prompts page and click \"Create Prompt\". Add a title, description, and your prompt content with variables using {{variable_name}} syntax.",
        "tips": [
          "Use clear variable names like {{customer_name}} or {{product_description}}",
          "Test your prompt with sample data",
          "Save frequently to avoid losing work"
        ]
      },
      {
        "id": "step-4",
        "title": "Upload Documents (Optional)",
        "content": "If you want to use RAG, upload your knowledge base documents from the Documents page. Supported formats: PDF, DOCX, TXT, MD.",
        "tips": [
          "Organize documents by topic or project",
          "Larger documents may take longer to process",
          "You can upload multiple files at once"
        ]
      },
      {
        "id": "step-5",
        "title": "Execute Your Prompt",
        "content": "Click \"Execute\" on your prompt, fill in the variable values, optionally enable RAG to search your documents, and run the AI generation.",
        "tips": [
          "Start with a simple prompt to test",
          "Review the execution history",
          "Adjust model parameters for better results"
        ]
      }
    ]
  },
  {
    "id": "creating-first-prompt",
    "slug": "creating-first-prompt",
    "title": "Creating Your First Prompt",
    "excerpt": "Learn the fundamentals of prompt creation with variables and best practices",
    "content": "# Creating Your First Prompt\n\nMaster the art of prompt creation with this comprehensive guide.\n\n## Understanding Prompts\n\nPrompts are reusable templates that generate dynamic content using AI. They allow you to create consistent, high-quality outputs by defining a structure with placeholders (variables) that can be filled in at execution time.\n\n## Key Concepts\n\n### Variables\n\nUse `{{variable_name}}` syntax to create placeholders in your prompts. When executing the prompt, you'll be asked to provide values for these variables.\n\n**Example:**\n```\nWrite a professional email to {{recipient_name}} about {{topic}}.\nThe tone should be {{tone}} and include {{key_points}}.\n```\n\n> [!TIP]\n> For comprehensive variable syntax, patterns, and advanced techniques, see [Prompt Variables Syntax and Patterns](/dashboard/help/core-features/variables-syntax).\n\n### Context\n\nProvide clear instructions to guide the AI. The more specific you are, the better the results.\n\n**Good:**\n```\nWrite a 200-word product description for {{product_name}}, highlighting its {{key_features}} and targeting {{target_audience}}. Use a professional yet friendly tone.\n```\n\n**Bad:**\n```\nWrite about {{product_name}}.\n```\n\n### Examples\n\nInclude sample outputs to help the AI understand the desired format and style.\n\n## Variables Best Practices\n\n### 1. Use Descriptive Names\n\nVariable names should be self-documenting:\n\n**✅ Good:**\n```\n{{customer_first_name}}\n{{product_category}}\n{{delivery_address}}\n{{order_total_amount}}\n```\n\n**❌ Bad:**\n```\n{{n}}\n{{p}}\n{{addr}}\n{{amt}}\n```\n\n### 2. Follow Naming Conventions\n\nChoose one style and stick to it:\n\n**snake_case (Recommended):**\n```\n{{customer_name}}\n{{order_date}}\n{{product_id}}\n```\n\n**camelCase:**\n```\n{{customerName}}\n{{orderDate}}\n{{productId}}\n```\n\n### 3. Group Related Variables\n\nUse consistent prefixes for related data:\n\n```\n{{user_name}}\n{{user_email}}\n{{user_role}}\n\n{{order_id}}\n{{order_date}}\n{{order_total}}\n```\n\n### 4. Validate Variable Names\n\n**Valid:**\n```\n{{customer_name}}        ✓\n{{product_id}}           ✓\n{{order_date_2024}}      ✓\n{{_private_var}}         ✓\n```\n\n**Invalid:**\n```\n{{customer name}}        ✗ Contains space\n{{product-id}}           ✗ Contains hyphen\n{{2024_order}}           ✗ Starts with number\n{{user@email}}           ✗ Contains special character\n```\n\n### 5. Limit Variable Count\n\nKeep prompts manageable:\n\n```\n✅ 3-7 variables: Easy to use\n⚠️ 8-12 variables: Acceptable\n❌ 13+ variables: Consider restructuring\n```\n\n**If you need many variables:**\n- Group related data into structured variables\n- Use RAG for context instead of variables\n- Split into multiple prompts\n\n### 6. Document Expected Format\n\nAdd comments or descriptions:\n\n```\nCustomer feedback (1-5 star rating): {{rating}}\nFeedback text (max 500 chars): {{feedback_text}}\nProduct ID (format: PROD-XXXX): {{product_id}}\nOrder date (YYYY-MM-DD): {{order_date}}\n```\n\n### 7. Provide Default Values (in description)\n\nHelp users understand what to enter:\n\n```\nTone (professional, friendly, or casual): {{tone}}\nWord count (default: 200): {{word_count}}\nTarget audience (e.g., developers, marketers): {{audience}}\n```\n\n### 8. Test with Edge Cases\n\nTest your variables with:\n- Empty strings\n- Very long text (near limits)\n- Special characters\n- Unicode characters\n- Numbers vs strings\n- JSON data\n\n## Best Practices\n\n1. **Be Specific**: Clearly define what you want the AI to generate\n2. **Use Descriptive Variable Names**: `{{customer_name}}` is better than `{{x}}`\n3. **Test Thoroughly**: Try your prompt with different variable values\n4. **Iterate**: Refine your prompt based on the outputs you receive\n5. **Document**: Add descriptions to help others understand your prompt's purpose\n6. **Follow Variable Conventions**: See [Variables Best Practices](#variables-best-practices) above\n\n## Common Patterns\n\n### Email Templates\n```\nSubject: {{subject}}\n\nDear {{recipient_name}},\n\n{{opening_paragraph}}\n\n{{main_content}}\n\n{{closing}}\n\nBest regards,\n{{sender_name}}\n```\n\n### Content Generation\n```\nCreate a {{content_type}} about {{topic}} for {{target_audience}}.\n\nKey points to cover:\n- {{point_1}}\n- {{point_2}}\n- {{point_3}}\n\nTone: {{tone}}\nLength: {{word_count}} words\n```\n\n### Data Analysis\n```\nAnalyze the following data: {{data}}\n\nProvide insights on:\n1. {{insight_1}}\n2. {{insight_2}}\n3. {{insight_3}}\n\nFormat the response as a {{format}}.\n```\n\n## Next Steps\n\n- [Prompt Variables Syntax and Patterns](/dashboard/help/core-features/variables-syntax) - Comprehensive variable guide\n- [Template Library Usage](/dashboard/help/core-features/template-library) - Browse pre-built templates\n- [Prompt Quality Tools](/dashboard/help/core-features/prompt-quality-tools) - Analyze and improve prompts",
    "category": "core-features",
    "subcategory": "prompts",
    "tags": ["prompts", "creation", "variables", "templates", "beginner"],
    "difficulty": "beginner",
    "lastUpdated": "2025-01-14",
    "featured": true,
    "faqs": [
      {
        "id": "faq-1",
        "question": "What are prompt variables?",
        "answer": "Variables are placeholders in your prompts that get replaced with actual values when executed. Use {{variable_name}} syntax to define them."
      },
      {
        "id": "faq-2",
        "question": "How many variables can I use?",
        "answer": "There's no strict limit, but we recommend keeping it under 10 variables per prompt for better usability and maintainability."
      },
      {
        "id": "faq-3",
        "question": "Can I use the same variable multiple times?",
        "answer": "Yes! You can reference the same variable multiple times in your prompt, and it will be replaced with the same value everywhere."
      }
    ]
  },
  {
    "id": "document-upload-guide",
    "slug": "document-upload-guide",
    "title": "Document Upload & Management Guide",
    "excerpt": "Learn how to upload, organize, and manage documents for RAG-powered prompts",
    "content": "# Document Upload & Management Guide\n\n## Overview\n\nUpload your knowledge base documents to enable RAG (Retrieval-Augmented Generation) in your prompts. The system will automatically process, chunk, and index your documents for semantic search.\n\n> [!TIP]\n> Well-organized, properly chunked documents are the foundation of effective RAG. Take time to understand chunking before uploading large document libraries.\n\n## Supported File Formats\n\n- **PDF**: Portable Document Format (.pdf)\n- **Word Documents**: Microsoft Word (.docx)\n- **Text Files**: Plain text (.txt)\n- **Markdown**: Markdown files (.md)\n\n## Upload Process\n\n1. Navigate to the Documents page from the dashboard\n2. Click \"Upload Documents\" button\n3. Select one or more files (max 10MB per file)\n4. Wait for processing to complete\n5. Documents are now available for RAG queries\n\n## Document Processing\n\n- **Chunking**: Documents are split into semantic chunks (512-1024 tokens)\n- **Embedding**: Each chunk is converted to vector embeddings\n- **Indexing**: Embeddings are stored for fast semantic search\n- **Processing Time**: Typically 10-30 seconds per document\n\n## Chunking 101\n\n### What is Chunking?\n\nChunking divides large documents into smaller, manageable pieces that can be:\n- Searched semantically\n- Retrieved and injected into prompts\n- Processed within token limits\n\n### Why Chunk Documents?\n\n**1. Token Limits:**\nAI models have context window limits (4K-128K tokens). Chunking allows you to retrieve only relevant sections instead of entire documents.\n\n**2. Relevance:**\nSmaller chunks = more precise retrieval. Instead of retrieving a 50-page manual, retrieve only the 3 paragraphs that answer the question.\n\n**3. Cost:**\nOnly pay for tokens you actually use. Retrieving 5 chunks (2,500 tokens) costs less than sending entire documents (50,000+ tokens).\n\n**4. Performance:**\nFaster embedding generation and search with smaller chunks.\n\n### Chunk Size Basics\n\n**Default Settings:**\n```\nChunk Size: 512 tokens (~380 words)\nChunk Overlap: 50 tokens (~38 words)\nStrategy: Semantic (respects paragraphs)\n```\n\n**Chunk Size Trade-offs:**\n\n**Small Chunks (256-512 tokens):**\n- ✅ Precise retrieval\n- ✅ Lower cost per chunk\n- ❌ May lose context\n\n**Medium Chunks (512-1024 tokens):**\n- ✅ Good balance (recommended)\n- ✅ Preserves context\n- ⚠️ Moderate cost\n\n**Large Chunks (1024-2048 tokens):**\n- ✅ Maximum context\n- ❌ Less precise retrieval\n- ❌ Higher cost\n\n### Chunk Overlap\n\nOverlap ensures important information at chunk boundaries isn't lost.\n\n**Example Without Overlap:**\n```\nChunk 1: \"...the product features include advanced\"\nChunk 2: \"analytics and real-time reporting...\"\n```\nQuery: \"What analytics features?\" → May miss the connection\n\n**Example With Overlap (50 tokens):**\n```\nChunk 1: \"...the product features include advanced analytics and\"\nChunk 2: \"advanced analytics and real-time reporting capabilities...\"\n```\nQuery: \"What analytics features?\" → Both chunks contain relevant context\n\n**Recommended Overlap:**\n- Minimum: 10% of chunk size\n- Recommended: 10-20% of chunk size\n- Maximum: 25% of chunk size\n\n### Chunking Strategies\n\n**Semantic Chunking (Default):**\n- Respects paragraph and section boundaries\n- Preserves meaning and context\n- Best for structured documents\n\n**Fixed-Size Chunking:**\n- Equal-sized chunks by token count\n- Simple and predictable\n- May split mid-sentence\n\n**Hybrid Chunking:**\n- Combines semantic boundaries with size constraints\n- Best of both worlds\n- Recommended for mixed document types\n\nFor detailed chunking configuration, see [Chunking and Retrieval Configuration](/dashboard/help/core-features/rag-chunking-config).\n\n## Best Practices\n\n### Document Preparation\n\n**1. Use Descriptive File Names:**\n```\n✅ \"Product_Return_Policy_2024.pdf\"\n✅ \"Customer_Support_FAQ_v2.docx\"\n❌ \"document1.pdf\"\n❌ \"untitled.docx\"\n```\n\n**2. Organize by Topic:**\n```\nFolder Structure:\n├── Policies/\n│   ├── Return_Policy.pdf\n│   └── Privacy_Policy.pdf\n├── Manuals/\n│   ├── Product_Manual_A.pdf\n│   └── Product_Manual_B.pdf\n└── FAQs/\n    └── Customer_FAQ.pdf\n```\n\n**3. Keep Documents Focused:**\n```\n✅ One topic per document\n✅ Clear structure with headings\n✅ Consistent formatting\n❌ Multiple unrelated topics\n❌ Poor formatting\n```\n\n**4. Update Regularly:**\n```\n- Remove outdated documents\n- Upload new versions\n- Archive old versions\n- Track document versions\n```\n\n**5. Optimize for Search:**\n```\n- Use clear headings\n- Include keywords naturally\n- Add table of contents\n- Use consistent terminology\n```\n\n## Troubleshooting\n\n### File Too Large\n\nIf your file exceeds 10MB, try:\n- Splitting it into smaller documents\n- Compressing images in PDFs\n- Converting to plain text format\n- Removing unnecessary pages\n\n### Processing Failed\n\nCommon causes:\n- Corrupted file\n- Unsupported format\n- Network interruption\n- File contains only images (no text)\n\nSolution: Re-upload the file or convert to a different format.\n\n### Poor Search Results\n\nIf RAG retrieves irrelevant chunks:\n- Check document quality (clear text, good structure)\n- Verify chunking settings are appropriate\n- Add more relevant documents\n- Use more specific queries\n- See [Chunking and Retrieval Configuration](/dashboard/help/core-features/rag-chunking-config)\n\n## Next Steps\n\n- [Chunking and Retrieval Configuration](/dashboard/help/core-features/rag-chunking-config) - Optimize chunking for your use case\n- [RAG Execution Basics](/dashboard/help/core-features/rag-execution-basics) - Execute RAG-enabled prompts\n- [Understanding Context Injection](/dashboard/help/core-features/rag-context-preview) - Preview retrieved chunks",
    "category": "core-features",
    "subcategory": "documents",
    "tags": ["documents", "upload", "rag", "knowledge-base", "pdf", "processing"],
    "difficulty": "beginner",
    "lastUpdated": "2025-01-15",
    "featured": true,
    "faqs": [
      {
        "id": "file-size-limit",
        "question": "What is the maximum file size for uploads?",
        "answer": "The maximum file size is 10MB per file. For larger documents, consider splitting them into smaller files or compressing them."
      },
      {
        "id": "processing-time",
        "question": "How long does document processing take?",
        "answer": "Processing typically takes 10-30 seconds per document, depending on size and complexity. You can continue working while documents process in the background."
      }
    ]
  },
  {
    "id": "rag-execution-basics",
    "slug": "rag-execution-basics",
    "title": "RAG-Enabled Prompt Execution",
    "excerpt": "Master RAG execution to enhance your prompts with document knowledge",
    "content": "# RAG-Enabled Prompt Execution\n\n## What is RAG?\n\nRetrieval-Augmented Generation (RAG) combines your uploaded documents with AI generation to provide accurate, context-aware responses based on your knowledge base.\n\n> [!TIP]\n> RAG allows AI to answer questions using YOUR data, not just its training data. This ensures accurate, up-to-date, and company-specific responses.\n\n## How RAG Works\n\n### The RAG Pipeline\n\n1. **Query**: Your prompt variables are used to search your documents\n2. **Retrieval**: The system finds the most relevant document chunks\n3. **Augmentation**: Retrieved context is added to your prompt\n4. **Generation**: The AI generates a response using both the prompt and retrieved context\n\n### Context Assembly Process\n\nWhen RAG is enabled, EthosPrompt:\n\n1. **Extracts Query**: Identifies search terms from your prompt and variables\n2. **Generates Embedding**: Converts query to vector representation\n3. **Searches Documents**: Finds semantically similar chunks in your document library\n4. **Ranks Results**: Scores chunks by relevance (0.0-1.0)\n5. **Retrieves Top K**: Selects the most relevant chunks (default: 5)\n6. **Assembles Context**: Combines chunks into structured context\n7. **Injects into Prompt**: Adds context before sending to AI model\n8. **Generates Response**: AI uses both prompt and context to respond\n\n**Example Context Assembly:**\n```\nOriginal Prompt:\n\"What is our return policy for {{product_type}}?\"\n\nAssembled Context:\n---\nRetrieved Context (3 chunks):\n\n[Chunk 1 - Relevance: 0.92]\nSource: Return Policy Guide (Page 3)\nOur return policy allows returns within 30 days of purchase...\n\n[Chunk 2 - Relevance: 0.87]\nSource: Product Guidelines (Page 12)\nElectronics have a 14-day return window...\n\n[Chunk 3 - Relevance: 0.81]\nSource: Terms of Service (Page 8)\nReturns must be in original packaging...\n---\n\nUser Query: What is our return policy for electronics?\n\nPlease answer based on the context above.\n```\n\n### Relevance Scoring\n\nEach retrieved chunk receives a relevance score:\n\n**Score Interpretation:**\n- **0.90-1.00**: Excellent match (highly relevant)\n- **0.80-0.89**: Good match (relevant)\n- **0.70-0.79**: Fair match (somewhat relevant)\n- **0.60-0.69**: Weak match (marginally relevant)\n- **<0.60**: Poor match (likely not relevant)\n\n> [!NOTE]\n> Only chunks above the similarity threshold (default: 0.70) are retrieved. If no chunks meet the threshold, RAG returns no context.\n\n## Enabling RAG\n\n### Step-by-Step\n\n1. Create or select a prompt\n2. Click \"Execute\"\n3. Toggle \"Enable RAG\" option\n4. Select which documents to search (or search all)\n5. Fill in prompt variables\n6. Click \"Run\"\n\n### Document Selection\n\n**Search All Documents:**\n- Searches entire document library\n- Best for general queries\n- May retrieve less relevant results\n\n**Search Specific Documents:**\n- Limit search to selected documents\n- Better precision\n- Faster retrieval\n- Recommended for focused queries\n\n**Filter by Metadata:**\n- Filter by document type, date, tags\n- Narrow search scope\n- Improve relevance\n\n## RAG Parameters\n\n### Top K Results\n\n**Number of chunks to retrieve:**\n- **Default**: 5 chunks\n- **Range**: 1-20 chunks\n- **Recommendation**: 3-5 for most use cases\n\n**Trade-offs:**\n```\nFewer chunks (1-3):\n✓ Lower cost\n✓ Faster retrieval\n✗ May miss information\n\nMore chunks (10-20):\n✓ More comprehensive\n✗ Higher cost\n✗ May include noise\n```\n\n### Similarity Threshold\n\n**Minimum relevance score (0.0-1.0):**\n- **Default**: 0.70\n- **Strict**: 0.85+ (only highly relevant)\n- **Lenient**: 0.60+ (include marginal matches)\n\n**When to adjust:**\n```\nIncrease threshold (0.80+):\n- Need high precision\n- Avoid false positives\n- Have many documents\n\nDecrease threshold (0.60+):\n- Need high recall\n- Few relevant documents\n- Exploratory queries\n```\n\n### Search Mode\n\n**Semantic Search:**\n- Uses AI embeddings\n- Understands meaning, not just keywords\n- Best for natural language queries\n\n**Keyword Search (BM25):**\n- Traditional keyword matching\n- Fast and efficient\n- Best for exact term matching\n\n**Hybrid Search (Recommended):**\n- Combines semantic + keyword\n- Best of both worlds\n- Default: 70% semantic, 30% keyword\n\nFor more details, see [Chunking and Retrieval Configuration](/dashboard/help/core-features/rag-chunking-config).\n\n## RAG Limits\n\n### Context Window Limits\n\nRetrieved context must fit within model limits:\n\n```\nGPT-4 Turbo: 128K tokens\nClaude 3.5: 200K tokens\nGemini 1.5 Pro: 1M tokens\n\nTypical RAG context: 2,000-4,000 tokens (5 chunks × 500 tokens)\n```\n\n**If context exceeds limits:**\n- Reduce Top K (retrieve fewer chunks)\n- Use smaller chunk sizes\n- Switch to larger context model\n- Summarize chunks before injection\n\n### Token Cost Impact\n\nRAG increases input tokens:\n\n```\nWithout RAG:\nPrompt: 100 tokens\nCost: $0.001 (GPT-4 Turbo)\n\nWith RAG (5 chunks):\nPrompt: 100 tokens\nContext: 2,500 tokens\nTotal: 2,600 tokens\nCost: $0.026 (26x higher)\n```\n\n**Cost optimization:**\n- Use fewer chunks when possible\n- Filter documents before retrieval\n- Use cheaper models for simple RAG queries\n- Cache frequently used contexts\n\n## Best Practices\n\n### 1. Use Specific Queries\n\n**❌ Vague:**\n```\n\"Tell me about products\"\n```\n\n**✅ Specific:**\n```\n\"What are the technical specifications for {{product_name}}?\"\n```\n\n### 2. Preview Retrieved Context\n\nBefore executing:\n1. Click \"Preview Context\"\n2. Review retrieved chunks\n3. Verify relevance scores\n4. Adjust parameters if needed\n\nSee [Understanding Context Injection](/dashboard/help/core-features/rag-context-preview) for details.\n\n### 3. Optimize Chunk Count\n\n```\nStart with 5 chunks\nIf missing info: Increase to 7-10\nIf too much noise: Decrease to 3\n```\n\n### 4. Monitor Costs\n\nTrack RAG costs:\n```\nRAG-enabled executions: $0.03 average\nNon-RAG executions: $0.005 average\nRAG overhead: 6x cost increase\n```\n\n### 5. Combine with Good Prompts\n\nRAG enhances prompts, doesn't replace them:\n\n```\n❌ \"Answer this: {{question}}\"\n✅ \"Based on the retrieved context, provide a detailed answer to: {{question}}\n    Include specific references and page numbers.\"\n```\n\n## Example Use Cases\n\n### Customer Support\n\n```\nPrompt: \"How do I {{action}} for {{product}}?\"\nRAG: Searches product manuals, FAQs, support docs\nResult: Accurate, product-specific instructions\n```\n\n### Policy Questions\n\n```\nPrompt: \"What is our policy on {{topic}}?\"\nRAG: Searches policy documents, handbooks\nResult: Official policy with citations\n```\n\n### Data Analysis\n\n```\nPrompt: \"Analyze trends in {{dataset}} for {{time_period}}\"\nRAG: Searches uploaded reports, data files\nResult: Analysis based on actual company data\n```\n\n## Troubleshooting\n\n### No Relevant Chunks Found\n\n**Symptoms:** \"No context retrieved\" message\n\n**Solutions:**\n- Lower similarity threshold (try 0.60)\n- Rephrase query to match document language\n- Verify documents are uploaded and processed\n- Check document filters aren't too restrictive\n\n### Low Relevance Scores\n\n**Symptoms:** All chunks below 0.70\n\n**Solutions:**\n- Use more specific queries\n- Add more relevant documents\n- Adjust chunking strategy (see [Chunking Configuration](/dashboard/help/core-features/rag-chunking-config))\n- Try hybrid search instead of semantic-only\n\n### Context Too Large\n\n**Symptoms:** \"Context exceeds token limit\" error\n\n**Solutions:**\n- Reduce Top K (try 3 instead of 5)\n- Use model with larger context window\n- Summarize chunks before injection\n\n## Next Steps\n\n- [Chunking and Retrieval Configuration](/dashboard/help/core-features/rag-chunking-config) - Optimize RAG performance\n- [Understanding Context Injection](/dashboard/help/core-features/rag-context-preview) - Preview and interpret retrieved chunks\n- [Document Upload Guide](/dashboard/help/core-features/document-upload-guide) - Upload and process documents for RAG",
    "category": "core-features",
    "subcategory": "rag",
    "tags": ["rag", "execution", "retrieval", "semantic-search", "ai", "generation"],
    "difficulty": "intermediate",
    "lastUpdated": "2025-01-15",
    "featured": true,
    "relatedArticles": ["rag-chunking-config", "rag-context-preview", "document-upload-guide"],
    "prerequisites": ["document-upload-guide"]
  },
  {
    "id": "troubleshooting-guide",
    "slug": "troubleshooting-guide",
    "title": "Troubleshooting Common Issues",
    "excerpt": "Solutions to frequently encountered problems and error messages",
    "content": "# Troubleshooting Common Issues\n\n## Authentication Errors\n\n### Error: \"Invalid API Key\"\n\n**Cause**: Your API key is incorrect or expired.\n\n**Solution**:\n1. Check your API key in the dashboard settings\n2. Regenerate if necessary\n3. Update your environment variables\n\n## Performance Issues\n\n### Slow Response Times\n\n**Causes**: Large documents, complex prompts, or network issues.\n\n**Solutions**:\n- Optimize document chunking (use smaller documents)\n- Simplify prompts and reduce variable complexity\n- Check network connectivity\n- Try a different AI model (some are faster)\n\n## Document Upload Problems\n\n### Error: \"File format not supported\"\n\n**Supported formats**: PDF, DOCX, TXT, MD\n\n**Solution**: Convert your file to a supported format using online converters or document software.\n\n### Error: \"File too large\"\n\n**Cause**: File exceeds 10MB limit\n\n**Solution**: Split large documents into smaller files or compress PDFs\n\n## Prompt Execution Errors\n\n### Error: \"Execution Failed\"\n\n**Common Causes:**\n\n**1. Model Unavailable:**\n```\nError: \"Model temporarily unavailable\"\nSolution: Try a different model or wait and retry\n```\n\n**2. Token Limit Exceeded:**\n```\nError: \"Input exceeds token limit\"\nSolution: \n- Reduce prompt length\n- Use fewer RAG chunks (lower Top K)\n- Switch to model with larger context window\n```\n\n**3. Rate Limit Exceeded:**\n```\nError: \"Rate limit exceeded\"\nSolution:\n- Wait 60 seconds and retry\n- Upgrade to higher tier plan\n- Batch requests instead of rapid-fire\n```\n\n**4. Invalid Variables:**\n```\nError: \"Missing required variable: {{variable_name}}\"\nSolution:\n- Fill in all required variables\n- Check variable names match prompt template\n- Remove unused variables from prompt\n```\n\n**5. Timeout:**\n```\nError: \"Execution timeout after 60 seconds\"\nSolution:\n- Simplify prompt\n- Reduce output length requirement\n- Try faster model\n- Check model status page\n```\n\n### Error: \"Invalid Response Format\"\n\n**Cause**: Model returned unexpected format or empty response.\n\n**Solutions**:\n1. Check prompt instructions are clear\n2. Verify model supports requested format (JSON, XML, etc.)\n3. Review execution history for partial responses\n4. Try different model\n\nSee [Execution History and Ratings](/dashboard/help/core-features/execution-history) to review failed executions.\n\n### Error: \"Cost Limit Exceeded\"\n\n**Cause**: Execution would exceed daily or per-execution cost limits.\n\n**Solutions**:\n1. Review cost settings in workspace\n2. Increase cost limits if justified\n3. Optimize prompt to reduce tokens\n4. Use cheaper model for testing\n\nSee [Token Usage and Cost Tracking](/dashboard/help/core-features/token-costs) for cost management.\n\n## Model Selection Issues\n\n### Which Model Should I Use?\n\n**For General Use:**\n- GPT-4 Turbo: Best quality, higher cost\n- Claude 3.5 Sonnet: Great balance\n- GPT-3.5 Turbo: Fast and cheap\n\n**For RAG:**\n- GPT-4 Turbo: Best at following context\n- Claude 3.5 Sonnet: Large context window (200K)\n- Gemini 1.5 Pro: Massive context (1M tokens)\n\n**For Code:**\n- GPT-4 Turbo: Best overall\n- Claude 3.5 Sonnet: Great for complex code\n- Codex: Specialized for code generation\n\nSee [Model Selection and Comparison](/dashboard/help/core-features/model-selection) for detailed comparison.\n\n### Model Returns Poor Results\n\n**Symptoms**: Output quality is low, irrelevant, or incorrect.\n\n**Solutions**:\n\n**1. Improve Prompt:**\n```\n❌ \"Write about {{topic}}\"\n✅ \"Write a 500-word professional article about {{topic}}, \n    targeting {{audience}}, with examples and actionable tips.\"\n```\n\n**2. Adjust Temperature:**\n```\nCreative tasks: 0.7-0.9\nFactual tasks: 0.1-0.3\nRAG tasks: 0.1-0.2 (follow context closely)\n```\n\n**3. Try Different Model:**\n```\nIf GPT-4 is too verbose → Try Claude (more concise)\nIf Claude is too formal → Try GPT-4 (more flexible)\nIf both fail → Check prompt clarity\n```\n\n**4. Add Examples:**\n```\nInclude 1-2 example outputs in your prompt to guide the model.\n```\n\n**5. Review Execution History:**\n```\nCheck past executions to identify patterns:\n- Does it fail with certain variables?\n- Does it work better with specific models?\n- Are there common error patterns?\n```\n\nSee [Execution History and Ratings](/dashboard/help/core-features/execution-history) for analysis tools.\n\n## RAG Execution Issues\n\n### No relevant results found\n\n**Causes**: Documents not indexed, poor query, or low similarity threshold\n\n**Solutions**:\n- Verify documents are uploaded and processed\n- Rephrase your query to be more specific\n- Lower the similarity threshold\n- Check that documents contain relevant information\n\n### Retrieved Context Not Relevant\n\n**Symptoms**: RAG retrieves chunks but they don't answer the question.\n\n**Solutions**:\n1. **Increase Similarity Threshold:**\n   ```\n   Current: 0.70 → Try: 0.80\n   Only retrieve highly relevant chunks\n   ```\n\n2. **Use Hybrid Search:**\n   ```\n   Combine semantic + keyword search\n   Better for specific terms and concepts\n   ```\n\n3. **Filter Documents:**\n   ```\n   Instead of searching all documents,\n   select only relevant document categories\n   ```\n\n4. **Improve Document Quality:**\n   ```\n   - Add more relevant documents\n   - Remove outdated/irrelevant documents\n   - Optimize chunking strategy\n   ```\n\nSee [RAG Execution Basics](/dashboard/help/core-features/rag-execution-basics) for optimization tips.\n\n### Context Too Large Error\n\n**Error**: \"Retrieved context exceeds model token limit\"\n\n**Solutions**:\n1. Reduce Top K (retrieve fewer chunks)\n2. Use model with larger context window\n3. Summarize chunks before injection\n4. Filter documents more aggressively\n\n## Getting Additional Help\n\n### Self-Service Resources\n\n- [Execution History](/dashboard/help/core-features/execution-history) - Review past executions\n- [Model Selection Guide](/dashboard/help/core-features/model-selection) - Choose the right model\n- [Token Cost Tracking](/dashboard/help/core-features/token-costs) - Monitor and optimize costs\n- [RAG Execution Basics](/dashboard/help/core-features/rag-execution-basics) - RAG troubleshooting\n\n### Contact Support\n\n- Check our FAQ section\n- Review the documentation\n- Contact support team at support@ethosprompt.com\n- Join our community Discord for peer help",
    "category": "troubleshooting",
    "tags": ["troubleshooting", "errors", "debugging", "support", "rag", "documents"],
    "difficulty": "beginner",
    "lastUpdated": "2025-01-15",
    "featured": false,
    "faqs": [
      {
        "id": "auth-error",
        "question": "Why am I getting authentication errors?",
        "answer": "Check that you are logged in and your session has not expired. Try logging out and logging back in. If the issue persists, clear your browser cache and cookies."
      },
      {
        "id": "slow-performance",
        "question": "Why are my prompts running slowly?",
        "answer": "Large documents or complex prompts can slow performance. Try optimizing your content by using smaller documents, simplifying prompts, or selecting a faster AI model."
      },
      {
        "id": "rag-not-working",
        "question": "Why is RAG not finding relevant information?",
        "answer": "Ensure your documents are fully processed (check the Documents page for status). Try lowering the similarity threshold or using more specific search terms in your prompt variables."
      }
    ]
  },
  {
    "id": "api-integration-guide",
    "slug": "api-integration-guide",
    "title": "API Integration Guide",
    "excerpt": "Complete guide to integrating RAG Prompt Library with your applications",
    "content": "# API Integration Guide\n\n## Overview\nEthosPrompt's dashboard uses Firebase Callable Functions for authenticated API calls. Prefer callables over raw REST in the dashboard. REST endpoints may be available if your environment enables Cloud Run.\n\n## Authentication\nWhen you use Firebase Callable Functions from the web SDK, your Firebase Auth ID token is sent automatically — no manual Authorization header required.\n\n## Execute a prompt (callable)\n```ts\nimport { getFunctions, httpsCallable } from 'firebase/functions';\nconst functions = getFunctions(undefined, 'australia-southeast1');\nconst executePrompt = httpsCallable(functions, 'execute_prompt');\n\nconst res = await executePrompt({\n  promptId: 'prompt-123',\n  variables: { customer_name: 'John Doe', product_name: 'Premium Widget' },\n  modelOverride: 'x-ai/grok-2-1212:free',\n});\nconsole.log(res.data);\n```\n\n## Upload a document for RAG (callable)\n```ts\nimport { getFunctions, httpsCallable } from 'firebase/functions';\nconst functions = getFunctions(undefined, 'australia-southeast1');\nconst uploadDocument = httpsCallable(functions, 'upload_document');\n\nasync function toBase64(file: File): Promise<string> {\n  return new Promise((resolve, reject) => {\n    const reader = new FileReader();\n    reader.onload = () => resolve((reader.result as string).split(',')[1]);\n    reader.onerror = reject;\n    reader.readAsDataURL(file);\n  });\n}\n\nconst content = await toBase64(file);\nawait uploadDocument({ filename: file.name, content, mimeType: file.type });\n```\n\n## Execution history and details (callable)\n```ts\nconst getExecutionHistory = httpsCallable(functions, 'get_execution_history');\nconst { data } = await getExecutionHistory({ limit: 20, promptId: 'prompt-123' });\n```\n\n## Optional REST endpoints (if enabled)\nSome deployments expose REST endpoints via Cloud Run under `/api/ai/*` (e.g., `/api/ai/chat`, `/api/ai/upload-document`). These require a Firebase ID token (Bearer) and are primarily for server-to-server or non-web clients. Consult your environment's base URL.\n\n## Billing and safety\n- Use OpenRouter free models with the `:free` suffix for manual testing.\n- Set `OPENROUTER_USE_MOCK=true` in non-production to guarantee zero billing during automated tests.\n\n## Error handling (callables)\n```ts\ntry {\n  const res = await executePrompt({ promptId: 'prompt-123', variables: {} });\n} catch (e: any) {\n  console.error('Callable error', e?.message || e);\n}\n```\n\n## Rate limiting\nServer-side rate limiting may apply; implement client retries with backoff when appropriate.",
    "category": "api",
    "tags": ["api", "integration", "authentication", "endpoints", "javascript"],
    "difficulty": "intermediate",
    "lastUpdated": "2025-01-13",
    "featured": true
  },
  {
    "id": "advanced-prompt-engineering",
    "slug": "advanced-prompt-engineering",
    "title": "Advanced Prompt Engineering Techniques",
    "excerpt": "Master sophisticated prompting strategies for complex AI tasks",
    "content": "# Advanced Prompt Engineering Techniques\n\n## Chain-of-Thought Prompting\n\nGuide the AI through step-by-step reasoning to improve accuracy and transparency.\n\n### Example:\n\n```\nQuestion: What is 15% of 240?\n\nLet me think step by step:\n1. First, I need to convert 15% to a decimal: 15% = 0.15\n2. Then multiply: 240 × 0.15 = 36\n\nTherefore, 15% of 240 is 36.\n```\n\n## Few-Shot Learning\n\nProvide multiple examples to help the model understand patterns.\n\n```\nClassify the sentiment of these reviews:\n\nReview: \"This product is amazing!\" → Positive\nReview: \"Terrible quality, very disappointed.\" → Negative\nReview: \"It's okay, nothing special.\" → Neutral\n\nNow classify: \"{{review_text}}\" → \n```\n\n## Prompt Chaining\n\nBreak complex tasks into smaller, manageable steps.\n\n```\nStep 1: Extract key information from {{document}}\nStep 2: Analyze the extracted information\nStep 3: Generate recommendations based on analysis\n```\n\n## Context Window Optimization\n\nEfficiently use available context space for better results.\n\n**Tips:**\n- Prioritize most relevant information\n- Use summarization for long documents\n- Structure context clearly with headings\n- Remove redundant information\n\n## Role-Based Prompting\n\nAssign a specific role to the AI for better responses.\n\n```\nYou are an expert {{role}} with {{years}} years of experience.\nYour task is to {{task}}.\nProvide your response in a {{format}} format.\n```\n\n## Constraint-Based Prompting\n\nSet clear constraints to guide output format and content.\n\n```\nGenerate a {{content_type}} with the following constraints:\n- Length: {{word_count}} words\n- Tone: {{tone}}\n- Audience: {{audience}}\n- Must include: {{required_elements}}\n- Must avoid: {{forbidden_elements}}\n```",
    "category": "best-practices",
    "tags": ["advanced", "techniques", "chain-of-thought", "few-shot", "optimization"],
    "difficulty": "advanced",
    "lastUpdated": "2025-01-14",
    "featured": false
  },
  {
    "id": "variables-syntax",
    "slug": "variables-syntax",
    "title": "Prompt Variables Syntax and Patterns",
    "excerpt": "Master variable syntax, patterns, validation, and best practices for dynamic prompts",
    "content": "# Prompt Variables Syntax and Patterns\n\n## Overview\n\nVariables allow you to create dynamic, reusable prompts that adapt to different inputs. EthosPrompt uses a simple `{{variable_name}}` syntax that's easy to read and maintain.\n\n> [!TIP]\n> Variables make your prompts flexible and reusable across different contexts without duplicating content.\n\n## Basic Syntax\n\n### Variable Declaration\n\nVariables are declared using double curly braces:\n\n```\nHello {{customer_name}}, welcome to {{company_name}}!\n```\n\n**Rules:**\n- Variable names must be alphanumeric with underscores\n- Case-sensitive: `{{Name}}` and `{{name}}` are different\n- No spaces allowed: Use `{{first_name}}` not `{{first name}}`\n- Must start with a letter: `{{1st_item}}` is invalid\n\n### Valid Variable Names\n\n```\n{{customer_name}}        ✓ Valid\n{{product_id}}           ✓ Valid\n{{order_date_2024}}      ✓ Valid\n{{user_email_address}}   ✓ Valid\n{{_private_var}}         ✓ Valid (starts with underscore)\n```\n\n### Invalid Variable Names\n\n```\n{{customer name}}        ✗ Contains space\n{{product-id}}           ✗ Contains hyphen\n{{2024_order}}           ✗ Starts with number\n{{user@email}}           ✗ Contains special character\n{{}}                     ✗ Empty variable name\n```\n\n## Common Patterns\n\n### Customer Communication\n\n```\nDear {{customer_name}},\n\nThank you for your inquiry about {{product_name}}. \n\nBased on your requirements for {{use_case}}, I recommend the following:\n\n{{recommendation}}\n\nBest regards,\n{{agent_name}}\n```\n\n### Content Generation\n\n```\nWrite a {{content_type}} about {{topic}} for {{target_audience}}.\n\nTone: {{tone}}\nLength: {{word_count}} words\nKey points to cover:\n{{key_points}}\n```\n\n### Data Analysis\n\n```\nAnalyze the following {{data_type}} data:\n\n{{data_input}}\n\nProvide insights on:\n1. {{metric_1}}\n2. {{metric_2}}\n3. {{metric_3}}\n\nFormat the response as {{output_format}}.\n```\n\n## Variable Validation\n\n### Required vs Optional Variables\n\nAll variables in EthosPrompt are **required by default**. The execution will fail if any variable is missing a value.\n\n> [!WARNING]\n> Always provide values for all variables before executing a prompt. Missing variables will cause execution errors.\n\n### Type Handling\n\nVariables are treated as strings. For structured data, use JSON formatting:\n\n```\nProcess this order:\n{{order_json}}\n\nExtract: customer name, items, total amount.\n```\n\nExample value for `{{order_json}}`:\n```json\n{\n  \"customer\": \"John Doe\",\n  \"items\": [\"Widget A\", \"Widget B\"],\n  \"total\": 99.99\n}\n```\n\n## Advanced Patterns\n\n### Conditional Logic (Prompt-Level)\n\nWhile variables don't support built-in conditionals, you can structure prompts to handle different scenarios:\n\n```\nScenario: {{scenario_type}}\n\nIf the scenario is \"urgent\":\n- Prioritize {{urgent_action}}\n- Notify {{urgent_contact}}\n\nIf the scenario is \"standard\":\n- Follow normal process for {{standard_action}}\n- Update {{standard_contact}}\n\nCurrent scenario details:\n{{scenario_details}}\n```\n\n### List Variables\n\nFor multiple items, use newline-separated or comma-separated values:\n\n```\nReview these items:\n{{item_list}}\n\nFor each item, provide:\n- Quality assessment\n- Recommendations\n- Priority level\n```\n\nExample value:\n```\nItem 1: Premium Widget\nItem 2: Standard Gadget\nItem 3: Economy Tool\n```\n\n### Nested Context\n\nOrganize related variables with prefixes:\n\n```\nCustomer Information:\n- Name: {{customer_name}}\n- Email: {{customer_email}}\n- Phone: {{customer_phone}}\n\nOrder Information:\n- ID: {{order_id}}\n- Date: {{order_date}}\n- Status: {{order_status}}\n\nGenerate a status update email.\n```\n\n## Escaping and Special Cases\n\n### Literal Curly Braces\n\nIf you need to display literal `{{` or `}}` in your output, currently there's no escape mechanism. Workaround:\n\n```\nTo use variables in your prompt, wrap them in double curly braces.\nExample: Use the format (open-brace)(open-brace)variable_name(close-brace)(close-brace)\n```\n\n### Variables in Code Blocks\n\nVariables inside markdown code blocks are NOT substituted:\n\n````\nExample prompt template:\n```\nHello {{name}}, your order {{order_id}} is ready.\n```\n````\n\nThis is useful for showing examples without triggering substitution.\n\n## Best Practices\n\n### 1. Use Descriptive Names\n\n**Good:**\n```\n{{customer_first_name}}\n{{product_category}}\n{{delivery_address}}\n```\n\n**Bad:**\n```\n{{n}}\n{{p}}\n{{addr}}\n```\n\n### 2. Group Related Variables\n\nUse consistent prefixes for related data:\n\n```\n{{user_name}}\n{{user_email}}\n{{user_role}}\n\n{{order_id}}\n{{order_date}}\n{{order_total}}\n```\n\n### 3. Document Expected Format\n\nAdd comments or descriptions in your prompt:\n\n```\nCustomer feedback (1-5 star rating): {{rating}}\nFeedback text: {{feedback_text}}\nProduct ID (format: PROD-XXXX): {{product_id}}\n```\n\n### 4. Validate Input Before Execution\n\nAlways review variable values before executing:\n- Check for typos\n- Verify data format\n- Ensure completeness\n\n### 5. Use Consistent Naming Conventions\n\nChoose one style and stick to it:\n- `snake_case`: `{{customer_name}}` (recommended)\n- `camelCase`: `{{customerName}}`\n- `PascalCase`: `{{CustomerName}}`\n\n## Common Anti-Patterns\n\n### ❌ Overly Generic Names\n\n```\n{{input}}\n{{data}}\n{{value}}\n{{text}}\n```\n\nThese don't convey meaning. Use specific names instead.\n\n### ❌ Too Many Variables\n\nPrompts with 20+ variables become hard to manage:\n\n```\n{{var1}} {{var2}} {{var3}} ... {{var20}}\n```\n\n**Solution:** Group related data into structured variables or use RAG for context.\n\n### ❌ Redundant Variables\n\n```\n{{customer_name}}\n{{customer_full_name}}\n{{name_of_customer}}\n```\n\nPick one and use it consistently.\n\n### ❌ Unclear Variable Purpose\n\n```\n{{x}}\n{{temp}}\n{{thing}}\n```\n\nAlways use self-documenting names.\n\n## Variable Limits\n\n- **Maximum variables per prompt:** No hard limit, but 10-15 is recommended for maintainability\n- **Variable name length:** 1-64 characters\n- **Variable value length:** Up to 10,000 characters per variable\n- **Total prompt size:** 100,000 characters (including variables)\n\n## Testing Variables\n\n### Create Test Data Sets\n\nMaintain sample values for testing:\n\n```json\n{\n  \"customer_name\": \"Jane Smith\",\n  \"product_name\": \"Premium Widget\",\n  \"order_id\": \"ORD-12345\",\n  \"order_date\": \"2025-01-15\"\n}\n```\n\n### Edge Cases to Test\n\n- Empty strings\n- Very long text (near limits)\n- Special characters\n- Unicode characters\n- Numbers vs strings\n- JSON data\n\n## Integration with RAG\n\nVariables work seamlessly with RAG-enabled prompts:\n\n```\nSearch my documents for information about {{topic}}.\n\nBased on the retrieved context, answer this question:\n{{user_question}}\n\nProvide sources and page numbers.\n```\n\nRAG will search for `{{topic}}` and use `{{user_question}}` in the final prompt.\n\n## Troubleshooting\n\n### Variable Not Substituted\n\n**Symptoms:** Variable appears as `{{var_name}}` in output\n\n**Causes:**\n- Typo in variable name\n- Variable not provided during execution\n- Extra spaces: `{{ var_name }}`\n\n**Solution:** Check spelling and ensure no spaces inside braces.\n\n### Execution Error: Missing Variable\n\n**Symptoms:** Error message \"Variable 'x' is required\"\n\n**Solution:** Provide a value for the variable, even if it's an empty string.\n\n### Unexpected Output\n\n**Symptoms:** Output doesn't match expected format\n\n**Causes:**\n- Variable contains unexpected data\n- Variable value too long\n- Special characters in value\n\n**Solution:** Validate and sanitize variable values before execution.\n\n## Next Steps\n\n- [Creating Your First Prompt](/dashboard/help/getting-started/creating-first-prompt)\n- [Execution History and Ratings](/dashboard/help/core-features/execution-history)\n- [Advanced Prompt Engineering](/dashboard/help/best-practices/advanced-prompt-engineering)\n- [Template Library Usage](/dashboard/help/core-features/template-library)",
    "category": "core-features",
    "subcategory": "prompts",
    "tags": ["variables", "syntax", "patterns", "best-practices", "validation"],
    "difficulty": "beginner",
    "lastUpdated": "2025-01-15",
    "featured": true,
    "estimatedReadTime": 12
  },
  {
    "id": "rag-chunking-config",
    "slug": "rag-chunking-config",
    "title": "Chunking and Retrieval Configuration",
    "excerpt": "Optimize RAG performance with chunk size, overlap, semantic vs fixed chunking, and hybrid search strategies",
    "content": "# Chunking and Retrieval Configuration\n\n## Overview\n\nDocument chunking is the foundation of effective RAG (Retrieval-Augmented Generation). How you split documents into chunks directly impacts retrieval quality, cost, and performance.\n\n> [!NOTE]\n> Chunking strategy can make or break your RAG system. Spend time optimizing it for your specific use case.\n\n## What is Chunking?\n\nChunking divides large documents into smaller, manageable pieces that can be:\n- Embedded as vectors\n- Searched semantically\n- Retrieved and injected into prompts\n- Processed within token limits\n\n### Why Chunk Documents?\n\n1. **Token Limits:** AI models have context window limits (4K-128K tokens)\n2. **Relevance:** Smaller chunks = more precise retrieval\n3. **Cost:** Only retrieve and process relevant sections\n4. **Performance:** Faster embedding and search\n\n## Chunking Strategies\n\n### 1. Fixed-Size Chunking\n\nSplit documents into equal-sized chunks by character or token count.\n\n**Configuration:**\n```json\n{\n  \"strategy\": \"fixed\",\n  \"chunkSize\": 512,\n  \"chunkOverlap\": 50,\n  \"unit\": \"tokens\"\n}\n```\n\n**Pros:**\n- Simple and predictable\n- Fast processing\n- Consistent chunk sizes\n\n**Cons:**\n- May split mid-sentence or mid-paragraph\n- Ignores document structure\n- Can break semantic meaning\n\n**Best For:**\n- Uniform documents (logs, transcripts)\n- Quick prototyping\n- Simple use cases\n\n### 2. Semantic Chunking\n\nSplit documents at natural boundaries (paragraphs, sections, sentences).\n\n**Configuration:**\n```json\n{\n  \"strategy\": \"semantic\",\n  \"maxChunkSize\": 1024,\n  \"minChunkSize\": 256,\n  \"respectBoundaries\": [\"paragraph\", \"sentence\"],\n  \"chunkOverlap\": 100\n}\n```\n\n**Pros:**\n- Preserves meaning and context\n- Respects document structure\n- Better retrieval quality\n\n**Cons:**\n- Variable chunk sizes\n- Slower processing\n- More complex implementation\n\n**Best For:**\n- Structured documents (articles, manuals)\n- High-quality retrieval requirements\n- Production RAG systems\n\n### 3. Hybrid Chunking\n\nCombines semantic boundaries with size constraints.\n\n**Configuration:**\n```json\n{\n  \"strategy\": \"hybrid\",\n  \"targetChunkSize\": 768,\n  \"maxChunkSize\": 1024,\n  \"minChunkSize\": 256,\n  \"chunkOverlap\": 75,\n  \"boundaries\": [\"section\", \"paragraph\"],\n  \"fallbackToFixed\": true\n}\n```\n\n**Pros:**\n- Best of both worlds\n- Flexible and robust\n- Handles edge cases\n\n**Cons:**\n- Most complex\n- Requires tuning\n\n**Best For:**\n- Mixed document types\n- Enterprise applications\n- Optimal quality/performance balance\n\n## Chunk Size Optimization\n\n### Recommended Sizes\n\n| Use Case | Chunk Size | Overlap | Rationale |\n|----------|------------|---------|----------|\n| **FAQ/Q&A** | 256-512 tokens | 25-50 | Short, focused answers |\n| **Technical Docs** | 512-1024 tokens | 50-100 | Detailed explanations |\n| **Legal/Contracts** | 768-1536 tokens | 100-200 | Context preservation |\n| **Code Documentation** | 512-768 tokens | 50-75 | Function/class level |\n| **General Knowledge** | 512-1024 tokens | 50-100 | Balanced approach |\n\n### Chunk Size Trade-offs\n\n**Small Chunks (256-512 tokens):**\n- ✅ Precise retrieval\n- ✅ Lower cost per chunk\n- ✅ Faster search\n- ❌ May lose context\n- ❌ More chunks to manage\n\n**Medium Chunks (512-1024 tokens):**\n- ✅ Good balance\n- ✅ Preserves context\n- ✅ Recommended default\n- ⚠️ Moderate cost\n\n**Large Chunks (1024-2048 tokens):**\n- ✅ Maximum context\n- ✅ Fewer chunks\n- ❌ Less precise retrieval\n- ❌ Higher cost per chunk\n- ❌ Slower search\n\n## Chunk Overlap\n\n### Why Overlap?\n\nOverlap ensures important information at chunk boundaries isn't lost.\n\n**Example Without Overlap:**\n```\nChunk 1: \"...the product features include advanced\"\nChunk 2: \"analytics and real-time reporting...\"\n```\n\nQuery: \"What analytics features are available?\"\nResult: May miss the connection between chunks.\n\n**Example With Overlap (50 tokens):**\n```\nChunk 1: \"...the product features include advanced analytics and\"\nChunk 2: \"advanced analytics and real-time reporting capabilities...\"\n```\n\nQuery: \"What analytics features are available?\"\nResult: Both chunks contain relevant context.\n\n### Overlap Guidelines\n\n- **Minimum:** 10% of chunk size\n- **Recommended:** 10-20% of chunk size\n- **Maximum:** 25% of chunk size\n\n**Formula:**\n```\nOverlap = ChunkSize × 0.15  (15% overlap)\n\nExample:\nChunkSize = 512 tokens\nOverlap = 512 × 0.15 = 77 tokens\n```\n\n> [!WARNING]\n> Too much overlap (>25%) wastes storage and increases costs without significant quality improvement.\n\n## Retrieval Strategies\n\n### 1. Semantic Search (Vector Search)\n\nUses embeddings to find semantically similar chunks.\n\n**How it works:**\n1. Convert query to embedding vector\n2. Calculate cosine similarity with chunk embeddings\n3. Return top-K most similar chunks\n\n**Pros:**\n- Understands meaning, not just keywords\n- Handles synonyms and paraphrasing\n- Language-agnostic\n\n**Cons:**\n- Requires embedding model\n- Higher computational cost\n- May miss exact keyword matches\n\n**Configuration:**\n```json\n{\n  \"retrievalStrategy\": \"semantic\",\n  \"embeddingModel\": \"text-embedding-3-small\",\n  \"topK\": 5,\n  \"similarityThreshold\": 0.7\n}\n```\n\n### 2. Keyword Search (BM25)\n\nTraditional keyword-based search using BM25 algorithm.\n\n**How it works:**\n1. Tokenize query and documents\n2. Calculate term frequency and inverse document frequency\n3. Rank chunks by BM25 score\n\n**Pros:**\n- Fast and efficient\n- Excellent for exact matches\n- No embedding required\n\n**Cons:**\n- Misses semantic similarity\n- Struggles with synonyms\n- Language-dependent\n\n**Configuration:**\n```json\n{\n  \"retrievalStrategy\": \"keyword\",\n  \"algorithm\": \"bm25\",\n  \"topK\": 5,\n  \"k1\": 1.5,\n  \"b\": 0.75\n}\n```\n\n### 3. Hybrid Search (Recommended)\n\nCombines semantic and keyword search for best results.\n\n**How it works:**\n1. Run both semantic and keyword search\n2. Normalize scores to 0-1 range\n3. Combine with weighted average\n4. Re-rank results\n\n**Configuration:**\n```json\n{\n  \"retrievalStrategy\": \"hybrid\",\n  \"semanticWeight\": 0.7,\n  \"keywordWeight\": 0.3,\n  \"topK\": 5,\n  \"rerank\": true\n}\n```\n\n**Weight Recommendations:**\n- **General use:** 70% semantic, 30% keyword\n- **Technical docs:** 60% semantic, 40% keyword\n- **Code search:** 50% semantic, 50% keyword\n- **FAQ:** 80% semantic, 20% keyword\n\n> [!TIP]\n> Start with 70/30 semantic/keyword split and adjust based on your retrieval quality metrics.\n\n## Advanced Configuration\n\n### Metadata Filtering\n\nFilter chunks by metadata before retrieval:\n\n```json\n{\n  \"filters\": {\n    \"documentType\": \"technical_manual\",\n    \"version\": \"2.0\",\n    \"language\": \"en\",\n    \"dateRange\": {\n      \"start\": \"2024-01-01\",\n      \"end\": \"2025-01-01\"\n    }\n  }\n}\n```\n\n### Re-ranking\n\nImprove retrieval quality with a re-ranking model:\n\n```json\n{\n  \"reranking\": {\n    \"enabled\": true,\n    \"model\": \"cross-encoder\",\n    \"topKBeforeRerank\": 20,\n    \"topKAfterRerank\": 5\n  }\n}\n```\n\n**Process:**\n1. Retrieve 20 candidates with fast search\n2. Re-rank with slower, more accurate model\n3. Return top 5 best matches\n\n### Context Window Management\n\nControl how much context to inject into prompts:\n\n```json\n{\n  \"contextWindow\": {\n    \"maxTokens\": 4000,\n    \"reserveForPrompt\": 1000,\n    \"reserveForResponse\": 1000,\n    \"availableForContext\": 2000\n  }\n}\n```\n\n## Cost and Performance Trade-offs\n\n### Cost Factors\n\n1. **Embedding Generation:**\n   - Cost per 1M tokens: $0.02-$0.13 (model dependent)\n   - One-time cost per document\n\n2. **Vector Storage:**\n   - Cost per GB/month: $0.10-$0.40 (provider dependent)\n   - Scales with number of chunks\n\n3. **Search Operations:**\n   - Cost per 1K queries: $0.001-$0.01\n   - Scales with query volume\n\n4. **LLM Context:**\n   - Cost per 1M tokens: $0.50-$30 (model dependent)\n   - Scales with retrieved chunk size\n\n### Cost Optimization Strategies\n\n**1. Optimize Chunk Size:**\n```\nSmaller chunks = More chunks = Higher storage cost\nLarger chunks = Fewer chunks = Lower storage cost\n\nSweet spot: 512-768 tokens\n```\n\n**2. Use Efficient Embedding Models:**\n```\ntext-embedding-3-small: $0.02/1M tokens (1536 dimensions)\ntext-embedding-3-large: $0.13/1M tokens (3072 dimensions)\n\nRecommendation: Use small model unless you need maximum quality\n```\n\n**3. Implement Caching:**\n```json\n{\n  \"caching\": {\n    \"enabled\": true,\n    \"ttl\": 3600,\n    \"maxSize\": 1000\n  }\n}\n```\n\n**4. Filter Before Retrieval:**\nUse metadata filters to reduce search space:\n```json\n{\n  \"preFilters\": {\n    \"documentType\": \"manual\",\n    \"version\": \"latest\"\n  }\n}\n```\n\n### Performance Optimization\n\n**1. Batch Processing:**\n```\nProcess documents in batches of 100-500\nParallelize embedding generation\nUse async operations\n```\n\n**2. Index Optimization:**\n```\nUse approximate nearest neighbor (ANN) algorithms:\n- HNSW (Hierarchical Navigable Small World)\n- IVF (Inverted File Index)\n- LSH (Locality-Sensitive Hashing)\n```\n\n**3. Lazy Loading:**\n```\nOnly load chunk content when needed\nStore metadata separately\nUse pagination for large result sets\n```\n\n## Testing and Evaluation\n\n### Metrics to Track\n\n1. **Retrieval Precision:** % of retrieved chunks that are relevant\n2. **Retrieval Recall:** % of relevant chunks that are retrieved\n3. **Mean Reciprocal Rank (MRR):** Quality of ranking\n4. **Latency:** Time to retrieve and rank chunks\n5. **Cost per Query:** Total cost including embeddings and LLM\n\n### A/B Testing Configurations\n\nTest different configurations:\n\n```json\n{\n  \"configA\": {\n    \"chunkSize\": 512,\n    \"overlap\": 50,\n    \"strategy\": \"semantic\"\n  },\n  \"configB\": {\n    \"chunkSize\": 768,\n    \"overlap\": 100,\n    \"strategy\": \"hybrid\"\n  }\n}\n```\n\nMeasure:\n- Retrieval quality (human evaluation)\n- Response quality\n- Latency\n- Cost\n\n## Troubleshooting\n\n### Poor Retrieval Quality\n\n**Symptoms:** Irrelevant chunks retrieved\n\n**Solutions:**\n- Increase chunk overlap\n- Try hybrid search instead of semantic-only\n- Adjust similarity threshold\n- Add metadata filters\n- Use re-ranking\n\n### High Costs\n\n**Symptoms:** Unexpected embedding or LLM costs\n\n**Solutions:**\n- Reduce chunk overlap\n- Use smaller embedding model\n- Implement caching\n- Filter documents before processing\n- Optimize chunk size\n\n### Slow Performance\n\n**Symptoms:** Long retrieval times\n\n**Solutions:**\n- Use ANN indexing\n- Reduce topK value\n- Implement caching\n- Optimize vector database\n- Use faster embedding model\n\n## Next Steps\n\n- [Document Upload Guide](/dashboard/help/core-features/document-upload-guide)\n- [RAG Execution Basics](/dashboard/help/core-features/rag-execution-basics)\n- [Understanding Context Injection](/dashboard/help/core-features/rag-context-preview)\n- [Model Performance Dashboard](/dashboard/help/core-features/model-performance)",
    "category": "core-features",
    "subcategory": "rag",
    "tags": ["rag", "chunking", "retrieval", "optimization", "performance", "cost"],
    "difficulty": "intermediate",
    "lastUpdated": "2025-01-15",
    "featured": true,
    "estimatedReadTime": 18
  },
  {
    "id": "execution-history",
    "slug": "execution-history",
    "title": "Execution History and Ratings",
    "excerpt": "Track, analyze, and rate your prompt executions with comprehensive history and metadata",
    "content": "# Execution History and Ratings\n\n## Overview\n\nEvery prompt execution in EthosPrompt is automatically tracked, allowing you to review past runs, compare results, rate quality, and analyze performance over time.\n\n> [!TIP]\n> Use execution history to identify patterns, optimize prompts, and track improvements over time.\n\n## Accessing Execution History\n\n### From Dashboard\n\n1. Navigate to **Dashboard** → **Executions**\n2. View all executions across all prompts\n3. Filter by date, prompt, model, or status\n\n### From Prompt Detail Page\n\n1. Open any prompt\n2. Click **Execution History** tab\n3. View executions specific to that prompt\n\n### Quick Access\n\n- Recent executions appear in the dashboard sidebar\n- Click any execution to view full details\n- Use search to find specific executions\n\n## Execution Metadata\n\nEach execution captures comprehensive metadata:\n\n### Basic Information\n\n- **Execution ID:** Unique identifier\n- **Timestamp:** When the execution started\n- **Duration:** Total execution time\n- **Status:** Success, Failed, or Cancelled\n- **User:** Who triggered the execution\n\n### Prompt Details\n\n- **Prompt ID:** Which prompt was executed\n- **Prompt Version:** Snapshot of prompt content\n- **Variables:** All variable values used\n- **RAG Enabled:** Whether RAG was used\n\n### Model Configuration\n\n- **Model:** AI model used (e.g., gpt-4, claude-3.5-sonnet)\n- **Temperature:** Creativity setting (0.0-1.0)\n- **Max Tokens:** Maximum response length\n- **Top P:** Nucleus sampling parameter\n\n### Cost and Usage\n\n- **Input Tokens:** Tokens in prompt + context\n- **Output Tokens:** Tokens in response\n- **Total Tokens:** Input + Output\n- **Estimated Cost:** Based on model pricing\n\n### RAG Details (if enabled)\n\n- **Documents Searched:** Number of documents queried\n- **Chunks Retrieved:** Number of relevant chunks\n- **Retrieval Time:** Time spent searching\n- **Relevance Scores:** Similarity scores for each chunk\n\n## Viewing Execution Details\n\nClick any execution to see:\n\n### Input Section\n\n```\nPrompt Template:\nAnalyze this customer feedback: {{feedback_text}}\n\nVariable Values:\n- feedback_text: \"Great product, fast shipping!\"\n\nRAG Context (if enabled):\n- Chunk 1: [Product review guidelines...]\n- Chunk 2: [Sentiment analysis framework...]\n```\n\n### Output Section\n\n```\nGenerated Response:\nSentiment: Positive\nKey Themes: Product Quality, Shipping Speed\nRecommendation: Feature in testimonials\n\nMetadata:\n- Model: gpt-4-turbo\n- Tokens: 1,234 (input) + 156 (output)\n- Cost: $0.0185\n- Duration: 2.3s\n```\n\n### Retrieved Context (RAG)\n\nIf RAG was enabled, view:\n- Source documents\n- Chunk content\n- Relevance scores\n- Page numbers\n\n## Rating Executions\n\n### Quality Ratings\n\nRate each execution on a 5-star scale:\n\n⭐ **1 Star:** Poor - Unusable output\n⭐⭐ **2 Stars:** Below Average - Needs major revision\n⭐⭐⭐ **3 Stars:** Average - Acceptable with minor edits\n⭐⭐⭐⭐ **4 Stars:** Good - Minor improvements possible\n⭐⭐⭐⭐⭐ **5 Stars:** Excellent - Perfect output\n\n### How to Rate\n\n1. Open execution details\n2. Click star rating at the top\n3. Optionally add notes\n4. Rating is saved automatically\n\n### Rating Benefits\n\n- **Track Quality:** See average ratings over time\n- **Identify Issues:** Find low-rated executions\n- **Optimize Prompts:** Compare ratings across versions\n- **Team Insights:** Share quality metrics\n\n## Annotations and Notes\n\n### Adding Notes\n\nAdd context to any execution:\n\n```\nNotes:\n\"This execution used the updated product guidelines.\nCustomer was satisfied with the response.\nConsider using this as a template for similar cases.\"\n```\n\n### Use Cases\n\n- Document why an execution failed\n- Note successful patterns\n- Track A/B test results\n- Share insights with team\n\n### Tagging Executions\n\nOrganize executions with tags:\n\n```\nTags: production, customer-support, high-priority, template-candidate\n```\n\n## Filtering and Search\n\n### Filter Options\n\n**By Date:**\n- Last 24 hours\n- Last 7 days\n- Last 30 days\n- Custom date range\n\n**By Status:**\n- ✅ Successful\n- ❌ Failed\n- ⏸️ Cancelled\n\n**By Rating:**\n- 5 stars only\n- 4+ stars\n- 3+ stars\n- Below 3 stars\n- Unrated\n\n**By Model:**\n- GPT-4 Turbo\n- Claude 3.5 Sonnet\n- Gemini Pro\n- Custom filters\n\n**By Prompt:**\n- Select specific prompt\n- View all executions for that prompt\n\n### Search Functionality\n\nSearch executions by:\n- Execution ID\n- Variable values\n- Output content\n- Notes/annotations\n- Tags\n\n**Example Searches:**\n```\n\"customer_name:John\"\n\"status:failed\"\n\"model:gpt-4\"\n\"rating:>=4\"\n\"tag:production\"\n```\n\n## Comparing Executions\n\n### Side-by-Side Comparison\n\n1. Select 2-4 executions\n2. Click **Compare**\n3. View differences:\n   - Input variables\n   - Model settings\n   - Output quality\n   - Cost and performance\n   - Ratings\n\n### Use Cases\n\n- **A/B Testing:** Compare different prompt versions\n- **Model Selection:** Compare outputs across models\n- **Parameter Tuning:** Test different temperatures\n- **Cost Optimization:** Find best cost/quality balance\n\n## Execution Analytics\n\n### Key Metrics\n\n**Success Rate:**\n```\nSuccessful Executions / Total Executions × 100%\nTarget: >95%\n```\n\n**Average Rating:**\n```\nSum of All Ratings / Number of Rated Executions\nTarget: >4.0 stars\n```\n\n**Average Cost:**\n```\nTotal Cost / Number of Executions\nTrack trends over time\n```\n\n**Average Duration:**\n```\nTotal Duration / Number of Executions\nTarget: <5 seconds\n```\n\n### Trends Over Time\n\nView charts for:\n- Execution volume (daily/weekly/monthly)\n- Success rate trends\n- Average rating trends\n- Cost trends\n- Token usage trends\n\n### Linking to Analytics Dashboard\n\nExecution history integrates with the Analytics Dashboard:\n\n1. Click **View in Analytics**\n2. See detailed breakdowns:\n   - By prompt\n   - By model\n   - By user\n   - By time period\n\n## Exporting Execution Data\n\n### Export Formats\n\n**CSV Export:**\n```csv\nID,Timestamp,Prompt,Model,Status,Rating,Cost,Duration\nexec_123,2025-01-15 10:30,Customer Support,gpt-4,Success,5,0.0185,2.3s\n```\n\n**JSON Export:**\n```json\n{\n  \"executions\": [\n    {\n      \"id\": \"exec_123\",\n      \"timestamp\": \"2025-01-15T10:30:00Z\",\n      \"prompt\": \"Customer Support\",\n      \"variables\": {...},\n      \"output\": \"...\",\n      \"metadata\": {...}\n    }\n  ]\n}\n```\n\n### Export Options\n\n- Export all executions\n- Export filtered results\n- Export date range\n- Include/exclude output content\n- Include/exclude RAG context\n\n## Best Practices\n\n### 1. Rate Consistently\n\nDevelop a rating rubric:\n\n```\n5 Stars: Output requires no edits, perfect for use\n4 Stars: Minor edits needed, >90% accurate\n3 Stars: Moderate edits needed, 70-90% accurate\n2 Stars: Major edits needed, <70% accurate\n1 Star: Unusable, requires complete rewrite\n```\n\n### 2. Add Context Notes\n\nDocument important executions:\n\n```\n\"First execution with new RAG configuration.\nRetrieved 5 chunks, all highly relevant.\nResponse quality improved significantly.\nConsider making this the default config.\"\n```\n\n### 3. Review Failed Executions\n\nRegularly check failed executions:\n- Identify patterns\n- Fix prompt issues\n- Update documentation\n- Prevent future failures\n\n### 4. Track High-Value Executions\n\nTag important executions:\n\n```\nTags: production, customer-facing, high-impact, template\n```\n\n### 5. Monitor Costs\n\nSet up alerts for:\n- Daily cost thresholds\n- Unusual token usage\n- Expensive model usage\n- Failed execution costs\n\n## Troubleshooting\n\n### Execution Not Appearing\n\n**Possible Causes:**\n- Execution still in progress\n- Filter hiding the execution\n- Permission issues\n\n**Solutions:**\n- Refresh the page\n- Clear all filters\n- Check execution status\n\n### Missing Metadata\n\n**Possible Causes:**\n- Execution interrupted\n- Old execution format\n- Data migration issue\n\n**Solutions:**\n- Re-run the execution\n- Contact support for data recovery\n\n### Export Failing\n\n**Possible Causes:**\n- Too many executions selected\n- Network timeout\n- Browser limitations\n\n**Solutions:**\n- Export smaller date ranges\n- Use API for large exports\n- Try different browser\n\n## Next Steps\n\n- [Analytics Dashboard Overview](/dashboard/help/core-features/analytics-overview)\n- [Model Selection and Comparison](/dashboard/help/core-features/model-selection)\n- [Token Usage and Cost Tracking](/dashboard/help/core-features/token-costs)\n- [Prompt Quality Tools](/dashboard/help/core-features/prompt-quality-tools)",
    "category": "core-features",
    "subcategory": "executions",
    "tags": ["executions", "history", "ratings", "analytics", "tracking"],
    "difficulty": "beginner",
    "lastUpdated": "2025-01-15",
    "featured": true,
    "estimatedReadTime": 14
  },
  {
    "id": "model-selection",
    "slug": "model-selection",
    "title": "Model Selection and Comparison",
    "excerpt": "Choose the right AI model for your use case with comprehensive comparison and guidance",
    "content": "# Model Selection and Comparison\n\n## Overview\n\nEthosPrompt supports multiple AI models from leading providers. Choosing the right model impacts quality, cost, and performance.\n\n> [!NOTE]\n> Different models excel at different tasks. Understanding their strengths helps you optimize for quality and cost.\n\n## Available Models\n\n### OpenAI Models\n\n**GPT-4 Turbo**\n- **Best for:** Complex reasoning, long context, high accuracy\n- **Context:** 128K tokens\n- **Cost:** $10/1M input, $30/1M output\n- **Speed:** Moderate (3-5s)\n- **Use cases:** Analysis, research, complex tasks\n\n**GPT-4**\n- **Best for:** Balanced quality and cost\n- **Context:** 8K tokens\n- **Cost:** $30/1M input, $60/1M output\n- **Speed:** Moderate (3-5s)\n- **Use cases:** General purpose, production\n\n**GPT-3.5 Turbo**\n- **Best for:** Fast, cost-effective tasks\n- **Context:** 16K tokens\n- **Cost:** $0.50/1M input, $1.50/1M output\n- **Speed:** Fast (1-2s)\n- **Use cases:** Simple tasks, high volume\n\n### Anthropic Models\n\n**Claude 3.5 Sonnet**\n- **Best for:** Balanced performance, long context\n- **Context:** 200K tokens\n- **Cost:** $3/1M input, $15/1M output\n- **Speed:** Fast (2-3s)\n- **Use cases:** Analysis, coding, writing\n\n**Claude 3 Opus**\n- **Best for:** Highest quality, complex tasks\n- **Context:** 200K tokens\n- **Cost:** $15/1M input, $75/1M output\n- **Speed:** Slower (5-8s)\n- **Use cases:** Critical tasks, research\n\n**Claude 3 Haiku**\n- **Best for:** Speed and cost efficiency\n- **Context:** 200K tokens\n- **Cost:** $0.25/1M input, $1.25/1M output\n- **Speed:** Very fast (<1s)\n- **Use cases:** Simple tasks, real-time\n\n### Google Models\n\n**Gemini 1.5 Pro**\n- **Best for:** Multimodal, long context\n- **Context:** 1M tokens\n- **Cost:** $1.25/1M input, $5/1M output\n- **Speed:** Moderate (3-4s)\n- **Use cases:** Document analysis, multimodal\n\n### Free Models\n\n**X.AI Grok 2 (Free)**\n- **Model:** `x-ai/grok-2-1212:free`\n- **Best for:** Testing, development\n- **Context:** 32K tokens\n- **Cost:** Free (rate limited)\n- **Speed:** Variable\n- **Use cases:** Prototyping, learning\n\n> [!TIP]\n> Use free models for development and testing. Switch to paid models for production.\n\n## Model Comparison Matrix\n\n| Model | Quality | Speed | Cost | Context | Best For |\n|-------|---------|-------|------|---------|----------|\n| GPT-4 Turbo | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ | 128K | Complex reasoning |\n| Claude 3.5 Sonnet | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | 200K | Balanced |\n| GPT-3.5 Turbo | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 16K | High volume |\n| Claude 3 Haiku | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 200K | Real-time |\n| Gemini 1.5 Pro | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | 1M | Long docs |\n\n## Choosing the Right Model\n\n### By Use Case\n\n**Customer Support:**\n- **Recommended:** Claude 3.5 Sonnet, GPT-3.5 Turbo\n- **Why:** Fast, accurate, cost-effective\n- **Fallback:** Claude 3 Haiku\n\n**Content Generation:**\n- **Recommended:** GPT-4 Turbo, Claude 3.5 Sonnet\n- **Why:** Creative, coherent, high quality\n- **Fallback:** GPT-3.5 Turbo\n\n**Code Generation:**\n- **Recommended:** Claude 3.5 Sonnet, GPT-4 Turbo\n- **Why:** Excellent at code, debugging\n- **Fallback:** GPT-3.5 Turbo\n\n**Data Analysis:**\n- **Recommended:** GPT-4 Turbo, Claude 3 Opus\n- **Why:** Strong reasoning, accuracy\n- **Fallback:** Claude 3.5 Sonnet\n\n**Document Summarization:**\n- **Recommended:** Gemini 1.5 Pro, Claude 3.5 Sonnet\n- **Why:** Long context, efficient\n- **Fallback:** GPT-4 Turbo\n\n**Real-Time Chat:**\n- **Recommended:** Claude 3 Haiku, GPT-3.5 Turbo\n- **Why:** Fast response, low latency\n- **Fallback:** Claude 3.5 Sonnet\n\n### By Budget\n\n**High Budget (Quality First):**\n```\n1. Claude 3 Opus\n2. GPT-4 Turbo\n3. Claude 3.5 Sonnet\n```\n\n**Medium Budget (Balanced):**\n```\n1. Claude 3.5 Sonnet\n2. GPT-4 Turbo\n3. Gemini 1.5 Pro\n```\n\n**Low Budget (Cost First):**\n```\n1. GPT-3.5 Turbo\n2. Claude 3 Haiku\n3. Grok 2 (Free)\n```\n\n### By Context Requirements\n\n**Short Context (<8K tokens):**\n- Any model works\n- Optimize for speed/cost\n\n**Medium Context (8K-32K tokens):**\n- GPT-4 Turbo\n- Claude 3.5 Sonnet\n- Gemini 1.5 Pro\n\n**Long Context (32K-200K tokens):**\n- Claude 3.5 Sonnet\n- Claude 3 Opus\n- Gemini 1.5 Pro\n\n**Very Long Context (>200K tokens):**\n- Gemini 1.5 Pro (up to 1M)\n\n## Model Fallback Mechanics\n\nEthosPrompt automatically falls back to alternative models if:\n- Primary model is unavailable\n- Rate limit exceeded\n- Cost threshold reached\n- Context too large\n\n### Fallback Configuration\n\n```json\n{\n  \"primaryModel\": \"gpt-4-turbo\",\n  \"fallbackModels\": [\n    \"claude-3.5-sonnet\",\n    \"gpt-3.5-turbo\"\n  ],\n  \"fallbackTriggers\": [\n    \"unavailable\",\n    \"rate_limit\",\n    \"cost_threshold\"\n  ]\n}\n```\n\n### Fallback Behavior\n\n1. Try primary model\n2. If fails, try first fallback\n3. If fails, try second fallback\n4. If all fail, return error\n\n> [!WARNING]\n> Fallback models may produce different outputs. Test fallback behavior before production use.\n\n## Model Constraints\n\n### Rate Limits\n\n**Free Models:**\n- 10 requests/minute\n- 100 requests/day\n- Subject to availability\n\n**Paid Models:**\n- Varies by provider\n- Typically 500-10,000 requests/minute\n- Contact provider for higher limits\n\n### Context Limits\n\nIf your prompt + context exceeds model limits:\n\n**Options:**\n1. Use model with larger context\n2. Reduce RAG chunk count\n3. Summarize context\n4. Split into multiple requests\n\n### Cost Limits\n\nSet per-execution cost limits:\n\n```json\n{\n  \"maxCostPerExecution\": 0.50,\n  \"action\": \"fallback_to_cheaper_model\"\n}\n```\n\n## Testing Models\n\n### A/B Testing\n\nCompare models on same prompt:\n\n1. Create test prompt\n2. Execute with Model A\n3. Execute with Model B\n4. Compare:\n   - Output quality\n   - Cost\n   - Speed\n   - Consistency\n\n### Evaluation Criteria\n\n**Quality Metrics:**\n- Accuracy\n- Coherence\n- Relevance\n- Completeness\n\n**Performance Metrics:**\n- Latency (p50, p95, p99)\n- Throughput\n- Error rate\n\n**Cost Metrics:**\n- Cost per execution\n- Cost per 1K executions\n- Monthly projected cost\n\n## Best Practices\n\n### 1. Start with Free Models\n\nDevelop and test with free models:\n```\nDevelopment: Grok 2 (Free)\nTesting: GPT-3.5 Turbo\nProduction: GPT-4 Turbo / Claude 3.5 Sonnet\n```\n\n### 2. Match Model to Task\n\nDon't use expensive models for simple tasks:\n```\n❌ GPT-4 Turbo for \"Hello {{name}}\"\n✅ GPT-3.5 Turbo for simple templates\n✅ GPT-4 Turbo for complex analysis\n```\n\n### 3. Monitor Model Performance\n\nTrack metrics over time:\n- Success rate by model\n- Average cost by model\n- Average rating by model\n- Latency by model\n\n### 4. Use Fallbacks\n\nAlways configure fallback models:\n```\nPrimary: Claude 3.5 Sonnet\nFallback 1: GPT-4 Turbo\nFallback 2: GPT-3.5 Turbo\n```\n\n### 5. Optimize for Cost\n\nUse cheaper models when possible:\n```\nSimple tasks: GPT-3.5 Turbo ($0.50/1M)\nComplex tasks: GPT-4 Turbo ($10/1M)\nSavings: 95% on simple tasks\n```\n\n## Troubleshooting\n\n### Model Unavailable\n\n**Symptoms:** \"Model not available\" error\n\n**Solutions:**\n- Check model status page\n- Use fallback model\n- Try again later\n- Contact support\n\n### Context Too Large\n\n**Symptoms:** \"Context exceeds limit\" error\n\n**Solutions:**\n- Use model with larger context\n- Reduce RAG chunks\n- Summarize input\n- Split request\n\n### Unexpected Costs\n\n**Symptoms:** Higher than expected bills\n\n**Solutions:**\n- Review execution history\n- Check model usage\n- Set cost alerts\n- Use cheaper models\n\n## Next Steps\n\n- [Token Usage and Cost Tracking](/dashboard/help/core-features/token-costs)\n- [Execution History and Ratings](/dashboard/help/core-features/execution-history)\n- [Model Performance Dashboard](/dashboard/help/core-features/model-performance)\n- [API Integration Guide](/dashboard/help/api/api-integration-guide)",
    "category": "core-features",
    "subcategory": "models",
    "tags": ["models", "comparison", "selection", "cost", "performance"],
    "difficulty": "beginner",
    "lastUpdated": "2025-01-15",
    "featured": true,
    "estimatedReadTime": 16
  },
  {
    "id": "token-costs",
    "slug": "token-costs",
    "title": "Token Usage and Cost Tracking",
    "excerpt": "Understand token consumption, track costs, and optimize spending across models and executions",
    "content": "# Token Usage and Cost Tracking\n\n## Overview\n\nTokens are the fundamental unit of AI model consumption. Understanding token usage is essential for managing costs and optimizing performance.\n\n> [!NOTE]\n> Token costs vary significantly by model. A single execution can cost $0.001 or $1.00+ depending on model and context size.\n\n## What Are Tokens?\n\n### Token Basics\n\nTokens are pieces of text that AI models process:\n\n**Examples:**\n```\n\"Hello\" = 1 token\n\"Hello, world!\" = 4 tokens\n\"artificial intelligence\" = 2 tokens\n\"AI\" = 1 token\n```\n\n**Rules of Thumb:**\n- 1 token ≈ 4 characters\n- 1 token ≈ 0.75 words\n- 100 tokens ≈ 75 words\n- 1,000 tokens ≈ 750 words\n\n### Input vs Output Tokens\n\n**Input Tokens:**\n- Your prompt template\n- Variable values\n- RAG context (retrieved chunks)\n- System instructions\n\n**Output Tokens:**\n- AI-generated response\n- Typically costs 2-3x more than input\n\n**Example:**\n```\nPrompt: \"Summarize this article: {{article_text}}\"\nArticle: 2,000 tokens\nPrompt template: 10 tokens\nTotal Input: 2,010 tokens\n\nResponse: 200 tokens\nTotal Output: 200 tokens\n\nTotal Tokens: 2,210 tokens\n```\n\n## Token Costs by Model\n\n### Pricing Table (per 1M tokens)\n\n| Model | Input Cost | Output Cost | Example Cost |\n|-------|------------|-------------|-------------|\n| GPT-4 Turbo | $10.00 | $30.00 | $0.04 (1K in, 1K out) |\n| GPT-4 | $30.00 | $60.00 | $0.09 (1K in, 1K out) |\n| GPT-3.5 Turbo | $0.50 | $1.50 | $0.002 (1K in, 1K out) |\n| Claude 3.5 Sonnet | $3.00 | $15.00 | $0.018 (1K in, 1K out) |\n| Claude 3 Opus | $15.00 | $75.00 | $0.09 (1K in, 1K out) |\n| Claude 3 Haiku | $0.25 | $1.25 | $0.0015 (1K in, 1K out) |\n| Gemini 1.5 Pro | $1.25 | $5.00 | $0.00625 (1K in, 1K out) |\n| Grok 2 (Free) | $0.00 | $0.00 | $0.00 |\n\n### Cost Calculation Formula\n\n```\nTotal Cost = (Input Tokens / 1,000,000 × Input Price) + \n             (Output Tokens / 1,000,000 × Output Price)\n\nExample (GPT-4 Turbo):\nInput: 2,000 tokens\nOutput: 500 tokens\n\nCost = (2,000 / 1,000,000 × $10) + (500 / 1,000,000 × $30)\n     = $0.02 + $0.015\n     = $0.035\n```\n\n## Viewing Token Usage\n\n### In Execution History\n\nEach execution shows:\n```\nToken Usage:\n├── Input Tokens: 2,345\n│   ├── Prompt: 345 tokens\n│   └── RAG Context: 2,000 tokens\n├── Output Tokens: 456\n└── Total Tokens: 2,801\n\nEstimated Cost: $0.0421\n```\n\n### In Analytics Dashboard\n\nView aggregate metrics:\n- Total tokens (last 7/30/90 days)\n- Tokens by model\n- Tokens by prompt\n- Tokens by user\n- Cost trends over time\n\n### Real-Time Estimates\n\nBefore execution, see estimates:\n```\nEstimated Token Usage:\n├── Prompt: ~350 tokens\n├── Variables: ~200 tokens\n├── RAG Context: ~2,000 tokens (5 chunks)\n└── Total Input: ~2,550 tokens\n\nEstimated Output: ~500 tokens\nEstimated Cost: $0.04 - $0.06\n```\n\n## Cost Optimization Strategies\n\n### 1. Choose the Right Model\n\n**Cost Comparison (1K input, 1K output):**\n```\nGPT-4 Turbo:      $0.040  (baseline)\nClaude 3.5:       $0.018  (55% cheaper)\nGPT-3.5 Turbo:    $0.002  (95% cheaper)\nClaude 3 Haiku:   $0.0015 (96% cheaper)\n```\n\n**Strategy:**\n- Use GPT-3.5 Turbo for simple tasks\n- Use Claude 3.5 Sonnet for balanced needs\n- Reserve GPT-4 Turbo for complex tasks\n\n### 2. Optimize RAG Context\n\n**Problem:**\n```\nRetrieving 10 chunks × 500 tokens = 5,000 tokens\nCost with GPT-4 Turbo: $0.05 per execution\n```\n\n**Solution:**\n```\nRetrieve 5 chunks × 500 tokens = 2,500 tokens\nCost with GPT-4 Turbo: $0.025 per execution\nSavings: 50%\n```\n\n**Best Practices:**\n- Retrieve 3-5 chunks (not 10+)\n- Use smaller chunk sizes (512 vs 1024 tokens)\n- Implement relevance threshold (>0.7)\n- Filter by metadata before retrieval\n\n### 3. Reduce Prompt Size\n\n**Before:**\n```\nYou are an expert customer support agent with 10 years of experience.\nYou should always be polite, professional, and helpful.\nYou should provide detailed answers with examples.\nYou should ask clarifying questions when needed.\n...(500 tokens of instructions)\n\nCustomer question: {{question}}\n```\n\n**After:**\n```\nProvide a helpful, professional response to:\n{{question}}\n```\n\n**Savings:** 450 tokens per execution\n\n### 4. Limit Output Length\n\nSet `max_tokens` parameter:\n```json\n{\n  \"model\": \"gpt-4-turbo\",\n  \"max_tokens\": 500,\n  \"temperature\": 0.7\n}\n```\n\n**Impact:**\n- Prevents runaway costs\n- Ensures concise responses\n- Predictable pricing\n\n### 5. Cache Responses\n\nFor repeated queries:\n```\nFirst execution: $0.04\nCached response: $0.00\nSavings: 100% on cache hits\n```\n\n**Implementation:**\n- Cache by prompt + variables hash\n- Set TTL (1 hour - 24 hours)\n- Invalidate on prompt changes\n\n### 6. Batch Processing\n\nProcess multiple items in one request:\n\n**Individual Requests:**\n```\n10 executions × $0.04 = $0.40\n```\n\n**Batched Request:**\n```\n1 execution with 10 items = $0.15\nSavings: 62.5%\n```\n\n## Cost Monitoring\n\n### Daily Cost Tracking\n\nMonitor daily spending:\n```\nToday:     $12.45 (↑ 15% vs yesterday)\nYesterday: $10.82\nLast 7d:   $78.34 (avg $11.19/day)\nLast 30d:  $324.56 (avg $10.82/day)\n```\n\n### Cost Alerts\n\nSet up alerts:\n```json\n{\n  \"alerts\": [\n    {\n      \"type\": \"daily_threshold\",\n      \"threshold\": 50.00,\n      \"action\": \"email\"\n    },\n    {\n      \"type\": \"execution_threshold\",\n      \"threshold\": 1.00,\n      \"action\": \"require_approval\"\n    },\n    {\n      \"type\": \"monthly_budget\",\n      \"threshold\": 1000.00,\n      \"action\": \"pause_executions\"\n    }\n  ]\n}\n```\n\n### Cost Attribution\n\n**By Prompt:**\n```\nCustomer Support:  $45.23 (35%)\nContent Gen:       $32.18 (25%)\nData Analysis:     $28.91 (22%)\nOther:             $23.68 (18%)\n```\n\n**By Model:**\n```\nGPT-4 Turbo:       $67.89 (52%)\nClaude 3.5:        $42.34 (32%)\nGPT-3.5 Turbo:     $19.77 (16%)\n```\n\n**By User:**\n```\nuser@example.com:  $78.45 (60%)\nteam@example.com:  $51.55 (40%)\n```\n\n## Budget Management\n\n### Setting Budgets\n\n**Organization Level:**\n```json\n{\n  \"monthlyBudget\": 1000.00,\n  \"alertThreshold\": 0.8,\n  \"hardLimit\": true\n}\n```\n\n**User Level:**\n```json\n{\n  \"dailyBudget\": 50.00,\n  \"monthlyBudget\": 500.00,\n  \"alertThreshold\": 0.9\n}\n```\n\n**Prompt Level:**\n```json\n{\n  \"maxCostPerExecution\": 0.50,\n  \"dailyBudget\": 10.00\n}\n```\n\n### Budget Enforcement\n\n**Soft Limits (Alerts):**\n- Send email at 80% of budget\n- Send warning at 90% of budget\n- Require approval at 95% of budget\n\n**Hard Limits (Blocking):**\n- Pause executions at 100% of budget\n- Resume next billing period\n- Override with admin approval\n\n## Cost Forecasting\n\n### Projected Monthly Cost\n\nBased on current usage:\n```\nCurrent daily average: $12.45\nDays remaining: 15\nProjected month total: $373.50\n\nBudget: $500.00\nRemaining: $126.50\nStatus: ✅ On track\n```\n\n### Trend Analysis\n\n```\nWeek 1: $78.34 (avg $11.19/day)\nWeek 2: $87.15 (avg $12.45/day) ↑ 11%\nWeek 3: $95.67 (avg $13.67/day) ↑ 10%\nWeek 4: Projected $109.34 (avg $15.62/day) ↑ 14%\n\nTrend: Increasing\nAction: Review high-cost prompts\n```\n\n## Best Practices\n\n### 1. Monitor Regularly\n\nCheck costs daily:\n- Review execution history\n- Identify expensive prompts\n- Optimize high-cost operations\n\n### 2. Set Alerts\n\nNever exceed budget:\n- Daily threshold alerts\n- Monthly budget alerts\n- Per-execution limits\n\n### 3. Optimize Prompts\n\nReduce token usage:\n- Shorter prompts\n- Fewer RAG chunks\n- Smaller outputs\n\n### 4. Use Appropriate Models\n\nMatch model to task:\n- Simple tasks → Cheap models\n- Complex tasks → Expensive models\n- Test with free models first\n\n### 5. Cache Aggressively\n\nReuse responses:\n- Cache common queries\n- Set appropriate TTL\n- Invalidate on changes\n\n## Troubleshooting\n\n### Unexpected High Costs\n\n**Symptoms:** Bill higher than expected\n\n**Causes:**\n- Large RAG context\n- Expensive model usage\n- High execution volume\n- Long outputs\n\n**Solutions:**\n- Review execution history\n- Check token usage by prompt\n- Optimize RAG configuration\n- Switch to cheaper models\n\n### Token Count Mismatch\n\n**Symptoms:** Estimated vs actual tokens differ\n\n**Causes:**\n- Variable content size varies\n- RAG retrieves different chunks\n- Model adds extra tokens\n\n**Solutions:**\n- Use actual token counts from history\n- Add buffer to estimates (20%)\n- Monitor actual usage\n\n### Budget Exceeded\n\n**Symptoms:** Executions paused\n\n**Causes:**\n- Usage spike\n- Budget too low\n- Inefficient prompts\n\n**Solutions:**\n- Increase budget\n- Optimize prompts\n- Review high-cost executions\n- Implement cost controls\n\n## Next Steps\n\n- [Model Selection and Comparison](/dashboard/help/core-features/model-selection)\n- [Execution History and Ratings](/dashboard/help/core-features/execution-history)\n- [Analytics Dashboard Overview](/dashboard/help/core-features/analytics-overview)\n- [Chunking and Retrieval Configuration](/dashboard/help/core-features/rag-chunking-config)",
    "category": "core-features",
    "subcategory": "cost",
    "tags": ["tokens", "cost", "pricing", "budget", "optimization"],
    "difficulty": "beginner",
    "lastUpdated": "2025-01-15",
    "featured": true,
    "estimatedReadTime": 15
  },
  {
    "id": "analytics-overview",
    "slug": "analytics-overview",
    "title": "Analytics Dashboard Overview",
    "excerpt": "Monitor performance, track metrics, and gain insights with comprehensive analytics",
    "content": "# Analytics Dashboard Overview\n\n## Overview\n\nThe Analytics Dashboard provides real-time insights into your prompt usage, performance, costs, and quality metrics.\n\n> [!TIP]\n> Use analytics to identify trends, optimize costs, and improve prompt quality over time.\n\n## Accessing Analytics\n\n1. Navigate to **Dashboard** → **Analytics**\n2. Select time period (24h, 7d, 30d, 90d, custom)\n3. Filter by prompt, model, user, or workspace\n\n## Key Metrics\n\n### Execution Metrics\n\n**Total Executions:**\n```\nLast 30 days: 1,234 executions\n↑ 15% vs previous period\n```\n\n**Success Rate:**\n```\n98.5% successful\n1.2% failed\n0.3% cancelled\n\nTarget: >95%\n```\n\n**Average Duration:**\n```\n3.2 seconds per execution\n↓ 0.5s vs previous period\n\nBreakdown:\n- p50: 2.1s\n- p95: 5.8s\n- p99: 12.3s\n```\n\n### Quality Metrics\n\n**Average Rating:**\n```\n4.3 / 5.0 stars\n↑ 0.2 vs previous period\n\nDistribution:\n5 stars: 45%\n4 stars: 35%\n3 stars: 15%\n2 stars: 3%\n1 star: 2%\n```\n\n**Rating Trends:**\n- View ratings over time\n- Compare by prompt\n- Identify quality issues\n\n### Cost Metrics\n\n**Total Cost:**\n```\nLast 30 days: $324.56\n↑ 8% vs previous period\n\nBreakdown:\n- Input tokens: $145.23 (45%)\n- Output tokens: $179.33 (55%)\n```\n\n**Cost per Execution:**\n```\nAverage: $0.26\n↓ $0.03 vs previous period\n\nBy model:\n- GPT-4 Turbo: $0.45\n- Claude 3.5: $0.28\n- GPT-3.5: $0.08\n```\n\n**Budget Status:**\n```\nMonthly budget: $500.00\nSpent: $324.56 (65%)\nRemaining: $175.44 (35%)\nProjected: $432.75 (87%)\n\nStatus: ✅ On track\n```\n\n### Token Metrics\n\n**Total Tokens:**\n```\nLast 30 days: 12.5M tokens\n↑ 12% vs previous period\n\nBreakdown:\n- Input: 8.2M (66%)\n- Output: 4.3M (34%)\n```\n\n**Tokens per Execution:**\n```\nAverage: 10,130 tokens\n\nBy component:\n- Prompt: 1,200 tokens (12%)\n- Variables: 800 tokens (8%)\n- RAG context: 6,500 tokens (64%)\n- Output: 1,630 tokens (16%)\n```\n\n## Dashboard Sections\n\n### 1. Overview Tab\n\nHigh-level summary:\n- Key metrics cards\n- Trend charts (7/30/90 days)\n- Quick insights\n- Alerts and notifications\n\n### 2. Executions Tab\n\nDetailed execution analytics:\n- Execution volume over time\n- Success/failure rates\n- Duration distribution\n- Model usage breakdown\n\n### 3. Quality Tab\n\nQuality metrics:\n- Average ratings over time\n- Rating distribution\n- Top-rated prompts\n- Low-rated executions\n\n### 4. Cost Tab\n\nCost analytics:\n- Daily/weekly/monthly costs\n- Cost by prompt\n- Cost by model\n- Cost by user\n- Budget tracking\n\n### 5. Performance Tab\n\nPerformance metrics:\n- Latency trends\n- Token usage\n- RAG performance\n- Model comparison\n\n### 6. Usage Tab\n\nUsage patterns:\n- Most used prompts\n- Most used models\n- Peak usage times\n- User activity\n\n## Filtering and Segmentation\n\n### Time Filters\n\n- Last 24 hours\n- Last 7 days\n- Last 30 days\n- Last 90 days\n- Custom date range\n\n### Dimension Filters\n\n**By Prompt:**\n```\nFilter: prompt_id = \"customer-support-v2\"\nView metrics for specific prompt\n```\n\n**By Model:**\n```\nFilter: model = \"gpt-4-turbo\"\nCompare model performance\n```\n\n**By User:**\n```\nFilter: user_id = \"user@example.com\"\nTrack individual usage\n```\n\n**By Workspace:**\n```\nFilter: workspace = \"production\"\nSegment by environment\n```\n\n**By Status:**\n```\nFilter: status = \"failed\"\nAnalyze failures\n```\n\n### Multi-Dimension Analysis\n\nCombine filters:\n```\nPrompt: \"customer-support-v2\"\nModel: \"gpt-4-turbo\"\nTime: Last 30 days\nStatus: Successful\n\nResults: 456 executions\nAvg rating: 4.5 stars\nAvg cost: $0.42\n```\n\n## Charts and Visualizations\n\n### Line Charts\n\n**Execution Volume:**\n```\nX-axis: Time (daily/weekly)\nY-axis: Number of executions\nLines: Total, Successful, Failed\n```\n\n**Cost Trends:**\n```\nX-axis: Time\nY-axis: Cost ($)\nLines: Daily cost, Cumulative cost, Budget\n```\n\n### Bar Charts\n\n**Top Prompts:**\n```\nX-axis: Prompt name\nY-axis: Execution count\nSort: Descending\n```\n\n**Model Distribution:**\n```\nX-axis: Model name\nY-axis: Percentage of executions\n```\n\n### Pie Charts\n\n**Cost Breakdown:**\n```\nSegments:\n- GPT-4 Turbo: 52%\n- Claude 3.5: 32%\n- GPT-3.5: 16%\n```\n\n**Status Distribution:**\n```\nSegments:\n- Success: 98.5%\n- Failed: 1.2%\n- Cancelled: 0.3%\n```\n\n### Heatmaps\n\n**Usage by Hour:**\n```\nX-axis: Hour of day (0-23)\nY-axis: Day of week\nColor: Execution count\n```\n\n## Insights and Recommendations\n\n### Automated Insights\n\nThe dashboard automatically identifies:\n\n**Cost Anomalies:**\n```\n⚠️ Cost spike detected\nYesterday: $45.23 (+120% vs avg)\nCause: 234 executions with GPT-4 Turbo\nRecommendation: Review prompt \"data-analysis-v3\"\n```\n\n**Quality Issues:**\n```\n⚠️ Rating drop detected\nPrompt: \"customer-support-v2\"\nAvg rating: 3.2 (↓ 1.1 vs last week)\nRecommendation: Review recent changes\n```\n\n**Performance Degradation:**\n```\n⚠️ Latency increase detected\nAvg duration: 8.5s (↑ 3.2s vs last week)\nCause: Increased RAG chunk count\nRecommendation: Optimize retrieval config\n```\n\n### Optimization Recommendations\n\n**Cost Optimization:**\n```\n💡 Potential savings: $45/month\n\nActions:\n1. Switch \"simple-task\" prompt to GPT-3.5 Turbo\n   Savings: $25/month\n2. Reduce RAG chunks from 10 to 5\n   Savings: $15/month\n3. Enable response caching\n   Savings: $5/month\n```\n\n**Quality Improvement:**\n```\n💡 Improve average rating from 4.3 to 4.5\n\nActions:\n1. Update low-rated prompt \"email-gen-v1\"\n2. Add more RAG context to \"support-v2\"\n3. Increase temperature for \"creative-writing\"\n```\n\n## Exporting Analytics Data\n\n### Export Formats\n\n**CSV Export:**\n```csv\nDate,Executions,Success Rate,Avg Rating,Total Cost\n2025-01-15,45,97.8%,4.3,$12.45\n2025-01-14,52,98.1%,4.2,$14.23\n```\n\n**JSON Export:**\n```json\n{\n  \"period\": \"2025-01-01 to 2025-01-31\",\n  \"metrics\": {\n    \"totalExecutions\": 1234,\n    \"successRate\": 0.985,\n    \"avgRating\": 4.3,\n    \"totalCost\": 324.56\n  }\n}\n```\n\n**PDF Report:**\n- Executive summary\n- Key metrics\n- Charts and graphs\n- Recommendations\n\n### Scheduled Reports\n\nAutomate report delivery:\n```json\n{\n  \"frequency\": \"weekly\",\n  \"day\": \"monday\",\n  \"time\": \"09:00\",\n  \"recipients\": [\"team@example.com\"],\n  \"format\": \"pdf\",\n  \"sections\": [\"overview\", \"cost\", \"quality\"]\n}\n```\n\n## Custom Dashboards\n\n### Creating Custom Views\n\n1. Click **Customize Dashboard**\n2. Add/remove widgets\n3. Arrange layout\n4. Save as preset\n\n### Available Widgets\n\n- Metric cards (executions, cost, rating)\n- Line charts (trends over time)\n- Bar charts (comparisons)\n- Pie charts (distributions)\n- Tables (top prompts, recent executions)\n- Heatmaps (usage patterns)\n\n### Sharing Dashboards\n\nShare with team:\n- Generate shareable link\n- Set permissions (view/edit)\n- Embed in external tools\n\n## Integration with Other Tools\n\n### Slack Notifications\n\n```json\n{\n  \"trigger\": \"daily_summary\",\n  \"channel\": \"#analytics\",\n  \"message\": \"Yesterday: 45 executions, $12.45 cost, 4.3 avg rating\"\n}\n```\n\n### Email Alerts\n\n```json\n{\n  \"trigger\": \"budget_threshold\",\n  \"threshold\": 0.8,\n  \"recipients\": [\"admin@example.com\"],\n  \"subject\": \"Budget Alert: 80% of monthly budget used\"\n}\n```\n\n### API Access\n\nQuery analytics via API:\n```bash\ncurl -X GET \"https://api.ethosprompt.com/v1/analytics\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -d '{\n    \"startDate\": \"2025-01-01\",\n    \"endDate\": \"2025-01-31\",\n    \"metrics\": [\"executions\", \"cost\", \"rating\"]\n  }'\n```\n\n## Best Practices\n\n### 1. Monitor Daily\n\nCheck analytics every day:\n- Review key metrics\n- Identify anomalies\n- Track budget\n\n### 2. Set Up Alerts\n\nNever miss important events:\n- Cost thresholds\n- Quality drops\n- Performance issues\n\n### 3. Track Trends\n\nLook for patterns:\n- Weekly/monthly trends\n- Seasonal variations\n- Growth patterns\n\n### 4. Compare Periods\n\nMeasure improvement:\n- Week over week\n- Month over month\n- Before/after changes\n\n### 5. Share Insights\n\nKeep team informed:\n- Weekly reports\n- Monthly summaries\n- Quarterly reviews\n\n## Troubleshooting\n\n### Data Not Updating\n\n**Symptoms:** Metrics not refreshing\n\n**Solutions:**\n- Refresh page\n- Clear cache\n- Check time filter\n- Verify executions exist\n\n### Incorrect Metrics\n\n**Symptoms:** Numbers don't match expectations\n\n**Solutions:**\n- Check filters\n- Verify time zone\n- Review calculation method\n- Contact support\n\n### Slow Dashboard\n\n**Symptoms:** Dashboard loads slowly\n\n**Solutions:**\n- Reduce time range\n- Remove complex widgets\n- Clear browser cache\n- Use fewer filters\n\n## Next Steps\n\n- [Execution History and Ratings](/dashboard/help/core-features/execution-history)\n- [Token Usage and Cost Tracking](/dashboard/help/core-features/token-costs)\n- [Model Performance Dashboard](/dashboard/help/core-features/model-performance)\n- [Real-time Metrics and Alerts](/dashboard/help/core-features/metrics-alerts)",
    "category": "core-features",
    "subcategory": "analytics",
    "tags": ["analytics", "metrics", "dashboard", "insights", "reporting"],
    "difficulty": "beginner",
    "lastUpdated": "2025-01-15",
    "featured": true,
    "estimatedReadTime": 16
  },
  {
    "id": "workspaces-setup",
    "slug": "workspaces-setup",
    "title": "Workspaces Setup and Management",
    "excerpt": "Organize prompts, documents, and team collaboration with workspaces",
    "content": "# Workspaces Setup and Management\n\n## Overview\n\nWorkspaces help you organize prompts, documents, and team members into logical groups for better collaboration and access control.\n\n> [!TIP]\n> Use workspaces to separate development, staging, and production environments, or to organize by team, project, or client.\n\n## What Are Workspaces?\n\nWorkspaces are containers that group:\n- Prompts\n- Documents (for RAG)\n- Execution history\n- Team members\n- Settings and configurations\n\n### Benefits\n\n**Organization:**\n- Separate projects/clients\n- Environment isolation (dev/staging/prod)\n- Team-based access\n\n**Collaboration:**\n- Share prompts with team\n- Assign roles and permissions\n- Track team activity\n\n**Security:**\n- Access control\n- Data isolation\n- Audit logs\n\n## Creating a Workspace\n\n### Step 1: Navigate to Workspaces\n\n1. Click **Settings** → **Workspaces**\n2. Click **Create Workspace**\n\n### Step 2: Configure Workspace\n\n```\nWorkspace Name: Production Customer Support\nDescription: Production prompts for customer support team\nType: Production\nVisibility: Private\n```\n\n**Fields:**\n- **Name:** Descriptive name (required)\n- **Description:** Purpose and scope (optional)\n- **Type:** Development, Staging, Production\n- **Visibility:** Private (team only) or Public (organization)\n\n### Step 3: Add Team Members\n\n```\nMembers:\n- admin@example.com (Owner)\n- support@example.com (Editor)\n- viewer@example.com (Viewer)\n```\n\n### Step 4: Configure Settings\n\n```json\n{\n  \"defaultModel\": \"gpt-4-turbo\",\n  \"maxCostPerExecution\": 1.00,\n  \"enableRAG\": true,\n  \"requireApproval\": false\n}\n```\n\n## Workspace Roles\n\n### Owner\n\n**Permissions:**\n- ✅ Create/edit/delete prompts\n- ✅ Upload/delete documents\n- ✅ Execute prompts\n- ✅ Manage team members\n- ✅ Configure workspace settings\n- ✅ Delete workspace\n\n**Use case:** Workspace creator, admin\n\n### Editor\n\n**Permissions:**\n- ✅ Create/edit/delete prompts\n- ✅ Upload/delete documents\n- ✅ Execute prompts\n- ✅ View team members\n- ❌ Manage team members\n- ❌ Configure workspace settings\n- ❌ Delete workspace\n\n**Use case:** Team members, developers\n\n### Viewer\n\n**Permissions:**\n- ❌ Create/edit/delete prompts\n- ❌ Upload/delete documents\n- ✅ Execute prompts (read-only)\n- ✅ View team members\n- ❌ Manage team members\n- ❌ Configure workspace settings\n- ❌ Delete workspace\n\n**Use case:** Stakeholders, auditors\n\n### Guest\n\n**Permissions:**\n- ❌ Create/edit/delete prompts\n- ❌ Upload/delete documents\n- ✅ Execute specific prompts (limited)\n- ❌ View team members\n- ❌ Manage team members\n- ❌ Configure workspace settings\n- ❌ Delete workspace\n\n**Use case:** External collaborators, clients\n\n## Managing Team Members\n\n### Adding Members\n\n1. Open workspace settings\n2. Click **Team** tab\n3. Click **Add Member**\n4. Enter email address\n5. Select role\n6. Click **Send Invitation**\n\n**Invitation Email:**\n```\nYou've been invited to join the \"Production Customer Support\" workspace.\n\nRole: Editor\nInvited by: admin@example.com\n\nClick here to accept: [Accept Invitation]\n```\n\n### Changing Roles\n\n1. Open workspace settings\n2. Click **Team** tab\n3. Find member\n4. Click role dropdown\n5. Select new role\n6. Confirm change\n\n### Removing Members\n\n1. Open workspace settings\n2. Click **Team** tab\n3. Find member\n4. Click **Remove**\n5. Confirm removal\n\n> [!WARNING]\n> Removing a member immediately revokes their access to all workspace resources.\n\n## Workspace Settings\n\n### General Settings\n\n**Workspace Name:**\n```\nProduction Customer Support\n```\n\n**Description:**\n```\nProduction prompts and documents for customer support team.\nAll changes require approval.\n```\n\n**Type:**\n- Development\n- Staging\n- Production\n\n**Visibility:**\n- Private (team only)\n- Organization (all org members can view)\n\n### Default Model Settings\n\n```json\n{\n  \"defaultModel\": \"gpt-4-turbo\",\n  \"fallbackModels\": [\"claude-3.5-sonnet\", \"gpt-3.5-turbo\"],\n  \"temperature\": 0.7,\n  \"maxTokens\": 2000\n}\n```\n\n### RAG Settings\n\n```json\n{\n  \"enableRAG\": true,\n  \"chunkSize\": 512,\n  \"chunkOverlap\": 50,\n  \"topK\": 5,\n  \"retrievalStrategy\": \"hybrid\"\n}\n```\n\n### Cost Controls\n\n```json\n{\n  \"maxCostPerExecution\": 1.00,\n  \"dailyBudget\": 50.00,\n  \"monthlyBudget\": 1000.00,\n  \"alertThreshold\": 0.8\n}\n```\n\n### Approval Workflow\n\n```json\n{\n  \"requireApproval\": true,\n  \"approvers\": [\"admin@example.com\"],\n  \"autoApproveBelow\": 0.10\n}\n```\n\n## Organizing Prompts\n\n### Moving Prompts Between Workspaces\n\n1. Open prompt\n2. Click **Settings**\n3. Select **Move to Workspace**\n4. Choose destination workspace\n5. Confirm move\n\n> [!NOTE]\n> Moving a prompt also moves its execution history and associated documents.\n\n### Copying Prompts\n\n1. Open prompt\n2. Click **Duplicate**\n3. Select destination workspace\n4. Optionally rename\n5. Click **Create Copy**\n\n### Sharing Prompts\n\nShare prompts across workspaces:\n\n```json\n{\n  \"sharedWith\": [\n    {\n      \"workspace\": \"staging\",\n      \"permission\": \"read-only\"\n    },\n    {\n      \"workspace\": \"development\",\n      \"permission\": \"editable\"\n    }\n  ]\n}\n```\n\n## Document Management\n\n### Workspace Documents\n\nDocuments are scoped to workspaces:\n- Upload documents to specific workspace\n- Documents only accessible within workspace\n- RAG searches only workspace documents\n\n### Uploading Documents\n\n1. Select workspace\n2. Navigate to **Documents**\n3. Click **Upload**\n4. Select files\n5. Configure processing settings\n6. Click **Process**\n\n### Sharing Documents\n\nShare documents across workspaces:\n\n1. Open document\n2. Click **Share**\n3. Select workspaces\n4. Set permissions (read-only/editable)\n5. Confirm\n\n## Workspace Templates\n\n### Using Templates\n\nQuickly set up common workspace types:\n\n**Customer Support Template:**\n```\nIncludes:\n- 5 pre-built support prompts\n- Sample documents (FAQs, policies)\n- Recommended settings\n- Team roles configured\n```\n\n**Content Generation Template:**\n```\nIncludes:\n- 8 content prompts (blog, social, email)\n- Style guide documents\n- Brand voice settings\n- Approval workflow\n```\n\n**Data Analysis Template:**\n```\nIncludes:\n- 6 analysis prompts\n- Sample datasets\n- Visualization settings\n- Cost controls\n```\n\n### Creating Custom Templates\n\n1. Set up workspace as desired\n2. Click **Save as Template**\n3. Name template\n4. Select what to include:\n   - Prompts\n   - Documents\n   - Settings\n   - Team structure\n5. Save\n\n## Environment Workflows\n\n### Development → Staging → Production\n\n**Development Workspace:**\n```\nPurpose: Experiment and test\nSettings:\n- Use free models\n- No approval required\n- Unlimited executions\n```\n\n**Staging Workspace:**\n```\nPurpose: Pre-production testing\nSettings:\n- Use production models\n- Optional approval\n- Cost alerts enabled\n```\n\n**Production Workspace:**\n```\nPurpose: Live customer-facing\nSettings:\n- Production models only\n- Approval required\n- Strict cost controls\n- Audit logging\n```\n\n### Promotion Workflow\n\n1. Develop prompt in Development\n2. Test thoroughly\n3. Promote to Staging\n4. Validate with real data\n5. Promote to Production\n6. Monitor performance\n\n## Analytics by Workspace\n\n### Workspace Metrics\n\nView analytics per workspace:\n- Execution volume\n- Success rate\n- Average rating\n- Total cost\n- Token usage\n\n### Comparing Workspaces\n\n```\nDevelopment:  234 executions, $12.45 cost\nStaging:      89 executions, $23.67 cost\nProduction:   567 executions, $234.56 cost\n```\n\n### Cost Attribution\n\nTrack costs by workspace:\n```\nTotal monthly cost: $500.00\n\nBreakdown:\n- Production: $350.00 (70%)\n- Staging: $100.00 (20%)\n- Development: $50.00 (10%)\n```\n\n## Best Practices\n\n### 1. Use Descriptive Names\n\n**Good:**\n```\n- Production Customer Support\n- Staging Content Generation\n- Dev Experimental Prompts\n```\n\n**Bad:**\n```\n- Workspace 1\n- Test\n- My Workspace\n```\n\n### 2. Separate Environments\n\nAlways maintain separate workspaces:\n```\n✅ Development (testing)\n✅ Staging (validation)\n✅ Production (live)\n```\n\n### 3. Assign Appropriate Roles\n\n```\nOwner: 1-2 people (admins)\nEditor: Team members\nViewer: Stakeholders\nGuest: External collaborators\n```\n\n### 4. Set Cost Controls\n\nPrevent budget overruns:\n```\nDevelopment: $50/month\nStaging: $100/month\nProduction: $1000/month\n```\n\n### 5. Document Workspace Purpose\n\nAdd clear descriptions:\n```\nProduction Customer Support\n\nPurpose: Live customer-facing support prompts\nOwner: support-team@example.com\nApproval: Required for all changes\nBudget: $1000/month\n```\n\n## Troubleshooting\n\n### Cannot Access Workspace\n\n**Symptoms:** Workspace not visible\n\n**Causes:**\n- Not a member\n- Insufficient permissions\n- Workspace deleted\n\n**Solutions:**\n- Request access from owner\n- Check email for invitation\n- Verify workspace exists\n\n### Cannot Move Prompt\n\n**Symptoms:** Move option disabled\n\n**Causes:**\n- Insufficient permissions\n- Destination workspace full\n- Prompt has dependencies\n\n**Solutions:**\n- Request Editor role\n- Contact workspace owner\n- Copy instead of move\n\n### Cost Alerts Not Working\n\n**Symptoms:** No alerts received\n\n**Causes:**\n- Alerts not configured\n- Email notifications disabled\n- Threshold not reached\n\n**Solutions:**\n- Configure alert settings\n- Check notification preferences\n- Verify threshold values\n\n## Next Steps\n\n- [Shared Prompt Library](/dashboard/help/core-features/shared-library)\n- [Analytics Dashboard Overview](/dashboard/help/core-features/analytics-overview)\n- [Webhook Manager Setup](/dashboard/help/core-features/webhooks-setup)\n- [Security and Privacy Guidelines](/dashboard/help/best-practices/security-privacy)",
    "category": "core-features",
    "subcategory": "workspaces",
    "tags": ["workspaces", "collaboration", "teams", "organization", "permissions"],
    "difficulty": "intermediate",
    "lastUpdated": "2025-01-15",
    "featured": true,
    "estimatedReadTime": 18
  },
  {
    "id": "webhooks-setup",
    "slug": "webhooks-setup",
    "title": "Webhook Manager Setup and Examples",
    "excerpt": "Integrate EthosPrompt with external systems using webhooks for real-time notifications",
    "content": "# Webhook Manager Setup and Examples\n\n## Overview\n\nWebhooks allow EthosPrompt to send real-time notifications to your external systems when events occur, enabling seamless integration with your workflows.\n\n> [!TIP]\n> Use webhooks to trigger actions in other systems, log events, send notifications, or build custom integrations.\n\n## What Are Webhooks?\n\nWebhooks are HTTP callbacks that send event data to a URL you specify when specific events occur in EthosPrompt.\n\n### How Webhooks Work\n\n```\n1. Event occurs in EthosPrompt (e.g., prompt execution completes)\n2. EthosPrompt sends HTTP POST request to your webhook URL\n3. Your server receives the event data\n4. Your server processes the event (log, notify, trigger action)\n5. Your server responds with 200 OK\n```\n\n### Benefits\n\n**Real-Time Integration:**\n- Instant notifications\n- No polling required\n- Event-driven architecture\n\n**Automation:**\n- Trigger workflows\n- Update external systems\n- Send notifications\n\n**Flexibility:**\n- Custom processing logic\n- Integration with any system\n- Build custom dashboards\n\n## Supported Events\n\n### Execution Events\n\n**execution.started:**\n```json\n{\n  \"event\": \"execution.started\",\n  \"timestamp\": \"2025-01-15T10:30:00Z\",\n  \"data\": {\n    \"executionId\": \"exec_123\",\n    \"promptId\": \"prompt_456\",\n    \"userId\": \"user_789\",\n    \"model\": \"gpt-4-turbo\"\n  }\n}\n```\n\n**execution.completed:**\n```json\n{\n  \"event\": \"execution.completed\",\n  \"timestamp\": \"2025-01-15T10:30:05Z\",\n  \"data\": {\n    \"executionId\": \"exec_123\",\n    \"promptId\": \"prompt_456\",\n    \"status\": \"success\",\n    \"duration\": 5.2,\n    \"tokens\": {\n      \"input\": 1234,\n      \"output\": 567\n    },\n    \"cost\": 0.0234\n  }\n}\n```\n\n**execution.failed:**\n```json\n{\n  \"event\": \"execution.failed\",\n  \"timestamp\": \"2025-01-15T10:30:03Z\",\n  \"data\": {\n    \"executionId\": \"exec_123\",\n    \"promptId\": \"prompt_456\",\n    \"error\": \"Model unavailable\",\n    \"errorCode\": \"MODEL_UNAVAILABLE\"\n  }\n}\n```\n\n### Prompt Events\n\n**prompt.created:**\n```json\n{\n  \"event\": \"prompt.created\",\n  \"timestamp\": \"2025-01-15T10:00:00Z\",\n  \"data\": {\n    \"promptId\": \"prompt_456\",\n    \"name\": \"Customer Support V2\",\n    \"userId\": \"user_789\",\n    \"workspaceId\": \"workspace_012\"\n  }\n}\n```\n\n**prompt.updated:**\n```json\n{\n  \"event\": \"prompt.updated\",\n  \"timestamp\": \"2025-01-15T11:00:00Z\",\n  \"data\": {\n    \"promptId\": \"prompt_456\",\n    \"changes\": [\"content\", \"model\"],\n    \"userId\": \"user_789\"\n  }\n}\n```\n\n**prompt.deleted:**\n```json\n{\n  \"event\": \"prompt.deleted\",\n  \"timestamp\": \"2025-01-15T12:00:00Z\",\n  \"data\": {\n    \"promptId\": \"prompt_456\",\n    \"userId\": \"user_789\"\n  }\n}\n```\n\n### Document Events\n\n**document.uploaded:**\n```json\n{\n  \"event\": \"document.uploaded\",\n  \"timestamp\": \"2025-01-15T09:00:00Z\",\n  \"data\": {\n    \"documentId\": \"doc_789\",\n    \"filename\": \"product_manual.pdf\",\n    \"size\": 1024000,\n    \"userId\": \"user_789\"\n  }\n}\n```\n\n**document.processed:**\n```json\n{\n  \"event\": \"document.processed\",\n  \"timestamp\": \"2025-01-15T09:05:00Z\",\n  \"data\": {\n    \"documentId\": \"doc_789\",\n    \"chunks\": 45,\n    \"tokens\": 23456,\n    \"processingTime\": 12.3\n  }\n}\n```\n\n### Cost Events\n\n**cost.threshold_exceeded:**\n```json\n{\n  \"event\": \"cost.threshold_exceeded\",\n  \"timestamp\": \"2025-01-15T15:00:00Z\",\n  \"data\": {\n    \"threshold\": 100.00,\n    \"current\": 105.23,\n    \"period\": \"daily\",\n    \"workspaceId\": \"workspace_012\"\n  }\n}\n```\n\n## Creating a Webhook\n\n### Step 1: Navigate to Webhooks\n\n1. Go to **Settings** → **Webhooks**\n2. Click **Create Webhook**\n\n### Step 2: Configure Webhook\n\n**Basic Settings:**\n```\nName: Slack Notifications\nURL: https://hooks.slack.com/services/YOUR/WEBHOOK/URL\nDescription: Send execution notifications to Slack\n```\n\n**Events to Subscribe:**\n```\n☑ execution.completed\n☑ execution.failed\n☐ execution.started\n☑ cost.threshold_exceeded\n```\n\n**Filters (Optional):**\n```json\n{\n  \"workspaceId\": \"workspace_012\",\n  \"promptId\": \"prompt_456\"\n}\n```\n\n### Step 3: Configure Security\n\n**Secret Key:**\n```\nGenerate a secret key for webhook signature verification\nSecret: whsec_abc123def456...\n```\n\n**Headers (Optional):**\n```json\n{\n  \"Authorization\": \"Bearer YOUR_API_KEY\",\n  \"X-Custom-Header\": \"value\"\n}\n```\n\n### Step 4: Test Webhook\n\n1. Click **Send Test Event**\n2. Select event type\n3. Review payload\n4. Click **Send**\n5. Verify receipt on your server\n\n### Step 5: Activate\n\n1. Review configuration\n2. Click **Activate Webhook**\n3. Monitor delivery logs\n\n## Webhook Payload Structure\n\n### Standard Payload\n\nAll webhooks follow this structure:\n\n```json\n{\n  \"id\": \"evt_abc123\",\n  \"event\": \"execution.completed\",\n  \"timestamp\": \"2025-01-15T10:30:05Z\",\n  \"apiVersion\": \"v1\",\n  \"data\": {\n    // Event-specific data\n  },\n  \"metadata\": {\n    \"workspaceId\": \"workspace_012\",\n    \"userId\": \"user_789\"\n  }\n}\n```\n\n### Headers\n\n```\nContent-Type: application/json\nX-EthosPrompt-Event: execution.completed\nX-EthosPrompt-Signature: sha256=abc123...\nX-EthosPrompt-Delivery: evt_abc123\nUser-Agent: EthosPrompt-Webhooks/1.0\n```\n\n## Verifying Webhook Signatures\n\n### Why Verify?\n\nSignature verification ensures:\n- Webhook is from EthosPrompt\n- Payload hasn't been tampered with\n- Protection against replay attacks\n\n### Verification Process\n\n**Node.js Example:**\n```javascript\nconst crypto = require('crypto');\n\nfunction verifyWebhookSignature(payload, signature, secret) {\n  const hmac = crypto.createHmac('sha256', secret);\n  const digest = 'sha256=' + hmac.update(payload).digest('hex');\n  return crypto.timingSafeEqual(\n    Buffer.from(signature),\n    Buffer.from(digest)\n  );\n}\n\n// Express.js middleware\napp.post('/webhook', (req, res) => {\n  const signature = req.headers['x-ethosprompt-signature'];\n  const payload = JSON.stringify(req.body);\n  \n  if (!verifyWebhookSignature(payload, signature, WEBHOOK_SECRET)) {\n    return res.status(401).send('Invalid signature');\n  }\n  \n  // Process webhook\n  console.log('Event:', req.body.event);\n  res.status(200).send('OK');\n});\n```\n\n**Python Example:**\n```python\nimport hmac\nimport hashlib\n\ndef verify_webhook_signature(payload, signature, secret):\n    expected = 'sha256=' + hmac.new(\n        secret.encode(),\n        payload.encode(),\n        hashlib.sha256\n    ).hexdigest()\n    return hmac.compare_digest(signature, expected)\n\n# Flask example\n@app.route('/webhook', methods=['POST'])\ndef webhook():\n    signature = request.headers.get('X-EthosPrompt-Signature')\n    payload = request.get_data(as_text=True)\n    \n    if not verify_webhook_signature(payload, signature, WEBHOOK_SECRET):\n        return 'Invalid signature', 401\n    \n    # Process webhook\n    event = request.json\n    print(f\"Event: {event['event']}\")\n    return 'OK', 200\n```\n\n## Example Integrations\n\n### Slack Notifications\n\n**Webhook Handler:**\n```javascript\napp.post('/webhook/slack', async (req, res) => {\n  const { event, data } = req.body;\n  \n  if (event === 'execution.completed') {\n    await sendSlackMessage({\n      channel: '#executions',\n      text: `✅ Execution completed: ${data.promptId}`,\n      attachments: [{\n        color: 'good',\n        fields: [\n          { title: 'Duration', value: `${data.duration}s`, short: true },\n          { title: 'Cost', value: `$${data.cost}`, short: true },\n          { title: 'Tokens', value: data.tokens.input + data.tokens.output, short: true }\n        ]\n      }]\n    });\n  }\n  \n  res.status(200).send('OK');\n});\n```\n\n### Email Notifications\n\n**Webhook Handler:**\n```javascript\napp.post('/webhook/email', async (req, res) => {\n  const { event, data } = req.body;\n  \n  if (event === 'execution.failed') {\n    await sendEmail({\n      to: 'admin@example.com',\n      subject: `Execution Failed: ${data.promptId}`,\n      body: `\n        Execution ${data.executionId} failed.\n        \n        Error: ${data.error}\n        Error Code: ${data.errorCode}\n        Timestamp: ${req.body.timestamp}\n      `\n    });\n  }\n  \n  res.status(200).send('OK');\n});\n```\n\n### Database Logging\n\n**Webhook Handler:**\n```javascript\napp.post('/webhook/log', async (req, res) => {\n  const { event, data, timestamp } = req.body;\n  \n  await db.webhookLogs.insert({\n    event,\n    data,\n    timestamp,\n    receivedAt: new Date()\n  });\n  \n  res.status(200).send('OK');\n});\n```\n\n### Custom Dashboard\n\n**Webhook Handler:**\n```javascript\napp.post('/webhook/dashboard', async (req, res) => {\n  const { event, data } = req.body;\n  \n  if (event === 'execution.completed') {\n    // Update real-time dashboard\n    await updateMetrics({\n      totalExecutions: +1,\n      totalCost: +data.cost,\n      totalTokens: +data.tokens.input + data.tokens.output\n    });\n    \n    // Broadcast to connected clients via WebSocket\n    io.emit('execution_completed', data);\n  }\n  \n  res.status(200).send('OK');\n});\n```\n\n## Retry Logic\n\n### Automatic Retries\n\nEthosPrompt automatically retries failed webhook deliveries:\n\n**Retry Schedule:**\n```\nAttempt 1: Immediate\nAttempt 2: 1 minute later\nAttempt 3: 5 minutes later\nAttempt 4: 15 minutes later\nAttempt 5: 1 hour later\nAttempt 6: 6 hours later\n```\n\n**Retry Conditions:**\n- HTTP status 5xx (server error)\n- Network timeout\n- Connection refused\n\n**No Retry:**\n- HTTP status 2xx (success)\n- HTTP status 4xx (client error)\n- Invalid URL\n\n### Manual Retry\n\n1. Go to **Webhooks** → **Delivery Logs**\n2. Find failed delivery\n3. Click **Retry**\n4. Confirm\n\n## Monitoring Webhooks\n\n### Delivery Logs\n\nView all webhook deliveries:\n```\nEvent: execution.completed\nURL: https://hooks.slack.com/...\nStatus: 200 OK\nDuration: 234ms\nTimestamp: 2025-01-15 10:30:05\n```\n\n### Delivery Status\n\n**Success:**\n```\n✅ 200 OK\nDelivered successfully\n```\n\n**Failed:**\n```\n❌ 500 Internal Server Error\nRetrying... (Attempt 2/6)\n```\n\n**Disabled:**\n```\n⏸️ Webhook disabled\nToo many failures (>10 in 24h)\n```\n\n### Metrics\n\n**Delivery Rate:**\n```\nLast 24h: 234 deliveries\nSuccess: 230 (98.3%)\nFailed: 4 (1.7%)\n```\n\n**Average Latency:**\n```\np50: 123ms\np95: 456ms\np99: 1.2s\n```\n\n## Best Practices\n\n### 1. Respond Quickly\n\nReturn 200 OK immediately:\n```javascript\napp.post('/webhook', async (req, res) => {\n  // Respond immediately\n  res.status(200).send('OK');\n  \n  // Process asynchronously\n  processWebhookAsync(req.body);\n});\n```\n\n### 2. Verify Signatures\n\nAlways verify webhook signatures:\n```javascript\nif (!verifySignature(payload, signature, secret)) {\n  return res.status(401).send('Invalid signature');\n}\n```\n\n### 3. Handle Duplicates\n\nUse event ID to prevent duplicate processing:\n```javascript\nconst eventId = req.body.id;\nif (await isProcessed(eventId)) {\n  return res.status(200).send('Already processed');\n}\nawait markAsProcessed(eventId);\n```\n\n### 4. Implement Idempotency\n\nEnsure operations can be safely retried:\n```javascript\n// Use unique keys for database operations\nawait db.upsert({ id: eventId, data });\n```\n\n### 5. Monitor Failures\n\nSet up alerts for webhook failures:\n```json\n{\n  \"alert\": \"webhook_failures\",\n  \"threshold\": 5,\n  \"period\": \"1h\",\n  \"action\": \"email_admin\"\n}\n```\n\n## Troubleshooting\n\n### Webhook Not Receiving Events\n\n**Symptoms:** No webhook deliveries\n\n**Causes:**\n- Webhook disabled\n- URL incorrect\n- Firewall blocking\n- Event filters too restrictive\n\n**Solutions:**\n- Verify webhook is active\n- Test URL with curl\n- Check firewall rules\n- Review event filters\n\n### Signature Verification Failing\n\n**Symptoms:** 401 errors in logs\n\n**Causes:**\n- Wrong secret key\n- Payload modified\n- Encoding issues\n\n**Solutions:**\n- Verify secret key\n- Use raw payload for verification\n- Check character encoding\n\n### High Failure Rate\n\n**Symptoms:** Many failed deliveries\n\n**Causes:**\n- Server downtime\n- Slow response times\n- Invalid responses\n\n**Solutions:**\n- Check server health\n- Optimize webhook handler\n- Return 200 OK quickly\n- Process asynchronously\n\n## Next Steps\n\n- [API Integration Guide](/dashboard/help/api/api-integration-guide)\n- [Integration Patterns](/dashboard/help/core-features/integration-patterns)\n- [Security and Privacy Guidelines](/dashboard/help/best-practices/security-privacy)\n- [Workspaces Setup and Management](/dashboard/help/core-features/workspaces-setup)",
    "category": "core-features",
    "subcategory": "integrations",
    "tags": ["webhooks", "integrations", "api", "notifications", "automation"],
    "difficulty": "advanced",
    "lastUpdated": "2025-01-15",
    "featured": true,
    "estimatedReadTime": 20
  },
  {
    "id": "template-library",
    "slug": "template-library",
    "title": "Template Library Usage",
    "excerpt": "Browse, customize, and save pre-built prompt templates for common use cases",
    "content": "# Template Library Usage\n\n## Overview\n\nThe Template Library provides pre-built, production-ready prompts for common use cases. Browse templates, customize them, and save as your own prompts.\n\n> [!TIP]\n> Templates are a great starting point. Customize them to match your specific needs and brand voice.\n\n## Accessing the Template Library\n\n1. Navigate to **Dashboard** → **Templates**\n2. Browse by category or search\n3. Click any template to preview\n\n## Template Categories\n\n### Customer Support\n- Email response templates\n- FAQ generation\n- Complaint handling\n- Product recommendations\n\n### Content Generation\n- Blog post outlines\n- Social media posts\n- Email campaigns\n- Product descriptions\n\n### Data Analysis\n- Report summarization\n- Sentiment analysis\n- Data extraction\n- Trend identification\n\n### Code & Technical\n- Code review\n- Documentation generation\n- Bug analysis\n- API documentation\n\n## Using a Template\n\n### Step 1: Browse Templates\n\nFilter by:\n- Category\n- Difficulty level\n- Model compatibility\n- RAG enabled/disabled\n\n### Step 2: Preview Template\n\nView:\n- Template description\n- Variables required\n- Example output\n- Model recommendations\n- Estimated cost\n\n### Step 3: Customize Template\n\nEdit:\n- Prompt content\n- Variable names\n- Model settings\n- RAG configuration\n\n### Step 4: Save as Personal Prompt\n\n1. Click **Save as My Prompt**\n2. Name your prompt\n3. Select workspace\n4. Click **Create**\n\n## Template Structure\n\nEach template includes:\n\n```json\n{\n  \"name\": \"Customer Support Email Response\",\n  \"description\": \"Generate professional email responses to customer inquiries\",\n  \"category\": \"customer-support\",\n  \"difficulty\": \"beginner\",\n  \"content\": \"Generate a professional email response to this customer inquiry:\\n\\n{{customer_message}}\\n\\nTone: {{tone}}\\nInclude: {{key_points}}\",\n  \"variables\": [\n    {\"name\": \"customer_message\", \"description\": \"The customer's inquiry\", \"required\": true},\n    {\"name\": \"tone\", \"description\": \"Response tone (professional, friendly, formal)\", \"required\": true},\n    {\"name\": \"key_points\", \"description\": \"Key points to address\", \"required\": false}\n  ],\n  \"recommendedModel\": \"gpt-4-turbo\",\n  \"estimatedCost\": 0.02,\n  \"ragEnabled\": false\n}\n```\n\n## Customization Best Practices\n\n### 1. Adjust Variables\n\nRename variables to match your use case:\n```\nTemplate: {{customer_message}}\nYour version: {{support_ticket}}\n```\n\n### 2. Add Brand Voice\n\nInclude your brand guidelines:\n```\nTone: Professional and empathetic\nBrand voice: [Your brand voice guidelines]\nSign-off: [Your company signature]\n```\n\n### 3. Optimize for Your Model\n\nAdjust for your preferred model:\n- GPT-4: More detailed instructions\n- GPT-3.5: Simpler, more direct\n- Claude: Conversational style\n\n### 4. Enable RAG if Needed\n\nAdd document context:\n```\nSearch my knowledge base for information about {{topic}}.\nUse the retrieved context to answer: {{question}}\n```\n\n## Popular Templates\n\n### Email Response Generator\n```\nGenerate a {{tone}} email response to:\n\n{{customer_inquiry}}\n\nKey points to address:\n{{key_points}}\n\nSign-off: {{signature}}\n```\n\n### Blog Post Outline\n```\nCreate a blog post outline for:\n\nTopic: {{topic}}\nTarget audience: {{audience}}\nWord count: {{word_count}}\nKey takeaways: {{takeaways}}\n```\n\n### Data Summary\n```\nSummarize this data:\n\n{{data_input}}\n\nFocus on:\n- Key trends\n- Anomalies\n- Recommendations\n\nFormat: {{output_format}}\n```\n\n## Sharing Templates\n\nShare your customized templates:\n\n1. Open your prompt\n2. Click **Share as Template**\n3. Add description and tags\n4. Choose visibility (private/team/public)\n5. Submit for review\n\n## Next Steps\n\n- [Creating Your First Prompt](/dashboard/help/getting-started/creating-first-prompt)\n- [Prompt Variables Syntax](/dashboard/help/core-features/variables-syntax)\n- [Shared Prompt Library](/dashboard/help/core-features/shared-library)",
    "category": "core-features",
    "subcategory": "templates",
    "tags": ["templates", "library", "customization", "getting-started"],
    "difficulty": "beginner",
    "lastUpdated": "2025-01-15",
    "featured": false,
    "estimatedReadTime": 8
  },
  {
    "id": "prompt-quality-tools",
    "slug": "prompt-quality-tools",
    "title": "Prompt Quality Tools",
    "excerpt": "Improve prompt quality with automated analysis, scoring, and AI-powered suggestions",
    "content": "# Prompt Quality Tools\n\n## Overview\n\nEthosPrompt provides built-in tools to analyze and improve your prompts. Get quality scores, identify issues, and receive AI-powered suggestions for optimization.\n\n> [!TIP]\n> Use quality tools during prompt development to catch issues early and optimize for better results.\n\n## Quality Analyzer\n\n### Accessing the Analyzer\n\n1. Open any prompt in edit mode\n2. Click **Analyze Quality** button\n3. View quality report\n\n### Quality Score\n\nPrompts receive a score from 0-100:\n\n**90-100: Excellent**\n- Clear, specific instructions\n- Well-structured variables\n- Appropriate length\n- Good examples provided\n\n**70-89: Good**\n- Minor improvements possible\n- Generally well-written\n- May lack some specificity\n\n**50-69: Fair**\n- Needs improvement\n- Vague instructions\n- Missing context\n\n**0-49: Poor**\n- Major issues detected\n- Requires significant revision\n- May produce inconsistent results\n\n### Analysis Dimensions\n\n**Clarity (0-25 points):**\n```\n✓ Instructions are clear and unambiguous\n✓ No conflicting requirements\n✓ Expected output is well-defined\n```\n\n**Specificity (0-25 points):**\n```\n✓ Specific examples provided\n✓ Clear constraints defined\n✓ Output format specified\n```\n\n**Structure (0-25 points):**\n```\n✓ Logical organization\n✓ Proper use of sections\n✓ Variables well-named\n```\n\n**Completeness (0-25 points):**\n```\n✓ All necessary context included\n✓ Edge cases addressed\n✓ Fallback instructions provided\n```\n\n## Quality Assistant\n\n### AI-Powered Suggestions\n\nThe Quality Assistant analyzes your prompt and provides specific recommendations:\n\n**Example Report:**\n```\nQuality Score: 72/100 (Good)\n\nStrengths:\n✓ Clear variable names\n✓ Good structure\n✓ Specific output format\n\nIssues Found:\n⚠️ Missing examples (Impact: Medium)\n⚠️ Vague tone instruction (Impact: Low)\n⚠️ No error handling (Impact: High)\n\nSuggestions:\n1. Add 2-3 examples of desired output\n2. Replace \"professional tone\" with specific guidelines\n3. Add instructions for handling missing data\n```\n\n### Common Issues Detected\n\n**Vague Instructions:**\n```\n❌ \"Write something good about {{topic}}\"\n✅ \"Write a 200-word blog introduction about {{topic}} \n    highlighting 3 key benefits for {{audience}}\"\n```\n\n**Missing Context:**\n```\n❌ \"Analyze {{data}}\"\n✅ \"Analyze this sales data: {{data}}\n    Focus on: trends, anomalies, recommendations\n    Format: Executive summary with bullet points\"\n```\n\n**Ambiguous Variables:**\n```\n❌ {{input}}, {{data}}, {{text}}\n✅ {{customer_feedback}}, {{sales_data}}, {{product_description}}\n```\n\n**No Examples:**\n```\n❌ \"Generate a product description\"\n✅ \"Generate a product description like this example:\n    [Example description]\n    \n    Now generate for: {{product_name}}\"\n```\n\n## Improvement Workflow\n\n### Step 1: Run Analysis\n\n1. Click **Analyze Quality**\n2. Review score and issues\n3. Read suggestions\n\n### Step 2: Apply Suggestions\n\n1. Click **Apply Suggestion** for auto-fixes\n2. Manually address complex issues\n3. Re-run analysis\n\n### Step 3: Test Improvements\n\n1. Execute prompt with test data\n2. Compare output quality\n3. Iterate as needed\n\n### Step 4: Track Progress\n\nMonitor quality scores over time:\n```\nVersion 1: 58/100 (Fair)\nVersion 2: 74/100 (Good) ↑ 16 points\nVersion 3: 89/100 (Good) ↑ 15 points\n```\n\n## Best Practices\n\n### 1. Aim for 80+ Score\n\nTarget quality score:\n```\nDevelopment: 60+ acceptable\nStaging: 70+ required\nProduction: 80+ required\n```\n\n### 2. Address High-Impact Issues First\n\nPrioritize by impact:\n```\nHigh Impact: Fix immediately\nMedium Impact: Fix before production\nLow Impact: Nice to have\n```\n\n### 3. Use Examples\n\nAlways include examples:\n```\nGood prompts have 2-3 examples\nExcellent prompts have 5+ examples\n```\n\n### 4. Test After Changes\n\nVerify improvements:\n```\n1. Make changes\n2. Re-analyze\n3. Test execution\n4. Compare results\n```\n\n### 5. Version Control\n\nTrack quality over versions:\n```\nv1.0: 65/100\nv1.1: 72/100 (added examples)\nv1.2: 81/100 (clarified instructions)\nv2.0: 92/100 (complete rewrite)\n```\n\n## Quality Metrics Dashboard\n\nView aggregate quality metrics:\n\n**Average Quality Score:**\n```\nAll prompts: 76/100\nProduction: 84/100\nDevelopment: 68/100\n```\n\n**Quality Distribution:**\n```\nExcellent (90+): 12 prompts (15%)\nGood (70-89): 45 prompts (56%)\nFair (50-69): 20 prompts (25%)\nPoor (<50): 3 prompts (4%)\n```\n\n**Top Issues:**\n```\n1. Missing examples: 34 prompts\n2. Vague instructions: 28 prompts\n3. Poor variable names: 19 prompts\n```\n\n## Automated Quality Checks\n\n### Pre-Execution Checks\n\nBefore execution, verify:\n```\n✓ All variables have values\n✓ Prompt length within limits\n✓ No syntax errors\n✓ Model compatibility\n```\n\n### Pre-Production Checks\n\nBefore promoting to production:\n```\n✓ Quality score ≥ 80\n✓ All high-impact issues resolved\n✓ Tested with real data\n✓ Peer reviewed\n```\n\n## Integration with Workflow\n\n### Development Phase\n```\n1. Write initial prompt\n2. Run quality analysis\n3. Fix critical issues\n4. Test execution\n5. Iterate\n```\n\n### Review Phase\n```\n1. Run final quality check\n2. Address all high-impact issues\n3. Verify score ≥ 80\n4. Get peer review\n5. Promote to staging\n```\n\n### Production Phase\n```\n1. Monitor execution quality\n2. Track user ratings\n3. Re-analyze periodically\n4. Update as needed\n```\n\n## Troubleshooting\n\n### Low Quality Score\n\n**Symptoms:** Score below 70\n\n**Solutions:**\n- Add specific examples\n- Clarify instructions\n- Define output format\n- Add context\n\n### Inconsistent Results\n\n**Symptoms:** Same prompt produces varying quality\n\n**Solutions:**\n- Add more constraints\n- Reduce temperature\n- Provide more examples\n- Use more specific model\n\n### Analysis Not Available\n\n**Symptoms:** Analyze button disabled\n\n**Solutions:**\n- Save prompt first\n- Check prompt length\n- Verify permissions\n\n## Next Steps\n\n- [Prompt Variables Syntax](/dashboard/help/core-features/variables-syntax)\n- [Execution History and Ratings](/dashboard/help/core-features/execution-history)\n- [Advanced Prompt Engineering](/dashboard/help/best-practices/advanced-prompt-engineering)",
    "category": "core-features",
    "subcategory": "quality",
    "tags": ["quality", "analysis", "optimization", "best-practices"],
    "difficulty": "intermediate",
    "lastUpdated": "2025-01-15",
    "featured": false,
    "estimatedReadTime": 10
  },
  {
    "id": "rag-context-preview",
    "slug": "rag-context-preview",
    "title": "Understanding Context Injection",
    "excerpt": "Preview and understand how RAG retrieves and injects document context into your prompts",
    "content": "# Understanding Context Injection\n\n## Overview\n\nRAG (Retrieval-Augmented Generation) injects relevant document chunks into your prompt before sending to the AI model. Understanding this process helps you optimize retrieval and interpret results.\n\n> [!NOTE]\n> Context injection happens automatically when RAG is enabled. You can preview exactly what context will be sent to the model.\n\n## How Context Injection Works\n\n### The RAG Pipeline\n\n```\n1. User Query → \"What are the return policies?\"\n2. Generate Query Embedding → [0.123, 0.456, ...]\n3. Search Document Chunks → Find top 5 most relevant\n4. Retrieve Chunk Content → Extract text from matches\n5. Inject into Prompt → Add context before user query\n6. Send to AI Model → Process with full context\n7. Generate Response → AI uses context to answer\n```\n\n### Context Assembly\n\nRetrieved chunks are assembled into context:\n\n```\nContext:\n\n[Chunk 1 - Relevance: 0.92]\nSource: Return Policy Guide (Page 3)\nOur return policy allows returns within 30 days of purchase...\n\n[Chunk 2 - Relevance: 0.87]\nSource: FAQ Document (Page 12)\nQ: How do I initiate a return?\nA: Contact customer support with your order number...\n\n[Chunk 3 - Relevance: 0.81]\nSource: Terms of Service (Page 8)\nReturns must be in original packaging and unused...\n\nUser Query: What are the return policies?\n\nPlease answer based on the context above.\n```\n\n## Context Preview Tool\n\n### Accessing Preview\n\n1. Open prompt with RAG enabled\n2. Enter variable values\n3. Click **Preview Context**\n4. View retrieved chunks before execution\n\n### Preview Display\n\n**Chunk Card:**\n```\n┌─────────────────────────────────────┐\n│ Chunk 1 of 5                        │\n│ Relevance Score: 0.92 (Excellent)   │\n├─────────────────────────────────────┤\n│ Source: Return Policy Guide         │\n│ Page: 3                             │\n│ Tokens: 234                         │\n├─────────────────────────────────────┤\n│ Our return policy allows returns    │\n│ within 30 days of purchase for a    │\n│ full refund. Items must be in       │\n│ original condition...               │\n└─────────────────────────────────────┘\n```\n\n### Relevance Scores\n\n**Score Interpretation:**\n```\n0.90-1.00: Excellent match (highly relevant)\n0.80-0.89: Good match (relevant)\n0.70-0.79: Fair match (somewhat relevant)\n0.60-0.69: Weak match (marginally relevant)\n<0.60: Poor match (likely not relevant)\n```\n\n> [!TIP]\n> If top chunks have scores below 0.70, consider refining your query or adding more relevant documents.\n\n## Chunk Highlighting\n\n### Keyword Highlighting\n\nQuery terms are highlighted in chunks:\n\n```\nQuery: \"return policy 30 days\"\n\nChunk:\nOur [return] [policy] allows returns within [30] [days]\nof purchase for a full refund.\n```\n\n### Semantic Highlighting\n\nSemantically similar terms also highlighted:\n\n```\nQuery: \"refund\"\n\nChunk:\nCustomers can receive their [money back] within 5-7\nbusiness days after we process the [return].\n```\n\n## Interpreting Retrieved Chunks\n\n### Quality Indicators\n\n**Good Retrieval:**\n```\n✓ Top 3 chunks have scores > 0.80\n✓ Chunks directly answer the query\n✓ Consistent information across chunks\n✓ Relevant source documents\n```\n\n**Poor Retrieval:**\n```\n✗ Top chunks have scores < 0.70\n✗ Chunks don't address the query\n✗ Contradictory information\n✗ Irrelevant source documents\n```\n\n### Troubleshooting Poor Retrieval\n\n**Problem: Low relevance scores**\n\nSolutions:\n- Rephrase query to match document language\n- Add more relevant documents\n- Adjust chunk size (try larger chunks)\n- Use hybrid search instead of semantic-only\n\n**Problem: Wrong chunks retrieved**\n\nSolutions:\n- Add metadata filters (document type, date)\n- Improve document chunking strategy\n- Use more specific queries\n- Increase chunk overlap\n\n**Problem: Missing important information**\n\nSolutions:\n- Increase topK (retrieve more chunks)\n- Check if information exists in documents\n- Verify document processing completed\n- Review chunk boundaries\n\n## Context Window Management\n\n### Token Budget\n\nContext must fit within model limits:\n\n```\nModel: GPT-4 Turbo (128K context)\n\nToken Allocation:\n- System prompt: 500 tokens\n- User prompt: 1,000 tokens\n- Retrieved context: 4,000 tokens (5 chunks × 800 tokens)\n- Reserved for response: 2,000 tokens\n\nTotal: 7,500 tokens (well within limit)\n```\n\n### Context Truncation\n\nIf context exceeds limits:\n\n```\nStrategy 1: Reduce chunk count\n- Retrieve top 3 instead of top 5\n\nStrategy 2: Reduce chunk size\n- Use 512-token chunks instead of 1024\n\nStrategy 3: Summarize chunks\n- Summarize each chunk before injection\n\nStrategy 4: Use larger context model\n- Switch to Claude 3.5 (200K) or Gemini (1M)\n```\n\n## Context Formatting\n\n### Default Format\n\n```\nContext:\n\n[Document 1]\n{chunk_content}\n\n[Document 2]\n{chunk_content}\n\nQuery: {user_query}\n```\n\n### Custom Format\n\nCustomize context formatting:\n\n```json\n{\n  \"contextFormat\": \"xml\",\n  \"template\": \"<context>\\n<chunk id='1' source='{source}' score='{score}'>\\n{content}\\n</chunk>\\n</context>\"\n}\n```\n\n**Result:**\n```xml\n<context>\n  <chunk id='1' source='Return Policy' score='0.92'>\n    Our return policy allows returns within 30 days...\n  </chunk>\n  <chunk id='2' source='FAQ' score='0.87'>\n    Q: How do I initiate a return?\n  </chunk>\n</context>\n```\n\n## Advanced Features\n\n### Chunk Re-ranking\n\nRe-rank chunks for better relevance:\n\n```\nInitial Retrieval (fast, approximate):\n1. Chunk A: 0.85\n2. Chunk B: 0.83\n3. Chunk C: 0.81\n...\n10. Chunk J: 0.72\n\nRe-ranking (slow, accurate):\n1. Chunk C: 0.94 ↑\n2. Chunk A: 0.89 ↓\n3. Chunk E: 0.87 ↑\n4. Chunk B: 0.85 ↓\n5. Chunk D: 0.83 ↑\n```\n\n### Metadata Filtering\n\nFilter chunks by metadata:\n\n```json\n{\n  \"filters\": {\n    \"documentType\": \"policy\",\n    \"version\": \"2024\",\n    \"language\": \"en\",\n    \"department\": \"customer-service\"\n  }\n}\n```\n\n### Chunk Deduplication\n\nRemove duplicate or highly similar chunks:\n\n```\nBefore Deduplication:\n- Chunk 1: \"Our return policy...\" (0.92)\n- Chunk 2: \"Our return policy...\" (0.91) [duplicate]\n- Chunk 3: \"Returns are accepted...\" (0.87)\n\nAfter Deduplication:\n- Chunk 1: \"Our return policy...\" (0.92)\n- Chunk 3: \"Returns are accepted...\" (0.87)\n```\n\n## Best Practices\n\n### 1. Always Preview Context\n\nBefore executing:\n```\n1. Preview retrieved chunks\n2. Verify relevance scores\n3. Check for missing information\n4. Adjust query if needed\n```\n\n### 2. Monitor Relevance Scores\n\nSet quality thresholds:\n```\nExcellent: Top 3 chunks > 0.85\nGood: Top 3 chunks > 0.75\nFair: Top 3 chunks > 0.65\nPoor: Top 3 chunks < 0.65 (investigate)\n```\n\n### 3. Optimize Chunk Count\n\nBalance quality vs cost:\n```\nToo few chunks (1-2): May miss information\nOptimal (3-5): Good balance\nToo many (10+): Noise, high cost\n```\n\n### 4. Use Metadata Filters\n\nNarrow search scope:\n```\nWithout filters: Search all 1,000 documents\nWith filters: Search 50 relevant documents\nResult: Better relevance, faster search\n```\n\n### 5. Test with Edge Cases\n\nVerify retrieval quality:\n```\n- Ambiguous queries\n- Queries with no answer\n- Queries requiring multiple chunks\n- Queries with contradictory information\n```\n\n## Troubleshooting\n\n### No Chunks Retrieved\n\n**Symptoms:** Context preview shows no results\n\n**Solutions:**\n- Verify documents are uploaded and processed\n- Check RAG is enabled for prompt\n- Review metadata filters (may be too restrictive)\n- Try broader query terms\n\n### Irrelevant Chunks Retrieved\n\n**Symptoms:** Low relevance scores, wrong content\n\n**Solutions:**\n- Rephrase query to match document language\n- Add more relevant documents\n- Adjust chunking strategy\n- Use hybrid search\n\n### Context Too Large\n\n**Symptoms:** Token limit exceeded error\n\n**Solutions:**\n- Reduce chunk count (topK)\n- Use smaller chunks\n- Switch to larger context model\n- Summarize chunks\n\n## Next Steps\n\n- [Chunking and Retrieval Configuration](/dashboard/help/core-features/rag-chunking-config)\n- [RAG Execution Basics](/dashboard/help/core-features/rag-execution-basics)\n- [Document Upload Guide](/dashboard/help/core-features/document-upload-guide)",
    "category": "core-features",
    "subcategory": "rag",
    "tags": ["rag", "context", "retrieval", "preview", "chunks"],
    "difficulty": "intermediate",
    "lastUpdated": "2025-01-15",
    "featured": false,
    "estimatedReadTime": 11,
    "prerequisites": ["rag-chunking-config"],
    "relatedArticles": ["rag-chunking-config", "rag-execution-basics", "document-upload-guide"]
  },
  {
    "id": "model-performance",
    "slug": "model-performance",
    "title": "Model Performance Dashboard",
    "excerpt": "Monitor and compare AI model performance with detailed metrics, latency analysis, and success rates",
    "content": "# Model Performance Dashboard\n\n## Overview\n\nThe Model Performance Dashboard provides detailed insights into how different AI models perform across your prompts, helping you optimize model selection and costs.\n\n> [!TIP]\n> Use performance data to identify the best model for each use case and detect performance degradation early.\n\n## Accessing the Dashboard\n\n1. Navigate to **Analytics** → **Model Performance**\n2. Select time period (24h, 7d, 30d, custom)\n3. Filter by workspace, prompt, or user\n\n## Key Metrics\n\n### Success Rate by Model\n\n```\nGPT-4 Turbo:      98.5% (1,234 executions)\nClaude 3.5:       97.8% (892 executions)\nGPT-3.5 Turbo:    96.2% (2,456 executions)\nClaude 3 Haiku:   95.1% (567 executions)\nGemini 1.5 Pro:   97.3% (234 executions)\n```\n\n### Average Latency\n\n**Response Time (p50/p95/p99):**\n```\nGPT-4 Turbo:      2.3s / 4.8s / 8.2s\nClaude 3.5:       1.8s / 3.2s / 5.9s\nGPT-3.5 Turbo:    1.2s / 2.1s / 3.8s\nClaude 3 Haiku:   0.8s / 1.5s / 2.4s\nGemini 1.5 Pro:   2.1s / 4.2s / 7.1s\n```\n\n> [!NOTE]\n> p50 = median, p95 = 95th percentile, p99 = 99th percentile. Lower is better.\n\n### Quality Ratings\n\n**Average User Rating:**\n```\nGPT-4 Turbo:      4.5 / 5.0 ⭐⭐⭐⭐⭐\nClaude 3.5:       4.4 / 5.0 ⭐⭐⭐⭐⭐\nGPT-3.5 Turbo:    3.9 / 5.0 ⭐⭐⭐⭐\nClaude 3 Haiku:   3.7 / 5.0 ⭐⭐⭐⭐\nGemini 1.5 Pro:   4.2 / 5.0 ⭐⭐⭐⭐\n```\n\n### Cost Efficiency\n\n**Cost per Execution:**\n```\nGPT-4 Turbo:      $0.045 (highest quality)\nClaude 3.5:       $0.028 (best balance)\nGPT-3.5 Turbo:    $0.008 (most economical)\nClaude 3 Haiku:   $0.006 (fastest & cheapest)\nGemini 1.5 Pro:   $0.015 (long context)\n```\n\n**Quality-to-Cost Ratio:**\n```\nClaude 3.5:       157 (4.4 rating / $0.028)\nGPT-4 Turbo:      100 (4.5 rating / $0.045)\nGemini 1.5 Pro:   280 (4.2 rating / $0.015)\nGPT-3.5 Turbo:    488 (3.9 rating / $0.008)\n```\n\n## Performance Trends\n\n### Latency Over Time\n\n**Line Chart:**\n```\nLatency (seconds)\n5.0 ┤\n4.5 ┤     GPT-4 ──────\n4.0 ┤\n3.5 ┤\n3.0 ┤   Claude 3.5 ────\n2.5 ┤\n2.0 ┤ GPT-3.5 ──────────\n1.5 ┤\n1.0 ┤ Haiku ────────────\n0.5 ┤\n    └─────────────────────\n     Week 1  Week 2  Week 3\n```\n\n### Success Rate Trends\n\n```\nSuccess Rate (%)\n100 ┤ ████████████████████ GPT-4\n 98 ┤ ████████████████████ Claude 3.5\n 96 ┤ ████████████████████ GPT-3.5\n 94 ┤ ████████████████████ Haiku\n 92 ┤\n    └─────────────────────\n     Week 1  Week 2  Week 3\n```\n\n## Model Comparison\n\n### Side-by-Side Comparison\n\nSelect 2-4 models to compare:\n\n```\n┌─────────────┬──────────┬──────────┬──────────┐\n│ Metric      │ GPT-4    │ Claude   │ GPT-3.5  │\n├─────────────┼──────────┼──────────┼──────────┤\n│ Success %   │ 98.5%    │ 97.8%    │ 96.2%    │\n│ Avg Latency │ 2.3s     │ 1.8s     │ 1.2s     │\n│ Avg Rating  │ 4.5      │ 4.4      │ 3.9      │\n│ Avg Cost    │ $0.045   │ $0.028   │ $0.008   │\n│ Total Exec  │ 1,234    │ 892      │ 2,456    │\n└─────────────┴──────────┴──────────┴──────────┘\n```\n\n### Best Model by Use Case\n\n**Customer Support:**\n```\nRecommended: Claude 3.5 Sonnet\nWhy: Fast (1.8s), high quality (4.4★), cost-effective\nAlternative: GPT-3.5 Turbo (for simple queries)\n```\n\n**Content Generation:**\n```\nRecommended: GPT-4 Turbo\nWhy: Highest quality (4.5★), creative, coherent\nAlternative: Claude 3.5 Sonnet (faster, cheaper)\n```\n\n**Data Analysis:**\n```\nRecommended: GPT-4 Turbo\nWhy: Best reasoning, accuracy, complex tasks\nAlternative: Gemini 1.5 Pro (long documents)\n```\n\n**Real-Time Chat:**\n```\nRecommended: Claude 3 Haiku\nWhy: Fastest (0.8s), low cost, good quality\nAlternative: GPT-3.5 Turbo\n```\n\n## Failure Analysis\n\n### Failure Breakdown\n\n```\nGPT-4 Turbo Failures (1.5%):\n- Rate limit: 45%\n- Timeout: 30%\n- Invalid response: 15%\n- Other: 10%\n\nClaude 3.5 Failures (2.2%):\n- Overloaded: 50%\n- Timeout: 25%\n- Content filter: 15%\n- Other: 10%\n```\n\n### Common Failure Patterns\n\n**Rate Limiting:**\n```\nSymptom: 429 Too Many Requests\nAffected: GPT-4 Turbo (peak hours)\nSolution: Implement retry with backoff\n```\n\n**Timeouts:**\n```\nSymptom: Request timeout after 30s\nAffected: All models (large context)\nSolution: Reduce context size or increase timeout\n```\n\n**Content Filtering:**\n```\nSymptom: Response blocked by safety filter\nAffected: Claude models\nSolution: Rephrase prompt, avoid sensitive topics\n```\n\n## Performance Alerts\n\n### Configuring Alerts\n\n```json\n{\n  \"alerts\": [\n    {\n      \"metric\": \"success_rate\",\n      \"model\": \"gpt-4-turbo\",\n      \"threshold\": 95,\n      \"condition\": \"below\",\n      \"action\": \"email\"\n    },\n    {\n      \"metric\": \"latency_p95\",\n      \"model\": \"claude-3.5-sonnet\",\n      \"threshold\": 5.0,\n      \"condition\": \"above\",\n      \"action\": \"slack\"\n    }\n  ]\n}\n```\n\n### Alert Examples\n\n**Success Rate Drop:**\n```\n⚠️ Alert: GPT-4 Turbo success rate dropped to 92%\nPrevious 7d average: 98.5%\nCurrent: 92.0% (↓ 6.5%)\nAction: Investigate recent failures\n```\n\n**Latency Spike:**\n```\n⚠️ Alert: Claude 3.5 p95 latency increased to 6.2s\nPrevious 7d average: 3.2s\nCurrent: 6.2s (↑ 94%)\nAction: Check model status, consider fallback\n```\n\n## Optimization Recommendations\n\n### Automated Suggestions\n\n**Cost Optimization:**\n```\n💡 Potential savings: $125/month\n\nPrompt: \"Simple Email Response\"\nCurrent: GPT-4 Turbo ($0.045/exec, 234 exec/month)\nRecommended: GPT-3.5 Turbo ($0.008/exec)\nSavings: $8.66/month\nQuality impact: -0.3 stars (acceptable)\n```\n\n**Performance Optimization:**\n```\n💡 Improve response time by 40%\n\nPrompt: \"Customer Support Chat\"\nCurrent: GPT-4 Turbo (2.3s average)\nRecommended: Claude 3 Haiku (0.8s average)\nImprovement: 1.5s faster\nQuality impact: -0.8 stars (review needed)\n```\n\n## Best Practices\n\n### 1. Monitor Regularly\n\nCheck dashboard weekly:\n```\n- Review success rates\n- Check latency trends\n- Analyze failure patterns\n- Compare model performance\n```\n\n### 2. Set Performance Baselines\n\nEstablish targets:\n```\nSuccess Rate: ≥ 95%\nLatency p95: ≤ 5s\nAverage Rating: ≥ 4.0\nCost per Execution: ≤ $0.05\n```\n\n### 3. Test Before Switching\n\nBefore changing models:\n```\n1. Run A/B test (100 executions each)\n2. Compare quality ratings\n3. Measure latency impact\n4. Calculate cost difference\n5. Make informed decision\n```\n\n### 4. Use Fallback Models\n\nConfigure fallbacks:\n```\nPrimary: GPT-4 Turbo\nFallback 1: Claude 3.5 (if rate limited)\nFallback 2: GPT-3.5 (if both unavailable)\n```\n\n### 5. Track Quality Over Time\n\nMonitor rating trends:\n```\nWeek 1: 4.5 stars\nWeek 2: 4.3 stars ↓ (investigate)\nWeek 3: 4.6 stars ↑ (improvement confirmed)\n```\n\n## Troubleshooting\n\n### Declining Success Rate\n\n**Symptoms:** Success rate dropping over time\n\n**Solutions:**\n- Check model status page\n- Review recent prompt changes\n- Analyze failure logs\n- Consider model switch\n\n### Increasing Latency\n\n**Symptoms:** Response times getting slower\n\n**Solutions:**\n- Check if context size increased\n- Review model load (peak hours)\n- Consider faster model\n- Optimize prompt length\n\n### Inconsistent Performance\n\n**Symptoms:** High variance in metrics\n\n**Solutions:**\n- Identify patterns (time of day, prompt type)\n- Check for rate limiting\n- Review prompt complexity\n- Use more stable model\n\n## Next Steps\n\n- [Model Selection and Comparison](/dashboard/help/core-features/model-selection)\n- [Analytics Dashboard Overview](/dashboard/help/core-features/analytics-overview)\n- [Token Usage and Cost Tracking](/dashboard/help/core-features/token-costs)",
    "category": "core-features",
    "subcategory": "analytics",
    "tags": ["performance", "models", "metrics", "latency", "monitoring"],
    "difficulty": "intermediate",
    "lastUpdated": "2025-01-15",
    "featured": false,
    "estimatedReadTime": 12,
    "prerequisites": ["analytics-overview"],
    "relatedArticles": ["model-selection", "analytics-overview", "token-costs"]
  },
  {
    "id": "metrics-alerts",
    "slug": "metrics-alerts",
    "title": "Real-time Metrics and Alerts",
    "excerpt": "Monitor live metrics and configure alerts for cost thresholds, performance issues, and quality drops",
    "content": "# Real-time Metrics and Alerts\n\n## Overview\n\nReal-time metrics provide instant visibility into your EthosPrompt usage, while alerts notify you of important events before they become problems.\n\n> [!TIP]\n> Set up alerts for cost thresholds and performance degradation to avoid surprises and maintain quality.\n\n## Real-time Metrics Card\n\n### Dashboard Widget\n\nLive metrics displayed on dashboard:\n\n```\n┌─────────────────────────────────┐\n│ Real-time Metrics (Last Hour)  │\n├─────────────────────────────────┤\n│ Executions: 45 (↑ 12% vs prev)  │\n│ Success Rate: 98.2%             │\n│ Avg Latency: 2.3s               │\n│ Cost: $5.67                     │\n│ Active Users: 8                 │\n└─────────────────────────────────┘\n```\n\n### Auto-Refresh\n\nMetrics update automatically:\n- Every 30 seconds (default)\n- Every 1 minute (low activity)\n- Every 10 seconds (high activity)\n\n## Alert Types\n\n### Cost Alerts\n\n**Daily Budget Alert:**\n```json\n{\n  \"type\": \"daily_budget\",\n  \"threshold\": 50.00,\n  \"current\": 45.23,\n  \"percentage\": 90,\n  \"action\": \"email\"\n}\n```\n\n**Monthly Budget Alert:**\n```json\n{\n  \"type\": \"monthly_budget\",\n  \"threshold\": 1000.00,\n  \"current\": 856.34,\n  \"percentage\": 86,\n  \"action\": \"slack\"\n}\n```\n\n**Execution Cost Alert:**\n```json\n{\n  \"type\": \"execution_cost\",\n  \"threshold\": 1.00,\n  \"executionId\": \"exec_123\",\n  \"cost\": 1.23,\n  \"action\": \"require_approval\"\n}\n```\n\n### Performance Alerts\n\n**Success Rate Drop:**\n```\n⚠️ Success rate dropped below 95%\nCurrent: 92.3%\nThreshold: 95.0%\nPeriod: Last hour\nAction: Investigate failures\n```\n\n**Latency Spike:**\n```\n⚠️ Average latency exceeded 5 seconds\nCurrent: 6.8s\nThreshold: 5.0s\nPeriod: Last 15 minutes\nAction: Check model status\n```\n\n**High Failure Rate:**\n```\n⚠️ Failure rate above 5%\nCurrent: 8.2%\nThreshold: 5.0%\nAffected: GPT-4 Turbo\nAction: Switch to fallback model\n```\n\n### Quality Alerts\n\n**Rating Drop:**\n```\n⚠️ Average rating dropped below 4.0\nCurrent: 3.7 stars\nThreshold: 4.0 stars\nPrompt: Customer Support V2\nAction: Review recent changes\n```\n\n**Low Quality Score:**\n```\n⚠️ Prompt quality score below 70\nPrompt: Email Generator\nScore: 65/100\nIssues: Missing examples, vague instructions\nAction: Run quality analysis\n```\n\n## Configuring Alerts\n\n### Step 1: Navigate to Alerts\n\n1. Go to **Settings** → **Alerts**\n2. Click **Create Alert**\n\n### Step 2: Choose Alert Type\n\n- Cost alerts\n- Performance alerts\n- Quality alerts\n- Custom alerts\n\n### Step 3: Set Threshold\n\n```json\n{\n  \"metric\": \"daily_cost\",\n  \"threshold\": 50.00,\n  \"condition\": \"greater_than\",\n  \"period\": \"1d\"\n}\n```\n\n### Step 4: Configure Actions\n\n**Notification Channels:**\n- Email\n- Slack\n- Webhook\n- SMS (premium)\n\n**Automated Actions:**\n- Pause executions\n- Switch to fallback model\n- Require approval\n- Send notification\n\n### Step 5: Test Alert\n\n1. Click **Send Test Alert**\n2. Verify notification received\n3. Activate alert\n\n## Alert Examples\n\n### Budget Protection\n\n```json\n{\n  \"name\": \"Daily Budget 80% Warning\",\n  \"type\": \"cost\",\n  \"metric\": \"daily_cost\",\n  \"threshold\": 40.00,\n  \"budget\": 50.00,\n  \"percentage\": 80,\n  \"actions\": [\n    {\"type\": \"email\", \"recipients\": [\"admin@example.com\"]},\n    {\"type\": \"slack\", \"channel\": \"#alerts\"}\n  ]\n}\n```\n\n### Performance Monitoring\n\n```json\n{\n  \"name\": \"High Latency Alert\",\n  \"type\": \"performance\",\n  \"metric\": \"avg_latency\",\n  \"threshold\": 5.0,\n  \"period\": \"15m\",\n  \"actions\": [\n    {\"type\": \"slack\", \"channel\": \"#ops\"},\n    {\"type\": \"webhook\", \"url\": \"https://api.example.com/alerts\"}\n  ]\n}\n```\n\n### Quality Assurance\n\n```json\n{\n  \"name\": \"Low Rating Alert\",\n  \"type\": \"quality\",\n  \"metric\": \"avg_rating\",\n  \"threshold\": 4.0,\n  \"condition\": \"below\",\n  \"scope\": \"production\",\n  \"actions\": [\n    {\"type\": \"email\", \"recipients\": [\"team@example.com\"]}\n  ]\n}\n```\n\n## Alert Management\n\n### Alert Dashboard\n\nView all active alerts:\n\n```\n┌──────────────────────────────────────┐\n│ Active Alerts (3)                    │\n├──────────────────────────────────────┤\n│ ⚠️ Daily budget at 85% (2 min ago)   │\n│ ⚠️ Latency spike detected (5 min ago)│\n│ ✅ Success rate recovered (1h ago)   │\n└──────────────────────────────────────┘\n```\n\n### Alert History\n\nReview past alerts:\n```\nDate: 2025-01-15 10:30\nAlert: Daily Budget 80%\nValue: $42.34 / $50.00\nAction: Email sent\nResolved: 2025-01-15 11:00\n```\n\n### Snooze Alerts\n\nTemporarily disable:\n```\nSnooze for:\n- 1 hour\n- 4 hours\n- 24 hours\n- Until manually re-enabled\n```\n\n## Best Practices\n\n### 1. Set Multiple Thresholds\n\n```\n50% budget: Info notification\n80% budget: Warning email\n90% budget: Urgent alert + Slack\n100% budget: Pause executions\n```\n\n### 2. Use Appropriate Channels\n\n```\nInfo: Email\nWarning: Email + Slack\nCritical: Email + Slack + SMS + Webhook\n```\n\n### 3. Test Alerts Regularly\n\n```\nMonthly: Test all critical alerts\nQuarterly: Review and update thresholds\nAnnually: Audit alert effectiveness\n```\n\n### 4. Avoid Alert Fatigue\n\n```\n❌ Too many alerts: 50+ per day\n✅ Right amount: 5-10 per day\n❌ Too sensitive: Alert on every minor issue\n✅ Right sensitivity: Alert on actionable issues\n```\n\n### 5. Document Response Procedures\n\n```\nAlert: Daily Budget Exceeded\nResponse:\n1. Review execution history\n2. Identify high-cost prompts\n3. Optimize or pause if needed\n4. Update budget if justified\n```\n\n## Troubleshooting\n\n### Alerts Not Firing\n\n**Symptoms:** No alerts received\n\n**Solutions:**\n- Verify alert is active\n- Check threshold values\n- Test notification channels\n- Review alert conditions\n\n### Too Many Alerts\n\n**Symptoms:** Alert fatigue\n\n**Solutions:**\n- Increase thresholds\n- Add cooldown periods\n- Consolidate similar alerts\n- Use digest notifications\n\n### False Positives\n\n**Symptoms:** Alerts for non-issues\n\n**Solutions:**\n- Adjust sensitivity\n- Add context filters\n- Use longer time windows\n- Review alert logic\n\n## Next Steps\n\n- [Analytics Dashboard Overview](/dashboard/help/core-features/analytics-overview)\n- [Token Usage and Cost Tracking](/dashboard/help/core-features/token-costs)\n- [Model Performance Dashboard](/dashboard/help/core-features/model-performance)",
    "category": "core-features",
    "subcategory": "monitoring",
    "tags": ["alerts", "monitoring", "metrics", "real-time", "notifications"],
    "difficulty": "intermediate",
    "lastUpdated": "2025-01-15",
    "featured": false,
    "estimatedReadTime": 9,
    "prerequisites": ["analytics-overview"],
    "relatedArticles": ["analytics-overview", "token-costs", "model-performance"]
  },
  {
    "id": "shared-library",
    "slug": "shared-library",
    "title": "Shared Prompt Library",
    "excerpt": "Share prompts across teams, manage permissions, and collaborate on prompt development",
    "content": "# Shared Prompt Library\n\n## Overview\n\nThe Shared Prompt Library enables team collaboration by allowing you to share prompts, manage access permissions, and track usage across your organization.\n\n## Sharing Prompts\n\n### Share with Team\n\n1. Open prompt\n2. Click **Share**\n3. Select users or workspaces\n4. Set permissions (view/edit)\n5. Click **Share**\n\n### Permission Levels\n\n**Viewer:**\n- View prompt content\n- Execute prompt\n- Cannot edit or delete\n\n**Editor:**\n- View and edit prompt\n- Execute prompt\n- Cannot delete\n\n**Owner:**\n- Full control\n- Can delete\n- Manage sharing\n\n## Library Organization\n\n### Categories\n\n- Personal (private)\n- Team (workspace)\n- Organization (all users)\n- Public (community)\n\n### Filtering\n\n```\nFilter by:\n- Owner\n- Workspace\n- Tags\n- Category\n- Last modified\n```\n\n## Best Practices\n\n### 1. Use Descriptive Names\n\n```\n✅ \"Customer Support - Refund Request V2\"\n❌ \"Prompt 1\"\n```\n\n### 2. Add Tags\n\n```\nTags: customer-support, refunds, production, v2\n```\n\n### 3. Document Changes\n\n```\nVersion 2.0:\n- Added refund policy context\n- Improved tone guidelines\n- Added examples\n```\n\n### 4. Set Appropriate Permissions\n\n```\nProduction prompts: View-only for most users\nDevelopment prompts: Edit access for team\n```\n\n## Next Steps\n\n- [Workspaces Setup and Management](/dashboard/help/core-features/workspaces-setup)\n- [Template Library Usage](/dashboard/help/core-features/template-library)",
    "category": "core-features",
    "subcategory": "collaboration",
    "tags": ["sharing", "collaboration", "library", "permissions"],
    "difficulty": "beginner",
    "lastUpdated": "2025-01-15",
    "featured": false,
    "estimatedReadTime": 6,
    "prerequisites": ["workspaces-setup"],
    "relatedArticles": ["workspaces-setup", "template-library"]
  },
  {
    "id": "integration-patterns",
    "slug": "integration-patterns",
    "title": "Integration Patterns",
    "excerpt": "Integrate EthosPrompt with external tools using Zapier, Make, and custom integrations",
    "content": "# Integration Patterns\n\n## Overview\n\nEthosPrompt integrates with popular automation platforms and supports custom integrations via webhooks and API.\n\n## Zapier Integration\n\n### Available Triggers\n\n- Execution completed\n- Execution failed\n- Cost threshold exceeded\n- New prompt created\n\n### Available Actions\n\n- Execute prompt\n- Create prompt\n- Upload document\n\n### Example Zap\n\n```\nTrigger: New email in Gmail\nAction: Execute EthosPrompt \"Email Response\"\nAction: Send response via Gmail\n```\n\n## Make (Integromat) Integration\n\n### Modules\n\n- Watch executions\n- Execute prompt\n- Get execution result\n- Upload document\n\n### Example Scenario\n\n```\n1. Watch Google Sheets for new row\n2. Execute EthosPrompt with row data\n3. Update sheet with result\n4. Send Slack notification\n```\n\n## Custom Integrations\n\n### Using Webhooks\n\nSee [Webhook Manager Setup](/dashboard/help/core-features/webhooks-setup) for details.\n\n### Using API\n\nSee [API Integration Guide](/dashboard/help/api/api-integration-guide) for details.\n\n## Common Integration Patterns\n\n### Email Automation\n\n```\nIncoming Email → EthosPrompt → Draft Response → Send\n```\n\n### Data Processing\n\n```\nNew Data → EthosPrompt Analysis → Update Database → Notify Team\n```\n\n### Content Generation\n\n```\nScheduled Trigger → EthosPrompt → Publish to CMS → Share on Social\n```\n\n## Best Practices\n\n### 1. Use Webhooks for Real-time\n\n```\nWebhooks: Instant notifications\nPolling: Delayed, inefficient\n```\n\n### 2. Handle Errors Gracefully\n\n```\nImplement retry logic\nLog failures\nSend error notifications\n```\n\n### 3. Secure API Keys\n\n```\nUse environment variables\nRotate keys regularly\nLimit permissions\n```\n\n## Next Steps\n\n- [Webhook Manager Setup](/dashboard/help/core-features/webhooks-setup)\n- [API Integration Guide](/dashboard/help/api/api-integration-guide)",
    "category": "core-features",
    "subcategory": "integrations",
    "tags": ["integrations", "zapier", "make", "automation", "api"],
    "difficulty": "intermediate",
    "lastUpdated": "2025-01-15",
    "featured": false,
    "estimatedReadTime": 7,
    "prerequisites": ["webhooks-setup"],
    "relatedArticles": ["webhooks-setup", "api-integration-guide"]
  },
  {
    "id": "security-privacy",
    "slug": "security-privacy",
    "title": "Security and Privacy Guidelines",
    "excerpt": "Understand data security, API key management, RBAC, and compliance best practices",
    "content": "# Security and Privacy Guidelines\n\n## Overview\n\nEthosPrompt implements enterprise-grade security measures to protect your data, prompts, and API keys.\n\n> [!WARNING]\n> Never share API keys or include sensitive data in prompts without proper encryption and access controls.\n\n## Data Security\n\n### Encryption\n\n**At Rest:**\n- All data encrypted with AES-256\n- Firestore encryption enabled\n- Storage encryption enabled\n\n**In Transit:**\n- TLS 1.3 for all connections\n- HTTPS enforced\n- Certificate pinning\n\n### Data Isolation\n\n```\nWorkspace A data ≠ Workspace B data\nUser permissions enforced\nRow-level security in Firestore\n```\n\n## API Key Management\n\n### Best Practices\n\n**1. Use Environment Variables:**\n```bash\n# .env\nOPENROUTER_API_KEY=sk-xxx\nFIREBASE_API_KEY=AIza-xxx\n```\n\n**2. Rotate Keys Regularly:**\n```\nProduction: Every 90 days\nDevelopment: Every 180 days\n```\n\n**3. Limit Permissions:**\n```\nUse least privilege principle\nCreate separate keys for different environments\n```\n\n**4. Monitor Usage:**\n```\nTrack API key usage\nSet up alerts for unusual activity\nRevoke compromised keys immediately\n```\n\n### Storing Keys\n\n**✅ Secure:**\n```\n- Firebase environment config\n- Environment variables\n- Secret management services (AWS Secrets Manager, etc.)\n```\n\n**❌ Insecure:**\n```\n- Hardcoded in source code\n- Committed to Git\n- Shared via email/chat\n- Stored in plain text files\n```\n\n## Access Control (RBAC)\n\n### User Roles\n\n**Admin:**\n- Full system access\n- Manage users and workspaces\n- Configure security settings\n\n**Editor:**\n- Create and edit prompts\n- Execute prompts\n- Upload documents\n\n**Viewer:**\n- View prompts\n- Execute prompts (read-only)\n- View analytics\n\n**Guest:**\n- Limited access\n- Specific prompts only\n- No data export\n\n### Workspace Permissions\n\nSee [Workspaces Setup](/dashboard/help/core-features/workspaces-setup) for details.\n\n## Compliance\n\n### GDPR Compliance\n\n**Data Rights:**\n- Right to access\n- Right to deletion\n- Right to portability\n- Right to rectification\n\n**Implementation:**\n```\nData export: Available in settings\nData deletion: Contact support\nConsent management: Built-in\n```\n\n### SOC 2 Compliance\n\n**Controls:**\n- Access controls\n- Encryption\n- Audit logging\n- Incident response\n\n### HIPAA Compliance\n\n> [!WARNING]\n> EthosPrompt is not HIPAA-compliant by default. Contact enterprise sales for HIPAA-compliant deployment.\n\n## Audit Logging\n\n### Logged Events\n\n```\n- User login/logout\n- Prompt creation/modification/deletion\n- Document uploads\n- Execution history\n- Permission changes\n- API key usage\n```\n\n### Accessing Logs\n\n1. Navigate to **Settings** → **Audit Logs**\n2. Filter by user, action, date\n3. Export for compliance\n\n## Data Retention\n\n### Default Retention\n\n```\nExecution history: 90 days\nAudit logs: 1 year\nDocuments: Until deleted\nPrompts: Until deleted\n```\n\n### Custom Retention\n\nConfigure in workspace settings:\n```json\n{\n  \"retention\": {\n    \"executions\": 180,\n    \"logs\": 365,\n    \"autoDelete\": true\n  }\n}\n```\n\n## Sensitive Data Handling\n\n### PII (Personally Identifiable Information)\n\n**Best Practices:**\n```\n1. Minimize PII in prompts\n2. Use variables for sensitive data\n3. Enable auto-redaction\n4. Review execution history regularly\n```\n\n**Auto-Redaction:**\n```\nDetect and redact:\n- Email addresses\n- Phone numbers\n- Credit card numbers\n- SSN/Tax IDs\n```\n\n### Secrets in Prompts\n\n**❌ Never include:**\n```\n- API keys\n- Passwords\n- Private keys\n- Access tokens\n```\n\n**✅ Instead use:**\n```\n- Environment variables\n- Secret management\n- Secure variable injection\n```\n\n## Incident Response\n\n### Security Incident\n\n**If you suspect a breach:**\n```\n1. Immediately revoke API keys\n2. Change passwords\n3. Contact support: security@ethosprompt.com\n4. Review audit logs\n5. Document incident\n```\n\n### Data Breach Notification\n\n```\nEthosPrompt will notify affected users within 72 hours\nof discovering a data breach, as required by GDPR.\n```\n\n## Best Practices Summary\n\n### 1. Secure API Keys\n```\n✓ Use environment variables\n✓ Rotate regularly\n✓ Monitor usage\n✓ Revoke when compromised\n```\n\n### 2. Implement Least Privilege\n```\n✓ Assign minimum necessary permissions\n✓ Review permissions quarterly\n✓ Remove access when no longer needed\n```\n\n### 3. Encrypt Sensitive Data\n```\n✓ Use encryption at rest and in transit\n✓ Never store plain text secrets\n✓ Use secure variable injection\n```\n\n### 4. Monitor and Audit\n```\n✓ Enable audit logging\n✓ Review logs regularly\n✓ Set up security alerts\n✓ Conduct periodic security reviews\n```\n\n### 5. Train Your Team\n```\n✓ Security awareness training\n✓ Document security policies\n✓ Regular security updates\n✓ Incident response drills\n```\n\n## Next Steps\n\n- [Workspaces Setup and Management](/dashboard/help/core-features/workspaces-setup)\n- [API Integration Guide](/dashboard/help/api/api-integration-guide)\n- [Webhook Manager Setup](/dashboard/help/core-features/webhooks-setup)",
    "category": "best-practices",
    "subcategory": "security",
    "tags": ["security", "privacy", "compliance", "gdpr", "encryption"],
    "difficulty": "intermediate",
    "lastUpdated": "2025-01-15",
    "featured": false,
    "estimatedReadTime": 11
  },
  {
    "id": "account-profile-settings",
    "slug": "account-profile-settings",
    "title": "Account and Profile Settings",
    "excerpt": "Manage your profile, security, notification preferences, and workspace settings",
    "content": "# Account and Profile Settings\n\n## Profile\nUpdate your name, avatar, and email.\n\n## Security\nEnable two-factor authentication and change password.\n\n## Notifications\nConfigure email and in-app notifications.\n\n## Workspace Settings\nManage members, roles, and billing.",
    "category": "account-settings",
    "subcategory": "account",
    "tags": ["account", "settings", "profile", "security", "notifications"],
    "difficulty": "beginner",
    "lastUpdated": "2025-01-15",
    "featured": false
  }
]
